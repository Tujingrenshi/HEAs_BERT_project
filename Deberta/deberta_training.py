"""
DeBERTaæ¨¡åž‹è®­ç»ƒ - ç‹¬ç«‹è¿è¡Œç‰ˆæœ¬
é€‚ç”¨äºŽé’¢é“é¢†åŸŸæ•°æ®é›†è®­ç»ƒ
"""

# ==================== è­¦å‘Šå¿½ç•¥ ====================
import warnings
import numpy as np

# å¿½ç•¥ç‰¹å®šè­¦å‘Š
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)
np.warnings.filterwarnings('ignore')
import logging
import torch
import glob
import os
import json
import gc
import time
import shutil
from pathlib import Path
from tqdm import tqdm

from datasets import load_dataset, Dataset, DatasetDict
from tokenizers.normalizers import BertNormalizer

from transformers import (
    AutoConfig,
    AutoTokenizer,
    DebertaV2ForMaskedLM,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
    set_seed,
    EarlyStoppingCallback,
)

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)

import numpy as np
np.warnings.filterwarnings('ignore')

# ==================== å…¨å±€é…ç½®åŒºåŸŸ ====================

# æ•°æ®è·¯å¾„é…ç½®
TRAIN_FILE = r"..\data\train_data.txt"
VAL_FILE = r"..\data\val_data.txt"
OUTPUT_DIR = r"model_results_save"

# åŸºç¡€é…ç½®
SEED = 666
PRETRAINED_MODEL = "..\deberta-v3-base"
CACHE_DIR = None

# è¯è¡¨å’Œåºåˆ—é…ç½®
VOCAB_SIZE = 50000
MAX_SEQ_LENGTH = 256

# è®­ç»ƒé…ç½®
BATCH_SIZE = 8
EVAL_BATCH_SIZE = 16
GRADIENT_ACCUMULATION_STEPS = 4
NUM_EPOCHS = 100

# å­¦ä¹ çŽ‡ç­–ç•¥
LEARNING_RATE = 2e-5
WARMUP_RATIO = 0.3
WEIGHT_DECAY = 0.01
LR_SCHEDULER_TYPE = 'cosine'

# MLMé…ç½®
MLM_PROBABILITY = 0.15

# Early Stoppingé…ç½®
EARLY_STOPPING_PATIENCE = 15
EARLY_STOPPING_THRESHOLD = 0.00001

# åŠŸèƒ½å¼€å…³
SKIP_NORMALIZATION = False  # æ˜¯å¦è·³è¿‡æ–‡æœ¬æ ‡å‡†åŒ–
SKIP_TOKENIZER_TRAINING = False  # æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒtokenizer
USE_PRETRAINED_WEIGHTS = False  # æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡

# è®­ç»ƒä¼˜åŒ–é…ç½®
GRADIENT_CHECKPOINTING = False  # æ¢¯åº¦æ£€æŸ¥ç‚¹
MAX_GRAD_NORM = 0.5
ADAM_BETA1 = 0.9
ADAM_BETA2 = 0.999
ADAM_EPSILON = 1e-6

# ä¿å­˜å’Œæ—¥å¿—é…ç½®
EVAL_STEPS = 100
SAVE_STEPS = 100
SAVE_TOTAL_LIMIT = 5
LOGGING_STEPS = 50



# æ£€æŸ¥å¹¶åˆ é™¤çŽ°æœ‰æ–‡ä»¶å¤¹
if os.path.exists(OUTPUT_DIR):
    print(f"åˆ é™¤çŽ°æœ‰æ–‡ä»¶å¤¹: {OUTPUT_DIR}")
    shutil.rmtree(OUTPUT_DIR)





# ==================== æ—¥å¿—é…ç½® ====================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ==================== è¾…åŠ©å‡½æ•° ====================

def print_config():
    """æ‰“å°å½“å‰é…ç½®"""
    print("\n" + "=" * 80)
    print("ðŸ“‹ DeBERTaè®­ç»ƒé…ç½®")
    print("=" * 80)
    print(f"è®­ç»ƒæ•°æ®:          {TRAIN_FILE}")
    print(f"éªŒè¯æ•°æ®:          {VAL_FILE}")
    print(f"è¾“å‡ºç›®å½•:          {OUTPUT_DIR}")
    print(f"é¢„è®­ç»ƒæ¨¡åž‹:        {PRETRAINED_MODEL}")
    print("-" * 80)
    print(f"è¯è¡¨å¤§å°:          {VOCAB_SIZE}")
    print(f"æœ€å¤§åºåˆ—é•¿åº¦:      {MAX_SEQ_LENGTH}")
    print(f"è®­ç»ƒæ‰¹æ¬¡å¤§å°:      {BATCH_SIZE}")
    print(f"æ¢¯åº¦ç´¯ç§¯æ­¥æ•°:      {GRADIENT_ACCUMULATION_STEPS}")
    print(f"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°:      {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
    print(f"è®­ç»ƒè½®æ•°:          {NUM_EPOCHS}")
    print("-" * 80)
    print(f"å­¦ä¹ çŽ‡:            {LEARNING_RATE}")
    print(f"Warmupæ¯”ä¾‹:        {WARMUP_RATIO}")
    print(f"å­¦ä¹ çŽ‡è°ƒåº¦å™¨:      {LR_SCHEDULER_TYPE}")
    print(f"æƒé‡è¡°å‡:          {WEIGHT_DECAY}")
    print(f"Maskæ¯”ä¾‹:          {MLM_PROBABILITY}")
    print("-" * 80)
    print(f"ä½¿ç”¨é¢„è®­ç»ƒæƒé‡:    {USE_PRETRAINED_WEIGHTS}")
    print(f"è·³è¿‡æ ‡å‡†åŒ–:        {SKIP_NORMALIZATION}")
    print(f"è·³è¿‡tokenizerè®­ç»ƒ: {SKIP_TOKENIZER_TRAINING}")
    print(f"Early Stopè€å¿ƒå€¼:  {EARLY_STOPPING_PATIENCE}")
    print("=" * 80 + "\n")


def check_environment():
    """æ£€æŸ¥è¿è¡ŒçŽ¯å¢ƒ"""
    logger.info("ðŸ” æ£€æŸ¥è¿è¡ŒçŽ¯å¢ƒ...")

    # æ£€æŸ¥CUDA
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"è®­ç»ƒè®¾å¤‡: {device}")

    if torch.cuda.is_available():
        logger.info(f"GPUåž‹å·: {torch.cuda.get_device_name(0)}")
        logger.info(f"GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB")

    # æ£€æŸ¥æ–‡ä»¶
    if not Path(TRAIN_FILE).exists():
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {TRAIN_FILE}")
    if not Path(VAL_FILE).exists():
        raise FileNotFoundError(f"éªŒè¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {VAL_FILE}")

    logger.info(f"âœ“ è®­ç»ƒæ•°æ®: {TRAIN_FILE}")
    logger.info(f"âœ“ éªŒè¯æ•°æ®: {VAL_FILE}")

    # æ£€æŸ¥é¢„è®­ç»ƒæ¨¡åž‹
    if not Path(PRETRAINED_MODEL).exists():
        raise FileNotFoundError(f"é¢„è®­ç»ƒæ¨¡åž‹ä¸å­˜åœ¨: {PRETRAINED_MODEL}")
    logger.info(f"âœ“ é¢„è®­ç»ƒæ¨¡åž‹: {PRETRAINED_MODEL}")

    return device


def setup_directories():
    """åˆ›å»ºè¾“å‡ºç›®å½•"""
    output_dir = Path(OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)

    tokenizer_dir = output_dir / 'tokenizer'
    tokens_dir = output_dir / 'tokens'
    model_dir = output_dir / 'model_checkpoints'
    final_model_dir = output_dir / 'final_model'

    for d in [tokenizer_dir, tokens_dir, model_dir, final_model_dir]:
        d.mkdir(parents=True, exist_ok=True)

    logger.info(f"âœ“ è¾“å‡ºç›®å½•å·²åˆ›å»º: {output_dir}")

    return output_dir, tokenizer_dir, tokens_dir, model_dir, final_model_dir


def cleanup_memory():
    """æ¸…ç†å†…å­˜å’ŒGPUç¼“å­˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    logger.info("âœ“ å†…å­˜æ¸…ç†å®Œæˆ")


# ==================== æ ¸å¿ƒè®­ç»ƒå‡½æ•° ====================

from transformers import TrainerCallback
import json
import os


class TrainingHistoryCallback(TrainerCallback):
    def __init__(self, history_file):
        super().__init__()
        self.history_file = history_file
        os.makedirs(os.path.dirname(history_file), exist_ok=True)
        self._create_new_history()

    def _create_new_history(self):
        self.history = {
            'train_loss': [],
            'eval_loss': [],
            'learning_rates': [],
            'epochs': [],
            'steps': [],
            'train_logs': [],
            'eval_logs': [],
            'log_history': []
        }

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None:
            return

        # é¦–å…ˆä¿å­˜åˆ°å®Œæ•´æ—¥å¿—åŽ†å²
        self.history['log_history'].append(logs.copy())

        # è®°å½•æ‰€æœ‰ç±»åž‹çš„æ—¥å¿—
        if 'loss' in logs and 'eval_loss' not in logs:
            # è®­ç»ƒæ—¥å¿—
            train_log = logs.copy()
            self.history['train_logs'].append(train_log)
            self.history['train_loss'].append(train_log.get('loss'))

            # è®°å½•å­¦ä¹ çŽ‡
            if 'learning_rate' in logs:
                self.history['learning_rates'].append(logs['learning_rate'])

            # è®°å½•epoch
            if 'epoch' in logs:
                self.history['epochs'].append(logs['epoch'])

            # è®°å½•step
            if 'step' in logs:
                self.history['steps'].append(logs['step'])

        elif 'eval_loss' in logs:
            # è¯„ä¼°æ—¥å¿—
            eval_log = logs.copy()
            self.history['eval_logs'].append(eval_log)
            self.history['eval_loss'].append(eval_log['eval_loss'])

        self._save_history()

    def _save_history(self):
        try:
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(self.history, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"ä¿å­˜åŽ†å²æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def count_training_history_files(folder_path):
    """
    ç»Ÿè®¡æ–‡ä»¶å¤¹ä¸­åŒ…å«'training_history'çš„JSONæ–‡ä»¶æ•°é‡
    """
    pattern = os.path.join(folder_path, "*training_history*.json")
    matching_files = glob.glob(pattern)

    return len(matching_files)


training_history_num = count_training_history_files(OUTPUT_DIR)

# åˆ›å»ºè®­ç»ƒåŽ†å²æ–‡ä»¶è·¯å¾„
history_file = os.path.join(OUTPUT_DIR, f'training_history_{training_history_num}.json')
history_callback = TrainingHistoryCallback(history_file)


# ä¼˜åŒ–å™¨æ£€æŸ¥å›žè°ƒ
class OptimizerCheckCallback(TrainerCallback):
    def on_step_begin(self, args, state, control, **kwargs):
        """åœ¨è®­ç»ƒæ­¥éª¤å¼€å§‹æ—¶æ£€æŸ¥ä¼˜åŒ–å™¨ï¼ˆæ­¤æ—¶ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–ï¼‰"""
        trainer = kwargs.get('trainer')
        if trainer and hasattr(trainer, 'optimizer') and trainer.optimizer is not None:
            # åªåœ¨ç¬¬ä¸€æ¬¡æ­¥éª¤æ—¶æ‰“å°
            if state.global_step == 0:
                print("\n=== ä¼˜åŒ–å™¨çŠ¶æ€æ£€æŸ¥ ===")
                print("ä¼˜åŒ–å™¨å‚æ•°ç»„:")
                for i, param_group in enumerate(trainer.optimizer.param_groups):
                    print(f"  ç¬¬{i}ç»„ - å­¦ä¹ çŽ‡: {param_group['lr']}")
                print(f"ä¼˜åŒ–å™¨ç±»åž‹: {type(trainer.optimizer).__name__}")
                print("====================\n")


# è®­ç»ƒç›‘æŽ§å›žè°ƒ
class TrainingMonitorCallback(TrainerCallback):
    def on_evaluate(self, args, state, control, **kwargs):
        eval_results = kwargs.get('metrics', {})
        eval_loss = eval_results.get('eval_loss', float('inf'))


class ResumeTrainingCallback(TrainerCallback):
    def on_train_begin(self, args, state, control, **kwargs):
        # ä½¿ç”¨æ­£ç¡®çš„æ–¹å¼æ£€æŸ¥æ˜¯å¦ä»Žæ£€æŸ¥ç‚¹æ¢å¤
        if hasattr(state, 'resume_from_checkpoint') and state.resume_from_checkpoint is not None:
            print(f"ðŸ”„ ä»Žæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {state.resume_from_checkpoint}")
            print(f"ðŸ“Š æ¢å¤ä½ç½®: ç¬¬ {state.global_step} æ­¥, ç¬¬ {state.epoch:.2f} è½®")
        else:
            print("ðŸš€ å¼€å§‹æ–°çš„è®­ç»ƒ")




def normalize_text_file(input_file, output_file):
    """æ ‡å‡†åŒ–æ–‡æœ¬æ–‡ä»¶"""
    logger.info(f"æ­£åœ¨æ ‡å‡†åŒ– {input_file}...")

    normalizer = BertNormalizer(
        lowercase=False,
        strip_accents=True,
        clean_text=True,
        handle_chinese_chars=True
    )

    with open(input_file, 'r', encoding='utf-8') as f:
        texts = f.readlines()

    normalized_texts = [normalizer.normalize_str(text.strip()) for text in tqdm(texts)]

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(normalized_texts))

    logger.info(f"æ ‡å‡†åŒ–å®Œæˆï¼Œä¿å­˜åˆ° {output_file}")
    return output_file


def train_tokenizer(train_file, val_file, save_dir, vocab_size):
    """è®­ç»ƒæ–°çš„tokenizer"""
    logger.info("æ­£åœ¨è®­ç»ƒæ–°çš„tokenizer...")

    train_file_str = str(train_file)
    val_file_str = str(val_file)
    raw_datasets = load_dataset("text", data_files={"train": train_file_str, "val": val_file_str})
    logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆ: {raw_datasets}")

    def get_training_corpus():
        batch_size = 1000
        for i in range(0, len(raw_datasets["train"]), batch_size):
            yield raw_datasets["train"][i: i + batch_size]["text"]
        for i in range(0, len(raw_datasets["val"]), batch_size):
            yield raw_datasets["val"][i: i + batch_size]["text"]

    old_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL, cache_dir=CACHE_DIR)
    logger.info(f"åŽŸtokenizerè¯è¡¨å¤§å°: {len(old_tokenizer)}")

    training_corpus = get_training_corpus()
    tokenizer = old_tokenizer.train_new_from_iterator(
        text_iterator=training_corpus,
        vocab_size=vocab_size
    )

    logger.info(f"æ–°tokenizerè¯è¡¨å¤§å°: {len(tokenizer)}")
    tokenizer.save_pretrained(save_dir)
    logger.info(f"Tokenizerå·²ä¿å­˜åˆ° {save_dir}")

    return tokenizer


def tokenize_dataset(train_file, val_file, tokenizer_dir, save_dir, max_seq_length):
    """å¯¹æ•°æ®é›†è¿›è¡Œtokenization"""
    logger.info("æ­£åœ¨è¿›è¡Œtokenization...")

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)

    start_tok = tokenizer.convert_tokens_to_ids('[CLS]')
    sep_tok = tokenizer.convert_tokens_to_ids('[SEP]')

    def full_sent_tokenize(file_name):
        with open(file_name, 'r', encoding='utf-8') as f:
            sents = f.read().strip().split('\n')

        logger.info(f"å¤„ç† {len(sents)} ä¸ªå¥å­...")
        tok_sents = [tokenizer(s, padding=False, truncation=False)['input_ids']
                     for s in tqdm(sents, desc="Tokenizing")]

        for s in tok_sents:
            if len(s) > 0:
                s.pop(0)

        res = [[]]
        l_curr = 0

        for s in tok_sents:
            l_s = len(s)
            idx = 0
            while idx < l_s - 1:
                if l_curr == 0:
                    res[-1].append(start_tok)
                    l_curr = 1
                s_end = min(l_s, idx + max_seq_length - l_curr) - 1
                res[-1].extend(s[idx:s_end] + [sep_tok])
                idx = s_end
                if len(res[-1]) == max_seq_length:
                    res.append([])
                l_curr = len(res[-1])

        for s in res[:-1]:
            assert s[0] == start_tok and s[-1] == sep_tok
            assert len(s) == max_seq_length

        attention_mask = []
        for s in res:
            attention_mask.append([1] * len(s) + [0] * (max_seq_length - len(s)))

        return {'input_ids': res, 'attention_mask': attention_mask}

    import pandas as pd
    df_train = pd.DataFrame(full_sent_tokenize(train_file))
    df_val = pd.DataFrame(full_sent_tokenize(val_file))

    tokenized_datasets = DatasetDict({
        'train': Dataset.from_pandas(df_train),
        'val': Dataset.from_pandas(df_val)
    })

    logger.info(f"Tokenizedæ•°æ®é›†: {tokenized_datasets}")
    logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(tokenized_datasets['train'])}")
    logger.info(f"éªŒè¯æ ·æœ¬æ•°: {len(tokenized_datasets['val'])}")

    tokenized_datasets.save_to_disk(save_dir)
    logger.info(f"Tokenizedæ•°æ®é›†å·²ä¿å­˜åˆ° {save_dir}")

    return tokenized_datasets


def train_model(tokenizer_dir, tokens_dir, model_save_dir, final_save_dir):
    """è®­ç»ƒDeBERTaæ¨¡åž‹"""
    logger.info("æ­£åœ¨è®­ç»ƒæ¨¡åž‹...")

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)

    if USE_PRETRAINED_WEIGHTS:
        logger.info("ä½¿ç”¨é¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–æ¨¡åž‹ (æŽ¨èæ–¹å¼)")
        model = DebertaV2ForMaskedLM.from_pretrained(
            PRETRAINED_MODEL,
            cache_dir=CACHE_DIR,
        )
        model.resize_token_embeddings(len(tokenizer))
    else:
        logger.info("ä»Žå¤´è®­ç»ƒæ¨¡åž‹ (ä¸æŽ¨èï¼Œæ•°æ®é‡å¤ªå°)")
        config = AutoConfig.from_pretrained(PRETRAINED_MODEL, cache_dir=CACHE_DIR)
        model = DebertaV2ForMaskedLM(config=config)
        model.resize_token_embeddings(len(tokenizer))

    model_size = sum(t.numel() for t in model.parameters())
    logger.info(f"æ¨¡åž‹å‚æ•°é‡: {model_size / 1000 ** 2:.1f}M")

    dataset_train = Dataset.load_from_disk(Path(tokens_dir) / 'train')
    dataset_val = Dataset.load_from_disk(Path(tokens_dir) / 'val')
    logger.info(f"è®­ç»ƒé›†å¤§å°: {len(dataset_train)}, éªŒè¯é›†å¤§å°: {len(dataset_val)}")

    dataset_train.set_format(type='torch', columns=['input_ids'])
    dataset_val.set_format(type='torch', columns=['input_ids'])

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=True,
        mlm_probability=MLM_PROBABILITY
    )

    total_steps = (len(dataset_train) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NUM_EPOCHS
    warmup_steps = int(total_steps * WARMUP_RATIO)

    logger.info(f"æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
    logger.info(f"Warmupæ­¥æ•°: {warmup_steps}")
    logger.info(f"æœ‰æ•ˆbatch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")

    training_args = TrainingArguments(
        output_dir=model_save_dir,
        overwrite_output_dir=True,
        eval_strategy='steps',
        eval_steps=EVAL_STEPS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=EVAL_BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        adam_beta1=ADAM_BETA1,
        adam_beta2=ADAM_BETA2,
        adam_epsilon=ADAM_EPSILON,
        max_grad_norm=MAX_GRAD_NORM,
        gradient_checkpointing=GRADIENT_CHECKPOINTING,
        num_train_epochs=NUM_EPOCHS,
        lr_scheduler_type=LR_SCHEDULER_TYPE,
        warmup_ratio=WARMUP_RATIO,
        save_strategy='steps',
        save_steps=SAVE_STEPS,
        save_total_limit=SAVE_TOTAL_LIMIT,
        load_best_model_at_end=True,
        metric_for_best_model='eval_loss',
        greater_is_better=False,
        logging_strategy='steps',
        logging_steps=LOGGING_STEPS,
        logging_first_step=True,
        seed=SEED,
        data_seed=SEED,
        fp16=False,
        optim='adamw_torch',
        report_to='none',
        disable_tqdm=True,
    )

    early_stopping = EarlyStoppingCallback(
        early_stopping_patience=EARLY_STOPPING_PATIENCE,
        early_stopping_threshold=EARLY_STOPPING_THRESHOLD
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=dataset_train,
        eval_dataset=dataset_val,
        tokenizer=tokenizer,
        callbacks=[early_stopping,
               history_callback,
               OptimizerCheckCallback(),
               TrainingMonitorCallback(),
               ResumeTrainingCallback()],
    )

    logger.info("=" * 80)
    logger.info("å¼€å§‹è®­ç»ƒ...")
    logger.info("=" * 80)

    result = trainer.train()

    logger.info(f"\nè®­ç»ƒå®Œæˆ!")
    logger.info(f"è®­ç»ƒæ—¶é—´: {result.metrics['train_runtime']:.2f}ç§’")
    logger.info(f"æ ·æœ¬/ç§’: {result.metrics['train_samples_per_second']:.2f}")

    trainer.save_model(final_save_dir)
    tokenizer.save_pretrained(final_save_dir)
    logger.info(f"æœ€ç»ˆæ¨¡åž‹å·²ä¿å­˜åˆ° {final_save_dir}")

    logger.info("=" * 80)
    logger.info("æœ€ç»ˆè¯„ä¼°...")
    logger.info("=" * 80)
    eval_results = trainer.evaluate()

    logger.info(f"\næœ€ç»ˆè¯„ä¼°ç»“æžœ:")
    for key, value in eval_results.items():
        logger.info(f"  {key}: {value:.4f}")

    with open(final_save_dir / 'eval_results.json', 'w') as f:
        json.dump(eval_results, f, indent=2)

    return eval_results


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    start_time = time.time()

    print("\n" + "=" * 80)
    print("DeBERTaé«˜ç†µåˆé‡‘æ¨¡åž‹è®­ç»ƒ")
    print("=" * 80)

    # 1. è®¾ç½®éšæœºç§å­
    set_seed(SEED)
    logger.info(f"éšæœºç§å­è®¾ç½®ä¸º: {SEED}")

    # 2. æ‰“å°é…ç½®
    print_config()

    # 3. çŽ¯å¢ƒæ£€æŸ¥
    device = check_environment()

    # 4. åˆ›å»ºç›®å½•
    output_dir, tokenizer_dir, tokens_dir, model_dir, final_model_dir = setup_directories()

    # 5. æ¸…ç†å†…å­˜
    cleanup_memory()

    # 6. æ–‡æœ¬æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼‰
    if not SKIP_NORMALIZATION:
        train_norm_file = output_dir / 'train_normalized.txt'
        val_norm_file = output_dir / 'val_normalized.txt'

        normalize_text_file(TRAIN_FILE, train_norm_file)
        normalize_text_file(VAL_FILE, val_norm_file)

        train_file = train_norm_file
        val_file = val_norm_file
    else:
        logger.info("è·³è¿‡æ–‡æœ¬æ ‡å‡†åŒ–æ­¥éª¤")
        train_file = TRAIN_FILE
        val_file = VAL_FILE

    # 7. Tokenizerå¤„ç†
    if not SKIP_TOKENIZER_TRAINING:
        train_tokenizer(train_file, val_file, tokenizer_dir, VOCAB_SIZE)
    else:
        logger.info("ä½¿ç”¨é¢„è®­ç»ƒtokenizer")
        tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL, cache_dir=CACHE_DIR)
        tokenizer.save_pretrained(tokenizer_dir)

    # 8. Tokenization
    tokenize_dataset(train_file, val_file, tokenizer_dir, tokens_dir, MAX_SEQ_LENGTH)

    # 9. è®­ç»ƒæ¨¡åž‹
    eval_results = train_model(tokenizer_dir, tokens_dir, model_dir, final_model_dir)

    # 10. è®­ç»ƒå®Œæˆ
    end_time = time.time()
    total_seconds = end_time - start_time
    hours = int(total_seconds // 3600)
    minutes = int((total_seconds % 3600) // 60)
    seconds = total_seconds % 60

    print("\n" + "=" * 80)
    print("âœ… è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆ!")
    print("=" * 80)
    print(f"æ€»è¿è¡Œæ—¶é—´: {hours}å°æ—¶ {minutes}åˆ†é’Ÿ {seconds:.2f}ç§’")
    print(f"æœ€ç»ˆæ¨¡åž‹ä¿å­˜ä½ç½®: {final_model_dir}")
    print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {eval_results.get('eval_loss', 'N/A'):.4f}")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()