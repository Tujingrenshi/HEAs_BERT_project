{
  "train_loss": [
    10.9566,
    10.9637,
    10.9242,
    10.8563,
    10.7484,
    10.607,
    10.4584,
    10.3247,
    10.2045,
    10.0877,
    10.0033,
    9.9431,
    9.8636,
    9.8248,
    9.7678,
    9.7153,
    9.6619,
    9.6319,
    9.585,
    9.5446,
    9.5013,
    9.44,
    9.4015,
    9.357,
    9.3083,
    9.2621,
    9.2048,
    9.1319,
    9.0888,
    9.0645,
    8.988,
    8.9536,
    8.901,
    8.8035,
    8.7757,
    8.716,
    8.6659,
    8.5815,
    8.5174,
    8.4882,
    8.4066,
    8.3516,
    8.2808,
    8.2244,
    8.1779,
    8.1066,
    8.0674,
    7.9867,
    7.9411,
    7.8694,
    7.8063,
    7.7454,
    7.7271,
    7.6654,
    7.6028,
    7.5719,
    7.5314,
    7.462,
    7.4419,
    7.4076,
    7.4028,
    7.3582,
    7.2983,
    7.285,
    7.2676,
    7.2364,
    7.2037,
    7.18,
    7.1825,
    7.1655,
    7.109,
    7.1194,
    7.0859,
    7.0664,
    7.0763,
    7.0538,
    7.0181,
    6.999,
    6.9614,
    6.9383,
    6.9177,
    6.893,
    6.8593,
    6.8293,
    6.8214,
    6.7789,
    6.7782,
    6.7434,
    6.7036,
    6.6855,
    6.663,
    6.6692,
    6.6114,
    6.6032,
    6.6078,
    6.5359,
    6.5204,
    6.5034,
    6.5231,
    6.4598,
    6.4462,
    6.4307,
    6.4215,
    6.3845,
    6.373,
    6.3393,
    6.3159,
    6.3122,
    6.2616,
    6.2657,
    6.2483,
    6.2443,
    6.2253,
    6.2131,
    6.1756,
    6.1628,
    6.1509,
    6.1052,
    6.0775,
    6.0906,
    6.0672,
    6.0704,
    6.0438,
    6.0168,
    6.0195,
    5.9736,
    5.9814,
    5.971,
    5.9575,
    5.9475,
    5.9097,
    5.923,
    5.8914,
    5.8703,
    5.8798,
    5.8248,
    5.8148,
    5.823,
    5.8022,
    5.7439,
    5.8003,
    5.7476,
    5.7274,
    5.7334,
    5.6997,
    5.6878,
    5.6668,
    5.6633,
    5.6324,
    5.6224,
    5.6523,
    5.6421,
    5.6258,
    5.5907,
    5.5878,
    5.5442,
    5.5272,
    5.5382,
    5.5114,
    5.4866,
    5.4763,
    5.4455,
    5.4555,
    5.4657,
    5.4424,
    5.4505,
    5.4305,
    5.3949,
    5.3673,
    5.3688,
    5.3647,
    5.364,
    5.3046,
    5.3435,
    5.3101,
    5.258,
    5.2932,
    5.2822,
    5.246,
    5.2505,
    5.2314,
    5.2521,
    5.1649,
    5.2003,
    5.1765,
    5.1793,
    5.1649,
    5.1523,
    5.174,
    5.0915,
    5.1216,
    5.1102,
    5.0831,
    5.1148,
    5.0897,
    5.0725,
    5.0691,
    5.0325,
    5.024,
    4.9935,
    4.991,
    5.0076,
    4.9923,
    4.98,
    4.9648,
    4.9362,
    4.9245,
    4.9339,
    4.9294,
    4.9167,
    4.8912,
    4.8924,
    4.8747,
    4.888,
    4.8749,
    4.8883,
    4.8599,
    4.8099,
    4.8229,
    4.8226,
    4.8103,
    4.7602,
    4.7722,
    4.7811,
    4.7656,
    4.7306,
    4.7215,
    4.7437,
    4.7055,
    4.7336,
    4.7017,
    4.7151,
    4.6865,
    4.6668,
    4.6016,
    4.6664,
    4.6324,
    4.637,
    4.6472,
    4.6046,
    4.6078,
    4.5704,
    4.5874,
    4.6008,
    4.591,
    4.5503,
    4.5631,
    4.5501,
    4.5443,
    4.5519,
    4.5067,
    4.4879,
    4.4871,
    4.5046,
    4.483,
    4.4821,
    4.4594,
    4.4595,
    4.4443,
    4.48,
    4.4306,
    4.4462,
    4.4755,
    4.4005,
    4.4086,
    4.361,
    4.3933,
    4.3772,
    4.349,
    4.3928,
    4.3824,
    4.3262,
    4.3347,
    4.3795,
    4.3074,
    4.2926,
    4.2922,
    4.2843,
    4.2761,
    4.2782,
    4.2266,
    4.2641,
    4.266,
    4.2599,
    4.2743,
    4.2206,
    4.2428,
    4.2253,
    4.2378,
    4.213,
    4.1813,
    4.2236,
    4.206,
    4.1684,
    4.151,
    4.1365,
    4.1679,
    4.1295,
    4.1643,
    4.1413,
    4.1437,
    4.1395,
    4.1002,
    4.1018,
    4.1104,
    4.0903,
    4.101,
    4.0654,
    4.0643,
    4.0954,
    4.0769,
    4.0907,
    4.0047,
    4.0249,
    4.0378,
    4.0591,
    4.0087,
    4.0401,
    3.9866,
    4.0074,
    3.9988,
    3.9721,
    4.0282,
    3.9959,
    3.9968,
    3.9536,
    3.942,
    3.946,
    3.9567,
    3.9451,
    3.9509,
    3.9256,
    3.9313,
    3.9203,
    3.8963,
    3.9136,
    3.8807,
    3.9076,
    3.8835,
    3.8797,
    3.8614,
    3.903,
    3.8487,
    3.8679,
    3.8592,
    3.8392,
    3.8417,
    3.8357,
    3.8196,
    3.825,
    3.8176,
    3.8239,
    3.7923,
    3.8137,
    3.8083,
    3.7905,
    3.792,
    3.7625,
    3.7685,
    3.7701,
    3.7475,
    3.7379,
    3.7271,
    3.7502,
    3.6948,
    3.7248,
    3.7375,
    3.7327,
    3.701,
    3.71,
    3.7269,
    3.7414,
    3.703,
    3.7067,
    3.7285,
    3.621,
    3.6575,
    3.6531,
    3.6752,
    3.6596,
    3.6508,
    3.6315,
    3.6461,
    3.6779,
    3.6393,
    3.6548,
    3.6234,
    3.5941,
    3.6423,
    3.5772,
    3.6178,
    3.59,
    3.5963,
    3.5872,
    3.5827,
    3.5769,
    3.5958,
    3.5752,
    3.6035,
    3.5436,
    3.5683,
    3.5442,
    3.5863,
    3.5459,
    3.519,
    3.5545,
    3.5505,
    3.5551,
    3.5307,
    3.5328,
    3.4986,
    3.4889,
    3.5346,
    3.5707,
    3.4831,
    3.5075,
    3.4858,
    3.4922,
    3.4934,
    3.5015,
    3.4793,
    3.4815,
    3.4601,
    3.4573,
    3.4822,
    3.4779,
    3.4461,
    3.4464,
    3.4346,
    3.4337,
    3.4462,
    3.4644,
    3.4088,
    3.4368,
    3.4141,
    3.4231,
    3.4073,
    3.3959,
    3.4117,
    3.3969,
    3.4325,
    3.4361,
    3.3894,
    3.3752,
    3.3783,
    3.3644,
    3.3712,
    3.3592,
    3.3833,
    3.3384,
    3.3431,
    3.3364,
    3.3657,
    3.3439,
    3.3777,
    3.3585,
    3.3502,
    3.3232,
    3.3352,
    3.3275,
    3.3407,
    3.3054,
    3.323,
    3.3067,
    3.302,
    3.33,
    3.3251,
    3.3062,
    3.3245,
    3.3125,
    3.2885,
    3.2823,
    3.279,
    3.2571,
    3.2656,
    3.3084,
    3.297,
    3.2803,
    3.2694,
    3.2572,
    3.2308,
    3.2967,
    3.2299,
    3.2809,
    3.2196,
    3.2321,
    3.2553,
    3.2868,
    3.2417,
    3.2264,
    3.2344,
    3.208,
    3.2393,
    3.2341,
    3.2153,
    3.2445,
    3.1928,
    3.2157,
    3.1989,
    3.2282,
    3.1967,
    3.1825,
    3.1692,
    3.1827,
    3.1824,
    3.1964,
    3.2003,
    3.2054,
    3.1776,
    3.1892,
    3.1835,
    3.1524,
    3.2106,
    3.1843,
    3.1477,
    3.1471,
    3.1571,
    3.1543,
    3.137,
    3.1514,
    3.1311,
    3.149,
    3.137,
    3.1488,
    3.1164,
    3.1264,
    3.1427,
    3.1234,
    3.1065,
    3.1319,
    3.1167,
    3.125,
    3.072,
    3.0959,
    3.1229,
    3.0843,
    3.0978,
    3.1013,
    3.1131,
    3.1144,
    3.0812,
    3.1019,
    3.1112,
    3.0666,
    3.0545,
    3.101,
    3.0856,
    3.0661,
    3.0771,
    3.0534,
    3.0449,
    3.098,
    3.0636,
    3.0824,
    3.0852,
    3.0497,
    3.018,
    3.0578,
    3.028,
    3.0403,
    3.04,
    3.0379,
    3.0639,
    3.0707,
    3.0475,
    3.025,
    3.0305,
    3.0352,
    2.9999,
    3.0492,
    2.9868,
    3.0217,
    3.0159,
    3.0332,
    3.0296,
    2.9877,
    3.015,
    3.0351,
    3.0419,
    2.9759,
    2.9831,
    3.0037,
    2.9989,
    2.9946,
    3.0072,
    3.0112,
    3.0084,
    3.0229,
    2.9931,
    2.9668,
    2.9706,
    2.9612,
    2.9885,
    2.9756,
    2.9577,
    2.9612,
    2.9911,
    2.983,
    2.9607,
    2.9677,
    2.9452,
    2.9639,
    2.9311,
    2.9562,
    2.9429,
    2.9494,
    2.9022,
    2.9474,
    2.9657,
    2.9246,
    2.9424,
    2.9518,
    2.9389,
    2.927,
    2.9042,
    2.9466,
    2.9128,
    2.9199,
    2.9169,
    2.9109,
    2.9377,
    2.9109,
    2.9463,
    2.8894,
    2.9117,
    2.9209,
    2.9052,
    2.913,
    2.8999,
    2.9056,
    2.9129,
    2.901,
    2.8739,
    2.8857,
    2.9151,
    2.8962,
    2.8675,
    2.8887,
    2.9056,
    2.8693,
    2.8791,
    2.902,
    2.8969,
    2.8857,
    2.8532,
    2.8677,
    2.8637,
    2.8688,
    2.8497,
    2.8582,
    2.8607,
    2.8806,
    2.8542,
    2.8398,
    2.838,
    2.888,
    2.8591,
    2.829,
    2.8286,
    2.8595,
    2.8573,
    2.8553,
    2.8685,
    2.855,
    2.815,
    2.8352,
    2.8291,
    2.8014,
    2.8328,
    2.8268,
    2.8104,
    2.8472,
    2.8436,
    2.8534,
    2.8446,
    2.8304,
    2.831,
    2.7823,
    2.8286,
    2.8347,
    2.8089,
    2.7908,
    2.8219,
    2.7944,
    2.791,
    2.8304,
    2.8382,
    2.8108,
    2.8111,
    2.758,
    2.8052,
    2.7832,
    2.7928,
    2.8055,
    2.784,
    2.7915,
    2.8192,
    2.8045,
    2.7579,
    2.8001,
    2.7777,
    2.7602,
    2.7924,
    2.7603,
    2.7478,
    2.8009,
    2.7902,
    2.7765,
    2.7611,
    2.771,
    2.7796,
    2.7761,
    2.7476,
    2.7797,
    2.7824,
    2.7609,
    2.8021,
    2.7381,
    2.77,
    2.7742,
    2.7711,
    2.7545,
    2.7444,
    2.7507,
    2.7701,
    2.7417,
    2.7274,
    2.7439,
    2.7317,
    2.7415,
    2.7502,
    2.7176,
    2.7513,
    2.773,
    2.7524,
    2.7353,
    2.7475,
    2.7046,
    2.7286,
    2.7313,
    2.7436,
    2.7261,
    2.7474,
    2.7292,
    2.7261,
    2.7167,
    2.7465,
    2.724,
    2.7267,
    2.7329,
    2.7053,
    2.7036,
    2.7241,
    2.7029,
    2.7255,
    2.7178,
    2.7292,
    2.696,
    2.6884,
    2.6926,
    2.7144,
    2.684,
    2.6995,
    2.7057,
    2.7207,
    2.7129,
    2.6971,
    2.69,
    2.6991,
    2.6995,
    2.6711,
    2.6921,
    2.6912,
    2.6823,
    2.6695,
    2.678,
    2.6977,
    2.6809,
    2.7065,
    2.6738,
    2.6674,
    2.6734,
    2.7016,
    2.6665,
    2.6844,
    2.6938,
    2.6807,
    2.6849,
    2.6688,
    2.645,
    2.6993,
    2.6621,
    2.6845,
    2.6449,
    2.6712,
    2.6589,
    2.6499,
    2.6486,
    2.6436,
    2.6483,
    2.664,
    2.6328,
    2.6441,
    2.6513,
    2.6324,
    2.6648,
    2.6361,
    2.6715,
    2.6708,
    2.6401,
    2.6452,
    2.6622,
    2.6268,
    2.637,
    2.6417,
    2.6363,
    2.6538,
    2.6573,
    2.6669,
    2.621,
    2.6234,
    2.6609,
    2.6154,
    2.6162,
    2.6405,
    2.6547,
    2.6571,
    2.6388,
    2.6236,
    2.6115,
    2.6387,
    2.6409,
    2.6406,
    2.6251,
    2.6327,
    2.6242,
    2.6243,
    2.6257,
    2.6287,
    2.6333,
    2.6472,
    2.6138,
    2.6346,
    2.587,
    2.6129,
    2.6231,
    2.5902,
    2.6142,
    2.6554,
    2.6111,
    2.6021,
    2.6067,
    2.6034,
    2.6451,
    2.6164,
    2.6074,
    2.609,
    2.6114,
    2.5955,
    2.6085,
    2.5863,
    2.5996,
    2.5989,
    2.5898,
    2.6288,
    2.5986,
    2.6196,
    2.6141,
    2.6112,
    2.5973,
    2.5904,
    2.5859,
    2.5901,
    2.6015,
    2.6026,
    2.6027,
    2.6174,
    2.5799,
    2.5796,
    2.6234,
    2.6032,
    2.5752,
    2.597,
    2.5878,
    2.604,
    2.5939,
    2.5962,
    2.5922,
    2.5999,
    2.602,
    2.5758,
    2.5635,
    2.5918,
    2.5581,
    2.5736,
    2.5812,
    2.5737,
    2.6104,
    2.6017,
    2.6089,
    2.5666,
    2.5485,
    2.5852,
    2.6081,
    2.5738,
    2.5678,
    2.5715,
    2.5692,
    2.5867,
    2.5754,
    2.5662,
    2.5829,
    2.568,
    2.5509,
    2.5782,
    2.5496,
    2.5467,
    2.5809,
    2.5588,
    2.5844,
    2.5616,
    2.5573,
    2.5599,
    2.5614,
    2.5712,
    2.5555,
    2.516,
    2.5685,
    2.5838,
    2.5273,
    2.5391,
    2.5576,
    2.5501,
    2.5576,
    2.5489,
    2.5821,
    2.5591,
    2.5536,
    2.5489,
    2.5621,
    2.5577,
    2.566,
    2.5294,
    2.5621,
    2.549,
    2.5448,
    2.5431,
    2.5318,
    2.5685,
    2.5404,
    2.5563,
    2.5465,
    2.5696,
    2.5346,
    2.5511,
    2.5794,
    2.5409,
    2.5469,
    2.5527,
    2.5361,
    2.5317,
    2.5613,
    2.5662,
    2.5291,
    2.5424,
    2.5224,
    2.5307,
    2.5392,
    2.5442,
    2.5504,
    2.5605,
    2.5844,
    2.5446,
    2.5889,
    2.5467,
    2.5729,
    2.5173,
    2.5228,
    2.5195,
    2.5123,
    2.5303,
    2.532,
    2.5189,
    2.5408,
    2.5327
  ],
  "eval_loss": [
    10.879128456115723,
    10.635735511779785,
    10.325429916381836,
    10.083735466003418,
    9.907322883605957,
    9.785597801208496,
    9.686332702636719,
    9.582322120666504,
    9.494952201843262,
    9.412392616271973,
    9.317459106445312,
    9.217268943786621,
    9.122360229492188,
    9.00854778289795,
    8.909120559692383,
    8.78382682800293,
    8.682120323181152,
    8.56091594696045,
    8.438775062561035,
    8.322005271911621,
    8.196805953979492,
    8.092820167541504,
    7.970271110534668,
    7.851957321166992,
    7.731200218200684,
    7.646726131439209,
    7.554970741271973,
    7.4625563621521,
    7.393415927886963,
    7.3229217529296875,
    7.268559455871582,
    7.210257053375244,
    7.1743483543396,
    7.118061542510986,
    7.089191436767578,
    7.05344820022583,
    7.0124711990356445,
    6.956892967224121,
    6.893344402313232,
    6.864377498626709,
    6.800537586212158,
    6.749076843261719,
    6.702304840087891,
    6.638909816741943,
    6.585209846496582,
    6.540661334991455,
    6.469175338745117,
    6.448359966278076,
    6.388474464416504,
    6.346341133117676,
    6.304312705993652,
    6.261873245239258,
    6.2202324867248535,
    6.189189434051514,
    6.157534122467041,
    6.109443187713623,
    6.08407735824585,
    6.051288604736328,
    6.00562047958374,
    5.955612659454346,
    5.942311763763428,
    5.916191101074219,
    5.8641157150268555,
    5.8277058601379395,
    5.816876411437988,
    5.7909932136535645,
    5.760439872741699,
    5.711305141448975,
    5.695173263549805,
    5.657410144805908,
    5.629942893981934,
    5.583545684814453,
    5.568991184234619,
    5.542919635772705,
    5.517384052276611,
    5.4776506423950195,
    5.460651874542236,
    5.443472385406494,
    5.423609256744385,
    5.38193941116333,
    5.357533931732178,
    5.334133625030518,
    5.293779373168945,
    5.283754348754883,
    5.2625651359558105,
    5.207681655883789,
    5.209364891052246,
    5.183217525482178,
    5.160017013549805,
    5.144285678863525,
    5.106461524963379,
    5.09509801864624,
    5.080765247344971,
    5.038093090057373,
    5.012759685516357,
    5.004238605499268,
    4.994377613067627,
    4.970526695251465,
    4.925838470458984,
    4.9372735023498535,
    4.886546611785889,
    4.8834357261657715,
    4.848888397216797,
    4.848678112030029,
    4.827264308929443,
    4.796995639801025,
    4.76686429977417,
    4.754540920257568,
    4.734476089477539,
    4.730249404907227,
    4.713449954986572,
    4.6951093673706055,
    4.664693355560303,
    4.652712345123291,
    4.634620189666748,
    4.61090612411499,
    4.596757888793945,
    4.58631706237793,
    4.557436943054199,
    4.532100677490234,
    4.524337291717529,
    4.499527931213379,
    4.493046760559082,
    4.4688825607299805,
    4.452094078063965,
    4.441656112670898,
    4.434301376342773,
    4.410249710083008,
    4.389793395996094,
    4.386097431182861,
    4.369365215301514,
    4.351761341094971,
    4.321926593780518,
    4.320221424102783,
    4.302127361297607,
    4.27600622177124,
    4.26438570022583,
    4.265533924102783,
    4.254349231719971,
    4.220759391784668,
    4.222668647766113,
    4.185118675231934,
    4.180415153503418,
    4.173740863800049,
    4.152820110321045,
    4.138526916503906,
    4.115542888641357,
    4.125391483306885,
    4.115341663360596,
    4.07948637008667,
    4.091250896453857,
    4.061549186706543,
    4.060634613037109,
    4.0503926277160645,
    4.029573440551758,
    4.021984577178955,
    3.9922399520874023,
    3.9869275093078613,
    3.9877068996429443,
    3.9678125381469727,
    3.9659388065338135,
    3.936940908432007,
    3.933619737625122,
    3.935817003250122,
    3.910212993621826,
    3.905930280685425,
    3.883333683013916,
    3.872774839401245,
    3.868746519088745,
    3.8724300861358643,
    3.8509654998779297,
    3.8216724395751953,
    3.8185088634490967,
    3.822977066040039,
    3.7919721603393555,
    3.794698715209961,
    3.7908804416656494,
    3.7672083377838135,
    3.7560667991638184,
    3.7604968547821045,
    3.7441766262054443,
    3.7156827449798584,
    3.7316970825195312,
    3.6949868202209473,
    3.6969614028930664,
    3.694748878479004,
    3.6815686225891113,
    3.667508840560913,
    3.657566785812378,
    3.6693782806396484,
    3.6473681926727295,
    3.639246702194214,
    3.6180613040924072,
    3.6100614070892334,
    3.6150455474853516,
    3.6033637523651123,
    3.5966522693634033,
    3.5793328285217285,
    3.571507453918457,
    3.5554604530334473,
    3.562905788421631,
    3.5444228649139404,
    3.553972005844116,
    3.5422511100769043,
    3.5425634384155273,
    3.5281667709350586,
    3.5268945693969727,
    3.5163333415985107,
    3.4883246421813965,
    3.4946136474609375,
    3.502460241317749,
    3.486471652984619,
    3.469921350479126,
    3.47245192527771,
    3.466874361038208,
    3.452944040298462,
    3.4550275802612305,
    3.4616622924804688,
    3.435023069381714,
    3.4266960620880127,
    3.431136131286621,
    3.4136292934417725,
    3.414602756500244,
    3.3960444927215576,
    3.418092727661133,
    3.378563642501831,
    3.3861939907073975,
    3.3811521530151367,
    3.37243914604187,
    3.3724560737609863,
    3.3612220287323,
    3.3640027046203613,
    3.3507111072540283,
    3.3498647212982178,
    3.349191427230835,
    3.3258252143859863,
    3.322061777114868,
    3.325988531112671,
    3.3308424949645996,
    3.3181838989257812,
    3.3137474060058594,
    3.3190197944641113,
    3.2879443168640137,
    3.295941114425659,
    3.276482105255127,
    3.2859244346618652,
    3.274113655090332,
    3.256075620651245,
    3.262702703475952,
    3.2767136096954346,
    3.2513697147369385,
    3.2618753910064697,
    3.2425596714019775,
    3.2356631755828857,
    3.2405736446380615,
    3.2461225986480713,
    3.230443239212036,
    3.219569206237793,
    3.2099087238311768,
    3.223168134689331,
    3.2046031951904297,
    3.1903035640716553,
    3.2141690254211426,
    3.2099123001098633,
    3.1888222694396973,
    3.1834030151367188,
    3.1851277351379395,
    3.1820638179779053,
    3.1703684329986572,
    3.1656787395477295,
    3.1679763793945312,
    3.137481451034546,
    3.166499137878418,
    3.1540400981903076,
    3.145939826965332,
    3.1480653285980225,
    3.141754627227783,
    3.1397643089294434,
    3.1347315311431885,
    3.135179281234741,
    3.1284148693084717,
    3.134147882461548,
    3.128800630569458,
    3.129486083984375,
    3.099191188812256,
    3.114976167678833,
    3.1146786212921143,
    3.09824800491333,
    3.0991084575653076,
    3.0816140174865723,
    3.091029167175293,
    3.087475299835205,
    3.0860629081726074,
    3.071071147918701,
    3.0664379596710205,
    3.0687315464019775,
    3.048915147781372,
    3.0574257373809814,
    3.068362236022949,
    3.0626111030578613,
    3.068281888961792,
    3.048734664916992,
    3.0394341945648193,
    3.0622029304504395,
    3.0330963134765625,
    3.030937910079956,
    3.044794797897339,
    3.0288426876068115,
    3.020921468734741,
    3.0287253856658936,
    3.0280721187591553,
    3.0278849601745605,
    3.017292022705078,
    3.0350613594055176,
    3.0060997009277344,
    3.004136800765991,
    3.0087156295776367,
    3.0131402015686035,
    2.9874255657196045,
    2.9942455291748047,
    2.989356279373169,
    2.986487627029419,
    2.9830424785614014,
    2.985645055770874,
    2.988900899887085,
    2.980658531188965,
    2.9645206928253174,
    2.9654603004455566,
    2.9718544483184814,
    2.9673385620117188,
    2.9828858375549316,
    2.9606502056121826,
    2.9615843296051025,
    2.9554615020751953,
    2.9644298553466797,
    2.9586052894592285,
    2.9357962608337402,
    2.9519405364990234,
    2.948415994644165,
    2.944415330886841,
    2.9392316341400146,
    2.9452617168426514,
    2.9350993633270264,
    2.9408175945281982,
    2.9472968578338623,
    2.928577423095703,
    2.92002010345459,
    2.919623374938965,
    2.926647424697876,
    2.918654203414917,
    2.911079168319702,
    2.9151082038879395,
    2.92311429977417,
    2.8994140625,
    2.898406744003296,
    2.909198760986328,
    2.9260153770446777,
    2.914180278778076,
    2.9286255836486816,
    2.9006969928741455,
    2.9103360176086426,
    2.90128231048584,
    2.90374493598938,
    2.87467360496521,
    2.884253740310669,
    2.876120090484619,
    2.890573024749756,
    2.8625218868255615,
    2.8707284927368164,
    2.8870952129364014,
    2.8803999423980713,
    2.8675715923309326,
    2.87373948097229,
    2.8703103065490723,
    2.8625564575195312,
    2.8645052909851074,
    2.877948760986328,
    2.8776817321777344,
    2.854315757751465,
    2.858328104019165,
    2.857604742050171,
    2.863326072692871,
    2.8597261905670166,
    2.863450527191162,
    2.8526246547698975,
    2.847461462020874,
    2.84598708152771,
    2.851780652999878,
    2.857077121734619,
    2.8360917568206787,
    2.8438503742218018,
    2.8502748012542725,
    2.844392776489258,
    2.852677583694458,
    2.8355860710144043,
    2.8268582820892334,
    2.837480068206787,
    2.835357427597046,
    2.836359977722168,
    2.832552909851074,
    2.834019422531128,
    2.8283779621124268,
    2.814612627029419,
    2.8218207359313965,
    2.8187050819396973,
    2.822303533554077,
    2.8169522285461426,
    2.8173396587371826,
    2.798412322998047,
    2.805098295211792,
    2.8122010231018066,
    2.8020834922790527,
    2.80495023727417,
    2.8130273818969727,
    2.8064723014831543,
    2.7963640689849854,
    2.807535171508789,
    2.79221510887146,
    2.795353651046753,
    2.792280435562134,
    2.7935268878936768,
    2.7966244220733643,
    2.805509567260742,
    2.7834789752960205,
    2.7995612621307373,
    2.80234432220459,
    2.7835021018981934,
    2.791728973388672,
    2.7878260612487793,
    2.792365550994873,
    2.7806942462921143,
    2.777536630630493,
    2.7781546115875244,
    2.7832276821136475,
    2.775618314743042,
    2.791391611099243,
    2.770627975463867,
    2.7644472122192383,
    2.798290252685547,
    2.769012451171875,
    2.7822704315185547,
    2.7691400051116943,
    2.7629706859588623,
    2.7559871673583984,
    2.7634212970733643,
    2.767535448074341,
    2.7696590423583984,
    2.7650465965270996,
    2.7782602310180664,
    2.7736215591430664,
    2.766679525375366,
    2.768174171447754,
    2.765151023864746,
    2.7482223510742188,
    2.750168800354004,
    2.7595765590667725,
    2.7550623416900635,
    2.756676197052002,
    2.762590169906616,
    2.75960111618042,
    2.7718472480773926,
    2.7385473251342773,
    2.7447195053100586,
    2.7447822093963623,
    2.759553909301758,
    2.7610840797424316,
    2.747469902038574,
    2.762399435043335,
    2.7355268001556396,
    2.749480724334717,
    2.742067337036133,
    2.7442996501922607,
    2.7463793754577637,
    2.757061719894409,
    2.7360684871673584,
    2.7462432384490967,
    2.7301025390625,
    2.7538321018218994,
    2.7451651096343994,
    2.739567995071411,
    2.732771396636963,
    2.731144666671753,
    2.737589120864868,
    2.73954701423645,
    2.7315926551818848,
    2.743705987930298,
    2.742995023727417,
    2.731686592102051,
    2.7307310104370117,
    2.7427139282226562,
    2.7369229793548584,
    2.7358248233795166,
    2.7365987300872803
  ],
  "learning_rates": [
    0.0,
    5.417357656163627e-08,
    1.0945273631840797e-07,
    1.6473189607517967e-07,
    2.2001105583195135e-07,
    2.752902155887231e-07,
    3.3056937534549477e-07,
    3.8584853510226653e-07,
    4.411276948590382e-07,
    4.964068546158099e-07,
    5.516860143725816e-07,
    6.069651741293533e-07,
    6.62244333886125e-07,
    7.175234936428966e-07,
    7.728026533996684e-07,
    8.280818131564402e-07,
    8.833609729132118e-07,
    9.386401326699835e-07,
    9.939192924267551e-07,
    1.049198452183527e-06,
    1.1044776119402987e-06,
    1.1597567716970703e-06,
    1.215035931453842e-06,
    1.2703150912106138e-06,
    1.3255942509673855e-06,
    1.380873410724157e-06,
    1.4361525704809288e-06,
    1.4914317302377005e-06,
    1.5467108899944721e-06,
    1.601990049751244e-06,
    1.6572692095080156e-06,
    1.7125483692647873e-06,
    1.7678275290215592e-06,
    1.8231066887783308e-06,
    1.8783858485351025e-06,
    1.9336650082918743e-06,
    1.988944168048646e-06,
    2.0442233278054172e-06,
    2.099502487562189e-06,
    2.154781647318961e-06,
    2.2100608070757324e-06,
    2.2653399668325043e-06,
    2.320619126589276e-06,
    2.3758982863460476e-06,
    2.4311774461028195e-06,
    2.4864566058595913e-06,
    2.5417357656163628e-06,
    2.5970149253731346e-06,
    2.652294085129906e-06,
    2.707573244886678e-06,
    2.7628524046434494e-06,
    2.8181315644002217e-06,
    2.873410724156993e-06,
    2.9286898839137646e-06,
    2.9839690436705365e-06,
    3.039248203427308e-06,
    3.09452736318408e-06,
    3.1498065229408516e-06,
    3.2050856826976235e-06,
    3.260364842454395e-06,
    3.315644002211167e-06,
    3.3709231619679383e-06,
    3.4262023217247097e-06,
    3.481481481481482e-06,
    3.5367606412382534e-06,
    3.5920398009950253e-06,
    3.6473189607517968e-06,
    3.7025981205085686e-06,
    3.75787728026534e-06,
    3.8131564400221124e-06,
    3.868435599778883e-06,
    3.923714759535655e-06,
    3.978993919292427e-06,
    4.034273079049198e-06,
    4.089552238805971e-06,
    4.144831398562742e-06,
    4.200110558319514e-06,
    4.255389718076286e-06,
    4.3106688778330575e-06,
    4.3659480375898285e-06,
    4.421227197346601e-06,
    4.476506357103372e-06,
    4.531785516860144e-06,
    4.587064676616916e-06,
    4.642343836373687e-06,
    4.697622996130459e-06,
    4.752902155887231e-06,
    4.808181315644003e-06,
    4.8634604754007745e-06,
    4.918739635157546e-06,
    4.974018794914317e-06,
    5.029297954671089e-06,
    5.084577114427861e-06,
    5.139856274184632e-06,
    5.195135433941405e-06,
    5.250414593698177e-06,
    5.305693753454948e-06,
    5.36097291321172e-06,
    5.416252072968491e-06,
    5.471531232725263e-06,
    5.526810392482035e-06,
    5.582089552238806e-06,
    5.637368711995578e-06,
    5.69264787175235e-06,
    5.747927031509122e-06,
    5.803206191265893e-06,
    5.858485351022665e-06,
    5.913764510779436e-06,
    5.9690436705362084e-06,
    6.02432283029298e-06,
    6.079601990049751e-06,
    6.134881149806523e-06,
    6.190160309563296e-06,
    6.245439469320067e-06,
    6.300718629076839e-06,
    6.35599778883361e-06,
    6.411276948590382e-06,
    6.4665561083471536e-06,
    6.5218352681039254e-06,
    6.5771144278606965e-06,
    6.632393587617468e-06,
    6.687672747374241e-06,
    6.742951907131012e-06,
    6.798231066887784e-06,
    6.853510226644555e-06,
    6.908789386401328e-06,
    6.9640685461580995e-06,
    7.0193477059148706e-06,
    7.0746268656716424e-06,
    7.1299060254284134e-06,
    7.185185185185186e-06,
    7.240464344941957e-06,
    7.295743504698729e-06,
    7.351022664455501e-06,
    7.406301824212273e-06,
    7.461580983969045e-06,
    7.516860143725816e-06,
    7.5721393034825875e-06,
    7.6274184632393586e-06,
    7.682697622996131e-06,
    7.737976782752903e-06,
    7.793255942509675e-06,
    7.848535102266445e-06,
    7.903814262023219e-06,
    7.95909342177999e-06,
    8.01437258153676e-06,
    8.069651741293533e-06,
    8.124930901050305e-06,
    8.180210060807076e-06,
    8.235489220563848e-06,
    8.29076838032062e-06,
    8.34604754007739e-06,
    8.401326699834164e-06,
    8.456605859590936e-06,
    8.511885019347706e-06,
    8.567164179104478e-06,
    8.622443338861251e-06,
    8.677722498618022e-06,
    8.733001658374793e-06,
    8.788280818131565e-06,
    8.843559977888337e-06,
    8.898839137645109e-06,
    8.954118297401881e-06,
    9.009397457158651e-06,
    9.064676616915423e-06,
    9.119955776672196e-06,
    9.175234936428967e-06,
    9.230514096185739e-06,
    9.28579325594251e-06,
    9.341072415699282e-06,
    9.396351575456054e-06,
    9.451630735212826e-06,
    9.506909894969598e-06,
    9.562189054726368e-06,
    9.617468214483142e-06,
    9.672747374239912e-06,
    9.728026533996684e-06,
    9.783305693753456e-06,
    9.838584853510227e-06,
    9.893864013267e-06,
    9.949143173023771e-06,
    1.0004422332780543e-05,
    1.0059701492537315e-05,
    1.0114980652294087e-05,
    1.0170259812050859e-05,
    1.0225538971807629e-05,
    1.02808181315644e-05,
    1.0336097291321172e-05,
    1.0391376451077944e-05,
    1.0446655610834718e-05,
    1.0501934770591488e-05,
    1.055721393034826e-05,
    1.0612493090105032e-05,
    1.0667772249861804e-05,
    1.0723051409618574e-05,
    1.0778330569375346e-05,
    1.0833609729132118e-05,
    1.088888888888889e-05,
    1.0944168048645663e-05,
    1.0999447208402433e-05,
    1.1054726368159205e-05,
    1.1110005527915977e-05,
    1.1165284687672749e-05,
    1.1220563847429519e-05,
    1.1275843007186291e-05,
    1.1331122166943063e-05,
    1.1386401326699835e-05,
    1.1441680486456608e-05,
    1.149695964621338e-05,
    1.155223880597015e-05,
    1.1607517965726922e-05,
    1.1662797125483694e-05,
    1.1718076285240466e-05,
    1.1773355444997236e-05,
    1.1828634604754008e-05,
    1.188391376451078e-05,
    1.1939192924267553e-05,
    1.1994472084024325e-05,
    1.2049751243781095e-05,
    1.2105030403537867e-05,
    1.2160309563294639e-05,
    1.2215588723051411e-05,
    1.2270867882808181e-05,
    1.2326147042564953e-05,
    1.2381426202321725e-05,
    1.2436705362078498e-05,
    1.249198452183527e-05,
    1.254726368159204e-05,
    1.2602542841348812e-05,
    1.2657822001105584e-05,
    1.2713101160862356e-05,
    1.2768380320619126e-05,
    1.2823659480375898e-05,
    1.2878938640132672e-05,
    1.2934217799889444e-05,
    1.2989496959646215e-05,
    1.3044776119402987e-05,
    1.3100055279159757e-05,
    1.315533443891653e-05,
    1.3210613598673301e-05,
    1.3265892758430073e-05,
    1.3321171918186843e-05,
    1.3376451077943617e-05,
    1.3431730237700389e-05,
    1.348700939745716e-05,
    1.3542288557213932e-05,
    1.3597567716970703e-05,
    1.3652846876727474e-05,
    1.3708126036484246e-05,
    1.3763405196241018e-05,
    1.3818684355997788e-05,
    1.3873963515754562e-05,
    1.3929242675511334e-05,
    1.3984521835268106e-05,
    1.4039800995024878e-05,
    1.4095080154781648e-05,
    1.415035931453842e-05,
    1.4205638474295191e-05,
    1.4260917634051963e-05,
    1.4316196793808733e-05,
    1.4371475953565509e-05,
    1.4426755113322279e-05,
    1.448203427307905e-05,
    1.4537313432835823e-05,
    1.4592592592592594e-05,
    1.4647871752349365e-05,
    1.4703150912106137e-05,
    1.4758430071862908e-05,
    1.481370923161968e-05,
    1.4868988391376454e-05,
    1.4924267551133224e-05,
    1.4979546710889996e-05,
    1.5034825870646768e-05,
    1.509010503040354e-05,
    1.514538419016031e-05,
    1.5200663349917082e-05,
    1.5255942509673854e-05,
    1.5311221669430624e-05,
    1.53665008291874e-05,
    1.542177998894417e-05,
    1.5477059148700943e-05,
    1.553233830845771e-05,
    1.5587617468214483e-05,
    1.5642896627971255e-05,
    1.5698175787728027e-05,
    1.57534549474848e-05,
    1.5808734107241574e-05,
    1.5864013266998342e-05,
    1.5919292426755114e-05,
    1.5974571586511886e-05,
    1.6029850746268658e-05,
    1.608512990602543e-05,
    1.6140409065782202e-05,
    1.6195688225538974e-05,
    1.6250967385295745e-05,
    1.6306246545052517e-05,
    1.636152570480929e-05,
    1.641680486456606e-05,
    1.6472084024322833e-05,
    1.6527363184079605e-05,
    1.6582642343836373e-05,
    1.6637921503593145e-05,
    1.6693200663349917e-05,
    1.674847982310669e-05,
    1.6803758982863464e-05,
    1.6859038142620233e-05,
    1.6914317302377004e-05,
    1.6969596462133776e-05,
    1.7024875621890548e-05,
    1.708015478164732e-05,
    1.7135433941404092e-05,
    1.7190713101160864e-05,
    1.7245992260917636e-05,
    1.7301271420674408e-05,
    1.735655058043118e-05,
    1.741182974018795e-05,
    1.7467108899944723e-05,
    1.7522388059701495e-05,
    1.7577667219458267e-05,
    1.7632946379215035e-05,
    1.7688225538971807e-05,
    1.774350469872858e-05,
    1.7798783858485354e-05,
    1.7854063018242126e-05,
    1.7909342177998895e-05,
    1.7964621337755667e-05,
    1.801990049751244e-05,
    1.807517965726921e-05,
    1.8130458817025982e-05,
    1.8185737976782754e-05,
    1.8241017136539526e-05,
    1.8296296296296298e-05,
    1.835157545605307e-05,
    1.840685461580984e-05,
    1.8462133775566613e-05,
    1.8517412935323385e-05,
    1.8572692095080157e-05,
    1.862797125483693e-05,
    1.8683250414593697e-05,
    1.8738529574350473e-05,
    1.8793808734107245e-05,
    1.8849087893864016e-05,
    1.890436705362079e-05,
    1.8959646213377557e-05,
    1.901492537313433e-05,
    1.90702045328911e-05,
    1.9125483692647872e-05,
    1.9180762852404644e-05,
    1.9236042012161416e-05,
    1.9291321171918188e-05,
    1.934660033167496e-05,
    1.9401879491431732e-05,
    1.9457158651188504e-05,
    1.9512437810945276e-05,
    1.9567716970702047e-05,
    1.962299613045882e-05,
    1.9678275290215588e-05,
    1.9733554449972363e-05,
    1.9788833609729135e-05,
    1.9844112769485907e-05,
    1.989939192924268e-05,
    1.995467108899945e-05,
    1.9999997756510465e-05,
    1.9999903585496125e-05,
    1.999967092896331e-05,
    1.999929979013401e-05,
    1.9998790174148007e-05,
    1.999814208806281e-05,
    1.9997355540853566e-05,
    1.999643054341292e-05,
    1.9995367108550865e-05,
    1.9994165250994575e-05,
    1.9992824987388202e-05,
    1.9991346336292638e-05,
    1.9989729318185245e-05,
    1.9987973955459607e-05,
    1.998608027242519e-05,
    1.998404829530701e-05,
    1.9981878052245283e-05,
    1.9979569573295022e-05,
    1.997712289042562e-05,
    1.997453803752043e-05,
    1.9971815050376253e-05,
    1.996895396670289e-05,
    1.9965954826122583e-05,
    1.9962817670169494e-05,
    1.9959542542289103e-05,
    1.9956129487837624e-05,
    1.995257855408138e-05,
    1.994888979019613e-05,
    1.994506324726641e-05,
    1.9941098978284805e-05,
    1.9936997038151225e-05,
    1.993275748367216e-05,
    1.9928380373559855e-05,
    1.9923865768431534e-05,
    1.9919213730808544e-05,
    1.9914424325115493e-05,
    1.990949761767935e-05,
    1.9904433676728536e-05,
    1.989923257239198e-05,
    1.989389437669813e-05,
    1.9888419163573988e-05,
    1.9882807008844055e-05,
    1.9877057990229297e-05,
    1.9871172187346063e-05,
    1.9865149681704982e-05,
    1.9858990556709847e-05,
    1.9852694897656435e-05,
    1.9846262791731355e-05,
    1.983969432801081e-05,
    1.9832989597459393e-05,
    1.9826148692928805e-05,
    1.9819171709156582e-05,
    1.981205874276478e-05,
    1.9804809892258623e-05,
    1.979742525802517e-05,
    1.97899049423319e-05,
    1.9782249049325282e-05,
    1.9774457685029386e-05,
    1.9766530957344362e-05,
    1.9758468976044966e-05,
    1.9750271852779033e-05,
    1.9741939701065956e-05,
    1.9733472636295077e-05,
    1.972487077572411e-05,
    1.9716134238477517e-05,
    1.9707263145544857e-05,
    1.96982576197791e-05,
    1.9689117785894943e-05,
    1.967984377046707e-05,
    1.9670435701928397e-05,
    1.966089371056831e-05,
    1.965121792853084e-05,
    1.9641408489812858e-05,
    1.9631465530262186e-05,
    1.9621389187575746e-05,
    1.9611179601297636e-05,
    1.9600836912817204e-05,
    1.959036126536709e-05,
    1.9579752804021238e-05,
    1.9569011675692895e-05,
    1.9558138029132562e-05,
    1.9547132014925953e-05,
    1.9535993785491895e-05,
    1.9524723495080223e-05,
    1.951332129976964e-05,
    1.950178735746557e-05,
    1.9490121827897943e-05,
    1.947832487261901e-05,
    1.946639665500109e-05,
    1.945433734023432e-05,
    1.9442147095324356e-05,
    1.9429826089090064e-05,
    1.941737449216118e-05,
    1.9404792476975954e-05,
    1.9392080217778756e-05,
    1.9379237890617658e-05,
    1.936626567334202e-05,
    1.935316374559999e-05,
    1.9339932288836045e-05,
    1.9326571486288465e-05,
    1.9313081522986806e-05,
    1.9299462585749317e-05,
    1.928571486318038e-05,
    1.9271838545667876e-05,
    1.9257833825380567e-05,
    1.924370089626541e-05,
    1.9229439954044897e-05,
    1.9215051196214333e-05,
    1.9200534822039095e-05,
    1.918589103255188e-05,
    1.9171120030549925e-05,
    1.915622202059219e-05,
    1.914119720899652e-05,
    1.912604580383681e-05,
    1.91107680149401e-05,
    1.9095364053883688e-05,
    1.9079834133992175e-05,
    1.906417847033454e-05,
    1.9048397279721142e-05,
    1.9032490780700727e-05,
    1.9016459193557394e-05,
    1.900030274030755e-05,
    1.8984021644696828e-05,
    1.8967616132197003e-05,
    1.8951086430002856e-05,
    1.893443276702903e-05,
    1.8917655373906866e-05,
    1.89007544829812e-05,
    1.8883730328307154e-05,
    1.8866583145646892e-05,
    1.884931317246635e-05,
    1.883192064793195e-05,
    1.8814405812907295e-05,
    1.8796768909949828e-05,
    1.877901018330746e-05,
    1.8761129878915215e-05,
    1.8743128244391804e-05,
    1.8725005529036197e-05,
    1.870676198382418e-05,
    1.868839786140486e-05,
    1.8669913416097205e-05,
    1.8651308903886474e-05,
    1.8632584582420706e-05,
    1.8613740711007134e-05,
    1.8594777550608608e-05,
    1.857569536383997e-05,
    1.855649441496442e-05,
    1.8537174969889853e-05,
    1.8517737296165194e-05,
    1.8498181662976665e-05,
    1.8478508341144076e-05,
    1.8458717603117077e-05,
    1.843880972297137e-05,
    1.841878497640492e-05,
    1.8398643640734147e-05,
    1.8378385994890065e-05,
    1.8358012319414443e-05,
    1.83375228964559e-05,
    1.8316918009766012e-05,
    1.8296197944695366e-05,
    1.8275362988189627e-05,
    1.8254413428785552e-05,
    1.8233349556606998e-05,
    1.8212171663360902e-05,
    1.8190880042333247e-05,
    1.8169474988384994e-05,
    1.8147956797947994e-05,
    1.8126325769020906e-05,
    1.8104582201165036e-05,
    1.8082726395500215e-05,
    1.8060758654700622e-05,
    1.8038679282990587e-05,
    1.801648858614038e-05,
    1.799418687146198e-05,
    1.797177444780482e-05,
    1.79492516255515e-05,
    1.7926618716613504e-05,
    1.790387603442686e-05,
    1.788102389394782e-05,
    1.785806261164849e-05,
    1.7834992505512444e-05,
    1.781181389503032e-05,
    1.7788527101195407e-05,
    1.776513244649918e-05,
    1.7741630254926856e-05,
    1.7718020851952894e-05,
    1.7694304564536477e-05,
    1.7670481721117018e-05,
    1.7646552651609575e-05,
    1.76225176874003e-05,
    1.759837716134186e-05,
    1.7574131407748796e-05,
    1.7549780762392927e-05,
    1.752532556249867e-05,
    1.750076614673841e-05,
    1.7476102855227753e-05,
    1.745133602952087e-05,
    1.7426466012605738e-05,
    1.740149314889939e-05,
    1.737641778424317e-05,
    1.7351240265897903e-05,
    1.7325960942539124e-05,
    1.730058016425223e-05,
    1.727509828252763e-05,
    1.724951565025589e-05,
    1.722383262172283e-05,
    1.7198049552604638e-05,
    1.717216679996292e-05,
    1.714618472223978e-05,
    1.7120103679252834e-05,
    1.709392403219024e-05,
    1.7067646143605693e-05,
    1.704127037741341e-05,
    1.701479709888307e-05,
    1.698822667463478e-05,
    1.6961559472633985e-05,
    1.6934795862186382e-05,
    1.6907936213932787e-05,
    1.6880980899844013e-05,
    1.6853930293215735e-05,
    1.6826784768663283e-05,
    1.6799544702116487e-05,
    1.6772210470814464e-05,
    1.6744782453300378e-05,
    1.6717261029416218e-05,
    1.6689646580297518e-05,
    1.6661939488368098e-05,
    1.663414013733476e-05,
    1.660624891218197e-05,
    1.6578266199166523e-05,
    1.6550192385812212e-05,
    1.652202786090445e-05,
    1.6493773014484867e-05,
    1.6465428237845954e-05,
    1.6436993923525606e-05,
    1.640847046530168e-05,
    1.6379858258186593e-05,
    1.635115769842179e-05,
    1.6322369183472293e-05,
    1.6293493112021188e-05,
    1.62645298839641e-05,
    1.623547990040367e-05,
    1.6206343563643975e-05,
    1.6177121277184978e-05,
    1.6147813445716926e-05,
    1.611842047511476e-05,
    1.6088942772432478e-05,
    1.6059380745897514e-05,
    1.6029734804905072e-05,
    1.6000005360012455e-05,
    1.59701928229334e-05,
    1.5940297606532347e-05,
    1.5910320124818745e-05,
    1.5880260792941303e-05,
    1.5850120027182257e-05,
    1.5819898244951583e-05,
    1.5789595864781237e-05,
    1.5759213306319344e-05,
    1.5728750990324404e-05,
    1.5698209338659442e-05,
    1.5667588774286183e-05,
    1.563688972125919e-05,
    1.5606112604719985e-05,
    1.5575257850891178e-05,
    1.5544325887070546e-05,
    1.551331714162512e-05,
    1.548223204398526e-05,
    1.5451071024638707e-05,
    1.541983451512461e-05,
    1.538852294802756e-05,
    1.53571367569716e-05,
    1.5325676376614215e-05,
    1.5294142242640317e-05,
    1.5262534791756202e-05,
    1.5230854461683513e-05,
    1.5199101691153175e-05,
    1.5167276919899316e-05,
    1.5135380588653176e-05,
    1.5103413139137019e-05,
    1.507137501405799e-05,
    1.5039266657102005e-05,
    1.5007088512927594e-05,
    1.4974841027159757e-05,
    1.4942524646383773e-05,
    1.4910139818139035e-05,
    1.4877686990912837e-05,
    1.4845166614134174e-05,
    1.4812579138167515e-05,
    1.477992501430656e-05,
    1.4747204694767993e-05,
    1.4714418632685226e-05,
    1.4681567282102119e-05,
    1.4648651097966689e-05,
    1.4615670536124816e-05,
    1.4582626053313917e-05,
    1.454951810715665e-05,
    1.4516347156154536e-05,
    1.4483113659681639e-05,
    1.4449818077978193e-05,
    1.4416460872144237e-05,
    1.4383042504133222e-05,
    1.434956343674561e-05,
    1.431602413362247e-05,
    1.4282425059239057e-05,
    1.4248766678898386e-05,
    1.4215049458724773e-05,
    1.418127386565739e-05,
    1.4147440367443803e-05,
    1.411354943263348e-05,
    1.4079601530571317e-05,
    1.4045597131391127e-05,
    1.4011536706009134e-05,
    1.3977420726117457e-05,
    1.3943249664177563e-05,
    1.3909023993413737e-05,
    1.3874744187806528e-05,
    1.3840410722086182e-05,
    1.3806024071726054e-05,
    1.3771584712936053e-05,
    1.3737093122656017e-05,
    1.3702549778549134e-05,
    1.3667955158995305e-05,
    1.3633309743084535e-05,
    1.3598614010610282e-05,
    1.3563868442062836e-05,
    1.3529073518622637e-05,
    1.3494229722153638e-05,
    1.3459337535196607e-05,
    1.3424397440962467e-05,
    1.3389409923325592e-05,
    1.3354375466817101e-05,
    1.3319294556618164e-05,
    1.3284167678553263e-05,
    1.3248995319083484e-05,
    1.3213777965299768e-05,
    1.3178516104916162e-05,
    1.3143210226263082e-05,
    1.3107860818280533e-05,
    1.307246837051134e-05,
    1.3037033373094377e-05,
    1.3001556316757773e-05,
    1.2966037692812124e-05,
    1.2930477993143676e-05,
    1.2894877710207519e-05,
    1.2859237337020774e-05,
    1.2823557367155753e-05,
    1.2787838294733132e-05,
    1.2752080614415113e-05,
    1.2716284821398547e-05,
    1.2680451411408123e-05,
    1.2644580880689457e-05,
    1.260867372600224e-05,
    1.2572730444613363e-05,
    1.2536751534290019e-05,
    1.2500737493292818e-05,
    1.2464688820368884e-05,
    1.2428606014744943e-05,
    1.2392489576120417e-05,
    1.2356340004660507e-05,
    1.232015780098925e-05,
    1.22839434661826e-05,
    1.2247697501761484e-05,
    1.2211420409684867e-05,
    1.2175112692342773e-05,
    1.2138774852549366e-05,
    1.210240739353595e-05,
    1.2066010818944024e-05,
    1.2029585632818294e-05,
    1.1993132339599711e-05,
    1.1956651444118454e-05,
    1.1920143451586973e-05,
    1.188360886759297e-05,
    1.1847048198092402e-05,
    1.1810461949402474e-05,
    1.177385062819464e-05,
    1.1737214741487563e-05,
    1.1700554796640113e-05,
    1.1663871301344332e-05,
    1.1627164763618406e-05,
    1.1590435691799624e-05,
    1.1553684594537349e-05,
    1.1516911980785958e-05,
    1.1480118359797817e-05,
    1.1443304241116203e-05,
    1.140647013456826e-05,
    1.1369616550257943e-05,
    1.1332743998558944e-05,
    1.1295852990107634e-05,
    1.125894403579598e-05,
    1.1222017646764475e-05,
    1.1185074334395065e-05,
    1.1148114610304061e-05,
    1.1111138986335052e-05,
    1.107414797455182e-05,
    1.1037142087231247e-05,
    1.1000121836856228e-05,
    1.096308773610856e-05,
    1.0926040297861851e-05,
    1.0888980035174414e-05,
    1.0851907461282174e-05,
    1.0814823089591542e-05,
    1.0777727433672311e-05,
    1.0740621007250545e-05,
    1.0703504324201472e-05,
    1.066637789854236e-05,
    1.0629242244425394e-05,
    1.059209787613056e-05,
    1.0554945308058537e-05,
    1.0517785054723546e-05,
    1.0480617630746241e-05,
    1.0443443550846586e-05,
    1.0406263329836714e-05,
    1.0369077482613812e-05,
    1.0331886524152978e-05,
    1.0294690969500092e-05,
    1.0257491333764688e-05,
    1.022028813211282e-05,
    1.0183081879759916e-05,
    1.0145873091963665e-05,
    1.0108662284016857e-05,
    1.0071449971240268e-05,
    1.0034236668975508e-05,
    9.99702289257789e-06,
    9.9598091574093e-06,
    9.922595978831039e-06,
    9.885383872196713e-06,
    9.848173352845082e-06,
    9.810964936092917e-06,
    9.773759137227878e-06,
    9.736556471501363e-06,
    9.699357454121385e-06,
    9.662162600245437e-06,
    9.62497242497334e-06,
    9.587787443340134e-06,
    9.550608170308935e-06,
    9.513435120763791e-06,
    9.47626880950257e-06,
    9.439109751229825e-06,
    9.401958460549658e-06,
    9.364815451958615e-06,
    9.327681239838523e-06,
    9.29055633844941e-06,
    9.253441261922353e-06,
    9.216336524252368e-06,
    9.179242639291297e-06,
    9.142160120740673e-06,
    9.105089482144635e-06,
    9.068031236882798e-06,
    9.030985898163128e-06,
    8.993953979014877e-06,
    8.956935992281435e-06,
    8.919932450613254e-06,
    8.882943866460746e-06,
    8.845970752067165e-06,
    8.809013619461546e-06,
    8.772072980451594e-06,
    8.73514934661659e-06,
    8.69824322930032e-06,
    8.661355139603998e-06,
    8.624485588379164e-06,
    8.587635086220642e-06,
    8.550804143459431e-06,
    8.513993270155681e-06,
    8.477202976091583e-06,
    8.440433770764352e-06,
    8.403686163379144e-06,
    8.366960662842004e-06,
    8.330257777752841e-06,
    8.293578016398364e-06,
    8.256921886745041e-06,
    8.220289896432085e-06,
    8.183682552764401e-06,
    8.14710036270557e-06,
    8.110543832870838e-06,
    8.07401346952008e-06,
    8.037509778550808e-06,
    8.001033265491156e-06,
    7.96458443549287e-06,
    7.928163793324331e-06,
    7.891771843363557e-06,
    7.855409089591206e-06,
    7.81907603558362e-06,
    7.78277318450583e-06,
    7.746501039104596e-06,
    7.710260101701457e-06,
    7.674050874185742e-06,
    7.637873858007658e-06,
    7.6017295541713106e-06,
    7.565618463227797e-06,
    7.529541085268257e-06,
    7.493497919916941e-06,
    7.457489466324313e-06,
    7.421516223160108e-06,
    7.385578688606456e-06,
    7.349677360350972e-06,
    7.313812735579846e-06,
    7.277985310970985e-06,
    7.2421955826871285e-06,
    7.20644404636895e-06,
    7.170731197128238e-06,
    7.135057529540998e-06,
    7.099423537640631e-06,
    7.063829714911081e-06,
    7.028276554279996e-06,
    6.9927645481119125e-06,
    6.957294188201438e-06,
    6.9218659657664164e-06,
    6.886480371441162e-06,
    6.851137895269629e-06,
    6.815839026698653e-06,
    6.780584254571164e-06,
    6.745374067119401e-06,
    6.71020895195818e-06,
    6.675089396078108e-06,
    6.640015885838872e-06,
    6.604988906962478e-06,
    6.570008944526531e-06,
    6.53507648295753e-06,
    6.500192006024146e-06,
    6.46535599683052e-06,
    6.430568937809587e-06,
    6.395831310716378e-06,
    6.361143596621362e-06,
    6.326506275903781e-06,
    6.291919828244986e-06,
    6.257384732621812e-06,
    6.2229014672999366e-06,
    6.188470509827244e-06,
    6.1540923370272394e-06,
    6.119767424992411e-06,
    6.085496249077671e-06,
    6.051279283893758e-06,
    6.017117003300648e-06,
    5.9830098804010205e-06,
    5.948958387533688e-06,
    5.914962996267064e-06,
    5.881024177392627e-06,
    5.8471424009183954e-06,
    5.8133181360624336e-06,
    5.779551851246348e-06,
    5.745844014088788e-06,
    5.712195091398989e-06,
    5.678605549170288e-06,
    5.645075852573692e-06,
    5.611606465951421e-06,
    5.578197852810479e-06,
    5.544850475816232e-06,
    5.511564796786021e-06,
    5.478341276682733e-06,
    5.445180375608453e-06,
    5.412082552798059e-06,
    5.379048266612889e-06,
    5.346077974534384e-06,
    5.313172133157736e-06,
    5.280331198185593e-06,
    5.247555624421736e-06,
    5.214845865764766e-06,
    5.1822023752018445e-06,
    5.149625604802396e-06,
    5.117116005711872e-06,
    5.084674028145476e-06,
    5.0523001213819464e-06,
    5.0199947337573386e-06,
    4.987758312658795e-06,
    4.95559130451837e-06,
    4.923494154806845e-06,
    4.891467308027539e-06,
    4.85951120771018e-06,
    4.827626296404751e-06,
    4.795813015675351e-06,
    4.7640718060941e-06,
    4.732403107235015e-06,
    4.700807357667953e-06,
    4.669284994952499e-06,
    4.637836455631943e-06,
    4.606462175227207e-06,
    4.5751625882308335e-06,
    4.5439381281009494e-06,
    4.512789227255285e-06,
    4.481716317065163e-06,
    4.450719827849539e-06,
    4.419800188869048e-06,
    4.388957828320032e-06,
    4.358193173328642e-06,
    4.3275066499449105e-06,
    4.296898683136836e-06,
    4.26636969678453e-06,
    4.235920113674314e-06,
    4.2055503554928805e-06,
    4.175260842821462e-06,
    4.145051995129984e-06,
    4.114924230771279e-06,
    4.08487796697527e-06,
    4.054913619843215e-06,
    4.025031604341932e-06,
    3.9952323342980456e-06,
    3.965516222392274e-06,
    3.935883680153706e-06,
    3.906335117954087e-06,
    3.876870945002166e-06,
    3.847491569337997e-06,
    3.8181973978273165e-06,
    3.7889888361558856e-06,
    3.7598662888238813e-06,
    3.730830159140302e-06,
    3.7018808492173753e-06,
    3.6730187599649804e-06,
    3.6442442910851163e-06,
    3.6155578410663418e-06,
    3.5869598071782828e-06,
    3.5584505854661143e-06,
    3.530030570745072e-06,
    3.5017001565950047e-06,
    3.473459735354897e-06,
    3.445309698117465e-06,
    3.41725043472371e-06,
    3.38928233375755e-06,
    3.361405782540408e-06,
    3.333621167125878e-06,
    3.3059288722943496e-06,
    3.2783292815477095e-06,
    3.250822777103998e-06,
    3.223409739892145e-06
  ],
  "epochs": [
    0.001658374792703151,
    0.08291873963515754,
    0.16583747927031509,
    0.24875621890547264,
    0.33167495854063017,
    0.41459369817578773,
    0.4975124378109453,
    0.5804311774461028,
    0.6633499170812603,
    0.746268656716418,
    0.8291873963515755,
    0.912106135986733,
    0.9950248756218906,
    1.077943615257048,
    1.1608623548922057,
    1.243781094527363,
    1.3266998341625207,
    1.4096185737976783,
    1.4925373134328357,
    1.5754560530679935,
    1.658374792703151,
    1.7412935323383083,
    1.8242122719734661,
    1.9071310116086235,
    1.9900497512437811,
    2.0729684908789388,
    2.155887230514096,
    2.2388059701492535,
    2.3217247097844114,
    2.4046434494195688,
    2.487562189054726,
    2.570480928689884,
    2.6533996683250414,
    2.7363184079601988,
    2.8192371475953566,
    2.902155887230514,
    2.9850746268656714,
    3.067993366500829,
    3.1509121061359866,
    3.2338308457711444,
    3.316749585406302,
    3.399668325041459,
    3.482587064676617,
    3.5655058043117744,
    3.6484245439469323,
    3.7313432835820897,
    3.814262023217247,
    3.897180762852405,
    3.9800995024875623,
    4.06301824212272,
    4.1459369817578775,
    4.2288557213930345,
    4.311774461028192,
    4.39469320066335,
    4.477611940298507,
    4.560530679933665,
    4.643449419568823,
    4.726368159203981,
    4.8092868988391375,
    4.892205638474295,
    4.975124378109452,
    5.05804311774461,
    5.140961857379768,
    5.223880597014926,
    5.306799336650083,
    5.389718076285241,
    5.472636815920398,
    5.555555555555555,
    5.638474295190713,
    5.721393034825871,
    5.804311774461028,
    5.887230514096186,
    5.970149253731344,
    6.053067993366501,
    6.135986733001658,
    6.218905472636816,
    6.301824212271973,
    6.384742951907131,
    6.467661691542289,
    6.550580431177446,
    6.633499170812604,
    6.7164179104477615,
    6.799336650082918,
    6.882255389718076,
    6.965174129353234,
    7.048092868988391,
    7.131011608623549,
    7.213930348258707,
    7.296849087893864,
    7.3797678275290215,
    7.462686567164179,
    7.545605306799336,
    7.628524046434494,
    7.711442786069652,
    7.79436152570481,
    7.877280265339967,
    7.960199004975125,
    8.043117744610282,
    8.12603648424544,
    8.208955223880597,
    8.291873963515755,
    8.374792703150913,
    8.457711442786069,
    8.540630182421227,
    8.623548922056385,
    8.706467661691542,
    8.7893864013267,
    8.872305140961858,
    8.955223880597014,
    9.038142620232172,
    9.12106135986733,
    9.203980099502488,
    9.286898839137645,
    9.369817578772803,
    9.45273631840796,
    9.535655058043117,
    9.618573797678275,
    9.701492537313433,
    9.78441127694859,
    9.867330016583749,
    9.950248756218905,
    10.033167495854062,
    10.11608623548922,
    10.199004975124378,
    10.281923714759536,
    10.364842454394694,
    10.447761194029852,
    10.530679933665008,
    10.613598673300165,
    10.696517412935323,
    10.779436152570481,
    10.862354892205639,
    10.945273631840797,
    11.028192371475953,
    11.11111111111111,
    11.194029850746269,
    11.276948590381426,
    11.359867330016584,
    11.442786069651742,
    11.525704809286898,
    11.608623548922056,
    11.691542288557214,
    11.774461028192372,
    11.85737976782753,
    11.940298507462687,
    12.023217247097843,
    12.106135986733001,
    12.189054726368159,
    12.271973466003317,
    12.354892205638475,
    12.437810945273633,
    12.520729684908789,
    12.603648424543946,
    12.686567164179104,
    12.769485903814262,
    12.85240464344942,
    12.935323383084578,
    13.018242122719734,
    13.101160862354892,
    13.18407960199005,
    13.266998341625207,
    13.349917081260365,
    13.432835820895523,
    13.51575456053068,
    13.598673300165837,
    13.681592039800995,
    13.764510779436153,
    13.84742951907131,
    13.930348258706468,
    14.013266998341626,
    14.096185737976782,
    14.17910447761194,
    14.262023217247098,
    14.344941956882256,
    14.427860696517413,
    14.510779436152571,
    14.593698175787727,
    14.676616915422885,
    14.759535655058043,
    14.8424543946932,
    14.925373134328359,
    15.008291873963516,
    15.091210613598673,
    15.17412935323383,
    15.257048092868988,
    15.339966832504146,
    15.422885572139304,
    15.505804311774462,
    15.588723051409618,
    15.671641791044776,
    15.754560530679933,
    15.837479270315091,
    15.92039800995025,
    16.003316749585405,
    16.086235489220563,
    16.16915422885572,
    16.25207296849088,
    16.334991708126037,
    16.417910447761194,
    16.500829187396352,
    16.58374792703151,
    16.666666666666668,
    16.749585406301826,
    16.83250414593698,
    16.915422885572138,
    16.998341625207296,
    17.081260364842453,
    17.16417910447761,
    17.24709784411277,
    17.330016583747927,
    17.412935323383085,
    17.495854063018243,
    17.5787728026534,
    17.66169154228856,
    17.744610281923716,
    17.827529021558874,
    17.91044776119403,
    17.993366500829186,
    18.076285240464344,
    18.1592039800995,
    18.24212271973466,
    18.325041459369817,
    18.407960199004975,
    18.490878938640133,
    18.57379767827529,
    18.65671641791045,
    18.739635157545607,
    18.822553897180764,
    18.90547263681592,
    18.988391376451077,
    19.071310116086234,
    19.154228855721392,
    19.23714759535655,
    19.320066334991708,
    19.402985074626866,
    19.485903814262024,
    19.56882255389718,
    19.65174129353234,
    19.734660033167497,
    19.817578772802655,
    19.90049751243781,
    19.983416252072967,
    20.066334991708125,
    20.149253731343283,
    20.23217247097844,
    20.3150912106136,
    20.398009950248756,
    20.480928689883914,
    20.563847429519072,
    20.64676616915423,
    20.729684908789388,
    20.812603648424545,
    20.895522388059703,
    20.978441127694857,
    21.061359867330015,
    21.144278606965173,
    21.22719734660033,
    21.31011608623549,
    21.393034825870647,
    21.475953565505804,
    21.558872305140962,
    21.64179104477612,
    21.724709784411278,
    21.807628524046436,
    21.890547263681594,
    21.973466003316748,
    22.056384742951906,
    22.139303482587064,
    22.22222222222222,
    22.30514096185738,
    22.388059701492537,
    22.470978441127695,
    22.553897180762853,
    22.63681592039801,
    22.71973466003317,
    22.802653399668326,
    22.885572139303484,
    22.96849087893864,
    23.051409618573796,
    23.134328358208954,
    23.217247097844112,
    23.30016583747927,
    23.383084577114428,
    23.466003316749585,
    23.548922056384743,
    23.6318407960199,
    23.71475953565506,
    23.797678275290217,
    23.880597014925375,
    23.963515754560532,
    24.046434494195687,
    24.129353233830845,
    24.212271973466002,
    24.29519071310116,
    24.378109452736318,
    24.461028192371476,
    24.543946932006634,
    24.62686567164179,
    24.70978441127695,
    24.792703150912107,
    24.875621890547265,
    24.958540630182423,
    25.041459369817577,
    25.124378109452735,
    25.207296849087893,
    25.29021558872305,
    25.37313432835821,
    25.456053067993366,
    25.538971807628524,
    25.621890547263682,
    25.70480928689884,
    25.787728026533998,
    25.870646766169155,
    25.953565505804313,
    26.036484245439468,
    26.119402985074625,
    26.202321724709783,
    26.28524046434494,
    26.3681592039801,
    26.451077943615257,
    26.533996683250415,
    26.616915422885572,
    26.69983416252073,
    26.782752902155888,
    26.865671641791046,
    26.948590381426204,
    27.03150912106136,
    27.114427860696516,
    27.197346600331674,
    27.28026533996683,
    27.36318407960199,
    27.446102819237147,
    27.529021558872305,
    27.611940298507463,
    27.69485903814262,
    27.77777777777778,
    27.860696517412936,
    27.943615257048094,
    28.026533996683252,
    28.109452736318406,
    28.192371475953564,
    28.275290215588722,
    28.35820895522388,
    28.441127694859038,
    28.524046434494196,
    28.606965174129353,
    28.68988391376451,
    28.77280265339967,
    28.855721393034827,
    28.938640132669985,
    29.021558872305143,
    29.104477611940297,
    29.187396351575455,
    29.270315091210612,
    29.35323383084577,
    29.436152570480928,
    29.519071310116086,
    29.601990049751244,
    29.6849087893864,
    29.76782752902156,
    29.850746268656717,
    29.933665008291875,
    30.016583747927033,
    30.09950248756219,
    30.182421227197345,
    30.265339966832503,
    30.34825870646766,
    30.43117744610282,
    30.514096185737976,
    30.597014925373134,
    30.679933665008292,
    30.76285240464345,
    30.845771144278608,
    30.928689883913766,
    31.011608623548923,
    31.09452736318408,
    31.177446102819236,
    31.260364842454393,
    31.34328358208955,
    31.42620232172471,
    31.509121061359867,
    31.592039800995025,
    31.674958540630183,
    31.75787728026534,
    31.8407960199005,
    31.923714759535656,
    32.00663349917081,
    32.08955223880597,
    32.172470978441126,
    32.25538971807629,
    32.33830845771144,
    32.4212271973466,
    32.50414593698176,
    32.58706467661692,
    32.66998341625207,
    32.75290215588723,
    32.83582089552239,
    32.91873963515754,
    33.001658374792704,
    33.08457711442786,
    33.16749585406302,
    33.250414593698174,
    33.333333333333336,
    33.41625207296849,
    33.49917081260365,
    33.582089552238806,
    33.66500829187396,
    33.74792703150912,
    33.830845771144276,
    33.91376451077944,
    33.99668325041459,
    34.07960199004975,
    34.16252072968491,
    34.24543946932007,
    34.32835820895522,
    34.411276948590384,
    34.49419568822554,
    34.5771144278607,
    34.660033167495854,
    34.74295190713101,
    34.82587064676617,
    34.908789386401324,
    34.991708126036485,
    35.07462686567164,
    35.1575456053068,
    35.240464344941955,
    35.32338308457712,
    35.40630182421227,
    35.48922056384743,
    35.57213930348259,
    35.65505804311775,
    35.7379767827529,
    35.82089552238806,
    35.90381426202322,
    35.98673300165837,
    36.069651741293534,
    36.15257048092869,
    36.23548922056385,
    36.318407960199,
    36.401326699834165,
    36.48424543946932,
    36.56716417910448,
    36.650082918739635,
    36.73300165837479,
    36.81592039800995,
    36.898839137645105,
    36.981757877280266,
    37.06467661691542,
    37.14759535655058,
    37.230514096185736,
    37.3134328358209,
    37.39635157545605,
    37.47927031509121,
    37.56218905472637,
    37.64510779436153,
    37.72802653399668,
    37.81094527363184,
    37.893864013267,
    37.97678275290215,
    38.059701492537314,
    38.14262023217247,
    38.22553897180763,
    38.308457711442784,
    38.391376451077946,
    38.4742951907131,
    38.55721393034826,
    38.640132669983416,
    38.72305140961858,
    38.80597014925373,
    38.888888888888886,
    38.97180762852405,
    39.0547263681592,
    39.13764510779436,
    39.22056384742952,
    39.30348258706468,
    39.38640132669983,
    39.469320066334994,
    39.55223880597015,
    39.63515754560531,
    39.718076285240464,
    39.80099502487562,
    39.88391376451078,
    39.966832504145934,
    40.049751243781095,
    40.13266998341625,
    40.21558872305141,
    40.298507462686565,
    40.38142620232173,
    40.46434494195688,
    40.54726368159204,
    40.6301824212272,
    40.71310116086236,
    40.79601990049751,
    40.87893864013267,
    40.96185737976783,
    41.04477611940298,
    41.127694859038144,
    41.2106135986733,
    41.29353233830846,
    41.376451077943614,
    41.459369817578775,
    41.54228855721393,
    41.62520729684909,
    41.708126036484245,
    41.791044776119406,
    41.87396351575456,
    41.956882255389715,
    42.039800995024876,
    42.12271973466003,
    42.20563847429519,
    42.288557213930346,
    42.37147595356551,
    42.45439469320066,
    42.53731343283582,
    42.62023217247098,
    42.70315091210614,
    42.78606965174129,
    42.86898839137645,
    42.95190713101161,
    43.03482587064676,
    43.117744610281925,
    43.20066334991708,
    43.28358208955224,
    43.366500829187395,
    43.449419568822556,
    43.53233830845771,
    43.61525704809287,
    43.698175787728026,
    43.78109452736319,
    43.86401326699834,
    43.946932006633496,
    44.02985074626866,
    44.11276948590381,
    44.19568822553897,
    44.27860696517413,
    44.36152570480929,
    44.44444444444444,
    44.527363184079604,
    44.61028192371476,
    44.69320066334992,
    44.776119402985074,
    44.859038142620236,
    44.94195688225539,
    45.024875621890544,
    45.107794361525706,
    45.19071310116086,
    45.27363184079602,
    45.356550580431175,
    45.43946932006634,
    45.52238805970149,
    45.60530679933665,
    45.68822553897181,
    45.77114427860697,
    45.85406301824212,
    45.93698175787728,
    46.01990049751244,
    46.10281923714759,
    46.185737976782754,
    46.26865671641791,
    46.35157545605307,
    46.434494195688224,
    46.517412935323385,
    46.60033167495854,
    46.6832504145937,
    46.766169154228855,
    46.84908789386402,
    46.93200663349917,
    47.014925373134325,
    47.09784411276949,
    47.18076285240464,
    47.2636815920398,
    47.346600331674956,
    47.42951907131012,
    47.51243781094527,
    47.59535655058043,
    47.67827529021559,
    47.76119402985075,
    47.8441127694859,
    47.927031509121065,
    48.00995024875622,
    48.09286898839137,
    48.175787728026535,
    48.25870646766169,
    48.34162520729685,
    48.424543946932005,
    48.507462686567166,
    48.59038142620232,
    48.67330016583748,
    48.756218905472636,
    48.8391376451078,
    48.92205638474295,
    49.004975124378106,
    49.08789386401327,
    49.17081260364842,
    49.25373134328358,
    49.33665008291874,
    49.4195688225539,
    49.50248756218905,
    49.585406301824214,
    49.66832504145937,
    49.75124378109453,
    49.834162520729684,
    49.917081260364846,
    50.0,
    50.082918739635154,
    50.165837479270316,
    50.24875621890547,
    50.33167495854063,
    50.414593698175786,
    50.49751243781095,
    50.5804311774461,
    50.66334991708126,
    50.74626865671642,
    50.82918739635158,
    50.91210613598673,
    50.995024875621894,
    51.07794361525705,
    51.1608623548922,
    51.243781094527364,
    51.32669983416252,
    51.40961857379768,
    51.492537313432834,
    51.575456053067995,
    51.65837479270315,
    51.74129353233831,
    51.824212271973465,
    51.90713101160863,
    51.99004975124378,
    52.072968490878935,
    52.1558872305141,
    52.23880597014925,
    52.32172470978441,
    52.40464344941957,
    52.48756218905473,
    52.57048092868988,
    52.653399668325044,
    52.7363184079602,
    52.81923714759536,
    52.90215588723051,
    52.985074626865675,
    53.06799336650083,
    53.15091210613598,
    53.233830845771145,
    53.3167495854063,
    53.39966832504146,
    53.482587064676615,
    53.565505804311776,
    53.64842454394693,
    53.73134328358209,
    53.814262023217246,
    53.89718076285241,
    53.98009950248756,
    54.06301824212272,
    54.14593698175788,
    54.22885572139303,
    54.31177446102819,
    54.39469320066335,
    54.47761194029851,
    54.56053067993366,
    54.643449419568825,
    54.72636815920398,
    54.80928689883914,
    54.892205638474294,
    54.975124378109456,
    55.05804311774461,
    55.140961857379764,
    55.223880597014926,
    55.30679933665008,
    55.38971807628524,
    55.472636815920396,
    55.55555555555556,
    55.63847429519071,
    55.72139303482587,
    55.80431177446103,
    55.88723051409619,
    55.97014925373134,
    56.053067993366504,
    56.13598673300166,
    56.21890547263681,
    56.301824212271974,
    56.38474295190713,
    56.46766169154229,
    56.550580431177444,
    56.633499170812605,
    56.71641791044776,
    56.79933665008292,
    56.882255389718075,
    56.96517412935324,
    57.04809286898839,
    57.13101160862355,
    57.21393034825871,
    57.29684908789386,
    57.37976782752902,
    57.46268656716418,
    57.54560530679934,
    57.62852404643449,
    57.711442786069654,
    57.79436152570481,
    57.87728026533997,
    57.960199004975124,
    58.043117744610285,
    58.12603648424544,
    58.208955223880594,
    58.291873963515755,
    58.37479270315091,
    58.45771144278607,
    58.540630182421225,
    58.623548922056386,
    58.70646766169154,
    58.7893864013267,
    58.872305140961856,
    58.95522388059702,
    59.03814262023217,
    59.12106135986733,
    59.20398009950249,
    59.28689883913764,
    59.3698175787728,
    59.45273631840796,
    59.53565505804312,
    59.61857379767827,
    59.701492537313435,
    59.78441127694859,
    59.86733001658375,
    59.950248756218905,
    60.033167495854066,
    60.11608623548922,
    60.19900497512438,
    60.281923714759536,
    60.36484245439469,
    60.44776119402985,
    60.530679933665006,
    60.61359867330017,
    60.69651741293532,
    60.77943615257048,
    60.86235489220564,
    60.9452736318408,
    61.02819237147595,
    61.111111111111114,
    61.19402985074627,
    61.27694859038142,
    61.359867330016584,
    61.44278606965174,
    61.5257048092869,
    61.608623548922054,
    61.691542288557216,
    61.77446102819237,
    61.85737976782753,
    61.940298507462686,
    62.02321724709785,
    62.106135986733,
    62.18905472636816,
    62.27197346600332,
    62.35489220563847,
    62.43781094527363,
    62.52072968490879,
    62.60364842454395,
    62.6865671641791,
    62.769485903814264,
    62.85240464344942,
    62.93532338308458,
    63.018242122719734,
    63.101160862354895,
    63.18407960199005,
    63.26699834162521,
    63.349917081260365,
    63.43283582089552,
    63.51575456053068,
    63.598673300165835,
    63.681592039801,
    63.76451077943615,
    63.84742951907131,
    63.930348258706466,
    64.01326699834162,
    64.09618573797678,
    64.17910447761194,
    64.2620232172471,
    64.34494195688225,
    64.42786069651741,
    64.51077943615257,
    64.59369817578772,
    64.67661691542288,
    64.75953565505804,
    64.8424543946932,
    64.92537313432835,
    65.00829187396351,
    65.09121061359868,
    65.17412935323384,
    65.25704809286898,
    65.33996683250415,
    65.42288557213931,
    65.50580431177445,
    65.58872305140962,
    65.67164179104478,
    65.75456053067994,
    65.83747927031509,
    65.92039800995025,
    66.00331674958541,
    66.08623548922057,
    66.16915422885572,
    66.25207296849088,
    66.33499170812604,
    66.41791044776119,
    66.50082918739635,
    66.58374792703151,
    66.66666666666667,
    66.74958540630182,
    66.83250414593698,
    66.91542288557214,
    66.9983416252073,
    67.08126036484245,
    67.16417910447761,
    67.24709784411277,
    67.33001658374793,
    67.41293532338308,
    67.49585406301824,
    67.5787728026534,
    67.66169154228855,
    67.74461028192371,
    67.82752902155887,
    67.91044776119404,
    67.99336650082918,
    68.07628524046434,
    68.1592039800995,
    68.24212271973467,
    68.32504145936981,
    68.40796019900498,
    68.49087893864014,
    68.57379767827528,
    68.65671641791045,
    68.7396351575456,
    68.82255389718077,
    68.90547263681592,
    68.98839137645108,
    69.07131011608624,
    69.1542288557214,
    69.23714759535655,
    69.32006633499171,
    69.40298507462687,
    69.48590381426202,
    69.56882255389718,
    69.65174129353234,
    69.7346600331675,
    69.81757877280265,
    69.90049751243781,
    69.98341625207297,
    70.06633499170813,
    70.14925373134328,
    70.23217247097844,
    70.3150912106136,
    70.39800995024876,
    70.48092868988391,
    70.56384742951907,
    70.64676616915423,
    70.72968490878938,
    70.81260364842454,
    70.8955223880597,
    70.97844112769486,
    71.06135986733001,
    71.14427860696517,
    71.22719734660033,
    71.3101160862355,
    71.39303482587064,
    71.4759535655058,
    71.55887230514097,
    71.64179104477611,
    71.72470978441127,
    71.80762852404644,
    71.8905472636816,
    71.97346600331674,
    72.0563847429519,
    72.13930348258707,
    72.22222222222223,
    72.30514096185738,
    72.38805970149254,
    72.4709784411277,
    72.55389718076285,
    72.636815920398,
    72.71973466003317,
    72.80265339966833,
    72.88557213930348,
    72.96849087893864,
    73.0514096185738,
    73.13432835820896,
    73.21724709784411,
    73.30016583747927,
    73.38308457711443,
    73.46600331674958,
    73.54892205638474,
    73.6318407960199,
    73.71475953565506,
    73.79767827529021,
    73.88059701492537,
    73.96351575456053,
    74.0464344941957,
    74.12935323383084,
    74.212271973466,
    74.29519071310116,
    74.37810945273633,
    74.46102819237147,
    74.54394693200663,
    74.6268656716418,
    74.70978441127694,
    74.7927031509121,
    74.87562189054727,
    74.95854063018243,
    75.04145936981757,
    75.12437810945273,
    75.2072968490879,
    75.29021558872306,
    75.3731343283582,
    75.45605306799337,
    75.53897180762853,
    75.62189054726367,
    75.70480928689884,
    75.787728026534,
    75.87064676616916,
    75.9535655058043,
    76.03648424543947,
    76.11940298507463,
    76.20232172470979,
    76.28524046434494,
    76.3681592039801,
    76.45107794361526,
    76.53399668325042,
    76.61691542288557,
    76.69983416252073,
    76.78275290215589,
    76.86567164179104,
    76.9485903814262,
    77.03150912106136,
    77.11442786069652,
    77.19734660033167,
    77.28026533996683,
    77.363184079602,
    77.44610281923715,
    77.5290215588723,
    77.61194029850746,
    77.69485903814262,
    77.77777777777777,
    77.86069651741293,
    77.9436152570481,
    78.02653399668326,
    78.1094527363184,
    78.19237147595356,
    78.27529021558873,
    78.35820895522389,
    78.44112769485903,
    78.5240464344942,
    78.60696517412936,
    78.6898839137645,
    78.77280265339967,
    78.85572139303483,
    78.93864013266999,
    79.02155887230514,
    79.1044776119403,
    79.18739635157546,
    79.27031509121062,
    79.35323383084577,
    79.43615257048093,
    79.51907131011609,
    79.60199004975124,
    79.6849087893864,
    79.76782752902156,
    79.85074626865672,
    79.93366500829187,
    80.01658374792703,
    80.09950248756219,
    80.18242122719735,
    80.2653399668325,
    80.34825870646766,
    80.43117744610282,
    80.51409618573798,
    80.59701492537313,
    80.67993366500829,
    80.76285240464345,
    80.8457711442786,
    80.92868988391376,
    81.01160862354892,
    81.09452736318408,
    81.17744610281923,
    81.2603648424544,
    81.34328358208955,
    81.42620232172472,
    81.50912106135986,
    81.59203980099502
  ],
  "steps": [],
  "train_logs": [
    {
      "loss": 10.9566,
      "grad_norm": 4.742486000061035,
      "learning_rate": 0.0,
      "epoch": 0.001658374792703151
    },
    {
      "loss": 10.9637,
      "grad_norm": 4.902157783508301,
      "learning_rate": 5.417357656163627e-08,
      "epoch": 0.08291873963515754
    },
    {
      "loss": 10.9242,
      "grad_norm": 4.549622535705566,
      "learning_rate": 1.0945273631840797e-07,
      "epoch": 0.16583747927031509
    },
    {
      "loss": 10.8563,
      "grad_norm": 4.168105602264404,
      "learning_rate": 1.6473189607517967e-07,
      "epoch": 0.24875621890547264
    },
    {
      "loss": 10.7484,
      "grad_norm": 4.485222816467285,
      "learning_rate": 2.2001105583195135e-07,
      "epoch": 0.33167495854063017
    },
    {
      "loss": 10.607,
      "grad_norm": 4.015215873718262,
      "learning_rate": 2.752902155887231e-07,
      "epoch": 0.41459369817578773
    },
    {
      "loss": 10.4584,
      "grad_norm": 3.7590181827545166,
      "learning_rate": 3.3056937534549477e-07,
      "epoch": 0.4975124378109453
    },
    {
      "loss": 10.3247,
      "grad_norm": 3.2363831996917725,
      "learning_rate": 3.8584853510226653e-07,
      "epoch": 0.5804311774461028
    },
    {
      "loss": 10.2045,
      "grad_norm": 3.1008269786834717,
      "learning_rate": 4.411276948590382e-07,
      "epoch": 0.6633499170812603
    },
    {
      "loss": 10.0877,
      "grad_norm": 3.0451407432556152,
      "learning_rate": 4.964068546158099e-07,
      "epoch": 0.746268656716418
    },
    {
      "loss": 10.0033,
      "grad_norm": 2.772944927215576,
      "learning_rate": 5.516860143725816e-07,
      "epoch": 0.8291873963515755
    },
    {
      "loss": 9.9431,
      "grad_norm": 2.675131320953369,
      "learning_rate": 6.069651741293533e-07,
      "epoch": 0.912106135986733
    },
    {
      "loss": 9.8636,
      "grad_norm": 2.739267110824585,
      "learning_rate": 6.62244333886125e-07,
      "epoch": 0.9950248756218906
    },
    {
      "loss": 9.8248,
      "grad_norm": 2.746826648712158,
      "learning_rate": 7.175234936428966e-07,
      "epoch": 1.077943615257048
    },
    {
      "loss": 9.7678,
      "grad_norm": 2.615436315536499,
      "learning_rate": 7.728026533996684e-07,
      "epoch": 1.1608623548922057
    },
    {
      "loss": 9.7153,
      "grad_norm": 2.7660531997680664,
      "learning_rate": 8.280818131564402e-07,
      "epoch": 1.243781094527363
    },
    {
      "loss": 9.6619,
      "grad_norm": 2.497063398361206,
      "learning_rate": 8.833609729132118e-07,
      "epoch": 1.3266998341625207
    },
    {
      "loss": 9.6319,
      "grad_norm": 2.9219963550567627,
      "learning_rate": 9.386401326699835e-07,
      "epoch": 1.4096185737976783
    },
    {
      "loss": 9.585,
      "grad_norm": 2.650625467300415,
      "learning_rate": 9.939192924267551e-07,
      "epoch": 1.4925373134328357
    },
    {
      "loss": 9.5446,
      "grad_norm": 2.7181077003479004,
      "learning_rate": 1.049198452183527e-06,
      "epoch": 1.5754560530679935
    },
    {
      "loss": 9.5013,
      "grad_norm": 2.53474497795105,
      "learning_rate": 1.1044776119402987e-06,
      "epoch": 1.658374792703151
    },
    {
      "loss": 9.44,
      "grad_norm": 2.5653345584869385,
      "learning_rate": 1.1597567716970703e-06,
      "epoch": 1.7412935323383083
    },
    {
      "loss": 9.4015,
      "grad_norm": 2.6627917289733887,
      "learning_rate": 1.215035931453842e-06,
      "epoch": 1.8242122719734661
    },
    {
      "loss": 9.357,
      "grad_norm": 2.401170253753662,
      "learning_rate": 1.2703150912106138e-06,
      "epoch": 1.9071310116086235
    },
    {
      "loss": 9.3083,
      "grad_norm": 2.6560723781585693,
      "learning_rate": 1.3255942509673855e-06,
      "epoch": 1.9900497512437811
    },
    {
      "loss": 9.2621,
      "grad_norm": 2.293715715408325,
      "learning_rate": 1.380873410724157e-06,
      "epoch": 2.0729684908789388
    },
    {
      "loss": 9.2048,
      "grad_norm": 2.6726884841918945,
      "learning_rate": 1.4361525704809288e-06,
      "epoch": 2.155887230514096
    },
    {
      "loss": 9.1319,
      "grad_norm": 2.4839704036712646,
      "learning_rate": 1.4914317302377005e-06,
      "epoch": 2.2388059701492535
    },
    {
      "loss": 9.0888,
      "grad_norm": 2.3492417335510254,
      "learning_rate": 1.5467108899944721e-06,
      "epoch": 2.3217247097844114
    },
    {
      "loss": 9.0645,
      "grad_norm": 2.525355577468872,
      "learning_rate": 1.601990049751244e-06,
      "epoch": 2.4046434494195688
    },
    {
      "loss": 8.988,
      "grad_norm": 2.28594970703125,
      "learning_rate": 1.6572692095080156e-06,
      "epoch": 2.487562189054726
    },
    {
      "loss": 8.9536,
      "grad_norm": 2.444484233856201,
      "learning_rate": 1.7125483692647873e-06,
      "epoch": 2.570480928689884
    },
    {
      "loss": 8.901,
      "grad_norm": 2.2346997261047363,
      "learning_rate": 1.7678275290215592e-06,
      "epoch": 2.6533996683250414
    },
    {
      "loss": 8.8035,
      "grad_norm": 2.2385382652282715,
      "learning_rate": 1.8231066887783308e-06,
      "epoch": 2.7363184079601988
    },
    {
      "loss": 8.7757,
      "grad_norm": 2.2244179248809814,
      "learning_rate": 1.8783858485351025e-06,
      "epoch": 2.8192371475953566
    },
    {
      "loss": 8.716,
      "grad_norm": 2.1351704597473145,
      "learning_rate": 1.9336650082918743e-06,
      "epoch": 2.902155887230514
    },
    {
      "loss": 8.6659,
      "grad_norm": 2.2782070636749268,
      "learning_rate": 1.988944168048646e-06,
      "epoch": 2.9850746268656714
    },
    {
      "loss": 8.5815,
      "grad_norm": 2.3956830501556396,
      "learning_rate": 2.0442233278054172e-06,
      "epoch": 3.067993366500829
    },
    {
      "loss": 8.5174,
      "grad_norm": 2.2015366554260254,
      "learning_rate": 2.099502487562189e-06,
      "epoch": 3.1509121061359866
    },
    {
      "loss": 8.4882,
      "grad_norm": 2.0785107612609863,
      "learning_rate": 2.154781647318961e-06,
      "epoch": 3.2338308457711444
    },
    {
      "loss": 8.4066,
      "grad_norm": 2.0011067390441895,
      "learning_rate": 2.2100608070757324e-06,
      "epoch": 3.316749585406302
    },
    {
      "loss": 8.3516,
      "grad_norm": 2.0129621028900146,
      "learning_rate": 2.2653399668325043e-06,
      "epoch": 3.399668325041459
    },
    {
      "loss": 8.2808,
      "grad_norm": 2.1556663513183594,
      "learning_rate": 2.320619126589276e-06,
      "epoch": 3.482587064676617
    },
    {
      "loss": 8.2244,
      "grad_norm": 1.8787713050842285,
      "learning_rate": 2.3758982863460476e-06,
      "epoch": 3.5655058043117744
    },
    {
      "loss": 8.1779,
      "grad_norm": 1.953739047050476,
      "learning_rate": 2.4311774461028195e-06,
      "epoch": 3.6484245439469323
    },
    {
      "loss": 8.1066,
      "grad_norm": 1.9441967010498047,
      "learning_rate": 2.4864566058595913e-06,
      "epoch": 3.7313432835820897
    },
    {
      "loss": 8.0674,
      "grad_norm": 1.991795301437378,
      "learning_rate": 2.5417357656163628e-06,
      "epoch": 3.814262023217247
    },
    {
      "loss": 7.9867,
      "grad_norm": 1.8914352655410767,
      "learning_rate": 2.5970149253731346e-06,
      "epoch": 3.897180762852405
    },
    {
      "loss": 7.9411,
      "grad_norm": 1.6657747030258179,
      "learning_rate": 2.652294085129906e-06,
      "epoch": 3.9800995024875623
    },
    {
      "loss": 7.8694,
      "grad_norm": 1.8613905906677246,
      "learning_rate": 2.707573244886678e-06,
      "epoch": 4.06301824212272
    },
    {
      "loss": 7.8063,
      "grad_norm": 1.9291983842849731,
      "learning_rate": 2.7628524046434494e-06,
      "epoch": 4.1459369817578775
    },
    {
      "loss": 7.7454,
      "grad_norm": 1.7083543539047241,
      "learning_rate": 2.8181315644002217e-06,
      "epoch": 4.2288557213930345
    },
    {
      "loss": 7.7271,
      "grad_norm": 1.596941351890564,
      "learning_rate": 2.873410724156993e-06,
      "epoch": 4.311774461028192
    },
    {
      "loss": 7.6654,
      "grad_norm": 1.6216955184936523,
      "learning_rate": 2.9286898839137646e-06,
      "epoch": 4.39469320066335
    },
    {
      "loss": 7.6028,
      "grad_norm": 1.6963496208190918,
      "learning_rate": 2.9839690436705365e-06,
      "epoch": 4.477611940298507
    },
    {
      "loss": 7.5719,
      "grad_norm": 1.5831496715545654,
      "learning_rate": 3.039248203427308e-06,
      "epoch": 4.560530679933665
    },
    {
      "loss": 7.5314,
      "grad_norm": 1.6628557443618774,
      "learning_rate": 3.09452736318408e-06,
      "epoch": 4.643449419568823
    },
    {
      "loss": 7.462,
      "grad_norm": 1.5095751285552979,
      "learning_rate": 3.1498065229408516e-06,
      "epoch": 4.726368159203981
    },
    {
      "loss": 7.4419,
      "grad_norm": 1.7124309539794922,
      "learning_rate": 3.2050856826976235e-06,
      "epoch": 4.8092868988391375
    },
    {
      "loss": 7.4076,
      "grad_norm": 1.5965691804885864,
      "learning_rate": 3.260364842454395e-06,
      "epoch": 4.892205638474295
    },
    {
      "loss": 7.4028,
      "grad_norm": 1.5632094144821167,
      "learning_rate": 3.315644002211167e-06,
      "epoch": 4.975124378109452
    },
    {
      "loss": 7.3582,
      "grad_norm": 1.5546607971191406,
      "learning_rate": 3.3709231619679383e-06,
      "epoch": 5.05804311774461
    },
    {
      "loss": 7.2983,
      "grad_norm": 1.4542357921600342,
      "learning_rate": 3.4262023217247097e-06,
      "epoch": 5.140961857379768
    },
    {
      "loss": 7.285,
      "grad_norm": 1.4676111936569214,
      "learning_rate": 3.481481481481482e-06,
      "epoch": 5.223880597014926
    },
    {
      "loss": 7.2676,
      "grad_norm": 1.4789509773254395,
      "learning_rate": 3.5367606412382534e-06,
      "epoch": 5.306799336650083
    },
    {
      "loss": 7.2364,
      "grad_norm": 1.4203070402145386,
      "learning_rate": 3.5920398009950253e-06,
      "epoch": 5.389718076285241
    },
    {
      "loss": 7.2037,
      "grad_norm": 1.6913678646087646,
      "learning_rate": 3.6473189607517968e-06,
      "epoch": 5.472636815920398
    },
    {
      "loss": 7.18,
      "grad_norm": 1.4575480222702026,
      "learning_rate": 3.7025981205085686e-06,
      "epoch": 5.555555555555555
    },
    {
      "loss": 7.1825,
      "grad_norm": 1.4059863090515137,
      "learning_rate": 3.75787728026534e-06,
      "epoch": 5.638474295190713
    },
    {
      "loss": 7.1655,
      "grad_norm": 1.3368442058563232,
      "learning_rate": 3.8131564400221124e-06,
      "epoch": 5.721393034825871
    },
    {
      "loss": 7.109,
      "grad_norm": 1.5474547147750854,
      "learning_rate": 3.868435599778883e-06,
      "epoch": 5.804311774461028
    },
    {
      "loss": 7.1194,
      "grad_norm": 1.548478126525879,
      "learning_rate": 3.923714759535655e-06,
      "epoch": 5.887230514096186
    },
    {
      "loss": 7.0859,
      "grad_norm": 1.4362413883209229,
      "learning_rate": 3.978993919292427e-06,
      "epoch": 5.970149253731344
    },
    {
      "loss": 7.0664,
      "grad_norm": 1.414383888244629,
      "learning_rate": 4.034273079049198e-06,
      "epoch": 6.053067993366501
    },
    {
      "loss": 7.0763,
      "grad_norm": 1.417783260345459,
      "learning_rate": 4.089552238805971e-06,
      "epoch": 6.135986733001658
    },
    {
      "loss": 7.0538,
      "grad_norm": 1.4196810722351074,
      "learning_rate": 4.144831398562742e-06,
      "epoch": 6.218905472636816
    },
    {
      "loss": 7.0181,
      "grad_norm": 1.328866958618164,
      "learning_rate": 4.200110558319514e-06,
      "epoch": 6.301824212271973
    },
    {
      "loss": 6.999,
      "grad_norm": 1.5315864086151123,
      "learning_rate": 4.255389718076286e-06,
      "epoch": 6.384742951907131
    },
    {
      "loss": 6.9614,
      "grad_norm": 1.4533332586288452,
      "learning_rate": 4.3106688778330575e-06,
      "epoch": 6.467661691542289
    },
    {
      "loss": 6.9383,
      "grad_norm": 1.517183542251587,
      "learning_rate": 4.3659480375898285e-06,
      "epoch": 6.550580431177446
    },
    {
      "loss": 6.9177,
      "grad_norm": 1.6613593101501465,
      "learning_rate": 4.421227197346601e-06,
      "epoch": 6.633499170812604
    },
    {
      "loss": 6.893,
      "grad_norm": 1.582265853881836,
      "learning_rate": 4.476506357103372e-06,
      "epoch": 6.7164179104477615
    },
    {
      "loss": 6.8593,
      "grad_norm": 1.5335623025894165,
      "learning_rate": 4.531785516860144e-06,
      "epoch": 6.799336650082918
    },
    {
      "loss": 6.8293,
      "grad_norm": 1.6206082105636597,
      "learning_rate": 4.587064676616916e-06,
      "epoch": 6.882255389718076
    },
    {
      "loss": 6.8214,
      "grad_norm": 1.558048963546753,
      "learning_rate": 4.642343836373687e-06,
      "epoch": 6.965174129353234
    },
    {
      "loss": 6.7789,
      "grad_norm": 1.649100661277771,
      "learning_rate": 4.697622996130459e-06,
      "epoch": 7.048092868988391
    },
    {
      "loss": 6.7782,
      "grad_norm": 1.6722900867462158,
      "learning_rate": 4.752902155887231e-06,
      "epoch": 7.131011608623549
    },
    {
      "loss": 6.7434,
      "grad_norm": 1.6320935487747192,
      "learning_rate": 4.808181315644003e-06,
      "epoch": 7.213930348258707
    },
    {
      "loss": 6.7036,
      "grad_norm": 1.8857557773590088,
      "learning_rate": 4.8634604754007745e-06,
      "epoch": 7.296849087893864
    },
    {
      "loss": 6.6855,
      "grad_norm": 1.7497819662094116,
      "learning_rate": 4.918739635157546e-06,
      "epoch": 7.3797678275290215
    },
    {
      "loss": 6.663,
      "grad_norm": 1.9301944971084595,
      "learning_rate": 4.974018794914317e-06,
      "epoch": 7.462686567164179
    },
    {
      "loss": 6.6692,
      "grad_norm": 1.759088158607483,
      "learning_rate": 5.029297954671089e-06,
      "epoch": 7.545605306799336
    },
    {
      "loss": 6.6114,
      "grad_norm": 1.9105181694030762,
      "learning_rate": 5.084577114427861e-06,
      "epoch": 7.628524046434494
    },
    {
      "loss": 6.6032,
      "grad_norm": 1.8860806226730347,
      "learning_rate": 5.139856274184632e-06,
      "epoch": 7.711442786069652
    },
    {
      "loss": 6.6078,
      "grad_norm": 1.851237416267395,
      "learning_rate": 5.195135433941405e-06,
      "epoch": 7.79436152570481
    },
    {
      "loss": 6.5359,
      "grad_norm": 1.9155042171478271,
      "learning_rate": 5.250414593698177e-06,
      "epoch": 7.877280265339967
    },
    {
      "loss": 6.5204,
      "grad_norm": 1.8901816606521606,
      "learning_rate": 5.305693753454948e-06,
      "epoch": 7.960199004975125
    },
    {
      "loss": 6.5034,
      "grad_norm": 2.0068373680114746,
      "learning_rate": 5.36097291321172e-06,
      "epoch": 8.043117744610282
    },
    {
      "loss": 6.5231,
      "grad_norm": 1.9599051475524902,
      "learning_rate": 5.416252072968491e-06,
      "epoch": 8.12603648424544
    },
    {
      "loss": 6.4598,
      "grad_norm": 2.0953476428985596,
      "learning_rate": 5.471531232725263e-06,
      "epoch": 8.208955223880597
    },
    {
      "loss": 6.4462,
      "grad_norm": 1.9975690841674805,
      "learning_rate": 5.526810392482035e-06,
      "epoch": 8.291873963515755
    },
    {
      "loss": 6.4307,
      "grad_norm": 1.860457420349121,
      "learning_rate": 5.582089552238806e-06,
      "epoch": 8.374792703150913
    },
    {
      "loss": 6.4215,
      "grad_norm": 1.9238072633743286,
      "learning_rate": 5.637368711995578e-06,
      "epoch": 8.457711442786069
    },
    {
      "loss": 6.3845,
      "grad_norm": 1.9647798538208008,
      "learning_rate": 5.69264787175235e-06,
      "epoch": 8.540630182421227
    },
    {
      "loss": 6.373,
      "grad_norm": 2.057391881942749,
      "learning_rate": 5.747927031509122e-06,
      "epoch": 8.623548922056385
    },
    {
      "loss": 6.3393,
      "grad_norm": 1.929376482963562,
      "learning_rate": 5.803206191265893e-06,
      "epoch": 8.706467661691542
    },
    {
      "loss": 6.3159,
      "grad_norm": 2.205397367477417,
      "learning_rate": 5.858485351022665e-06,
      "epoch": 8.7893864013267
    },
    {
      "loss": 6.3122,
      "grad_norm": 2.1180810928344727,
      "learning_rate": 5.913764510779436e-06,
      "epoch": 8.872305140961858
    },
    {
      "loss": 6.2616,
      "grad_norm": 2.2057225704193115,
      "learning_rate": 5.9690436705362084e-06,
      "epoch": 8.955223880597014
    },
    {
      "loss": 6.2657,
      "grad_norm": 2.1447715759277344,
      "learning_rate": 6.02432283029298e-06,
      "epoch": 9.038142620232172
    },
    {
      "loss": 6.2483,
      "grad_norm": 2.149600028991699,
      "learning_rate": 6.079601990049751e-06,
      "epoch": 9.12106135986733
    },
    {
      "loss": 6.2443,
      "grad_norm": 2.1924540996551514,
      "learning_rate": 6.134881149806523e-06,
      "epoch": 9.203980099502488
    },
    {
      "loss": 6.2253,
      "grad_norm": 2.1270806789398193,
      "learning_rate": 6.190160309563296e-06,
      "epoch": 9.286898839137645
    },
    {
      "loss": 6.2131,
      "grad_norm": 2.1898679733276367,
      "learning_rate": 6.245439469320067e-06,
      "epoch": 9.369817578772803
    },
    {
      "loss": 6.1756,
      "grad_norm": 2.2323975563049316,
      "learning_rate": 6.300718629076839e-06,
      "epoch": 9.45273631840796
    },
    {
      "loss": 6.1628,
      "grad_norm": 2.2351338863372803,
      "learning_rate": 6.35599778883361e-06,
      "epoch": 9.535655058043117
    },
    {
      "loss": 6.1509,
      "grad_norm": 2.113097906112671,
      "learning_rate": 6.411276948590382e-06,
      "epoch": 9.618573797678275
    },
    {
      "loss": 6.1052,
      "grad_norm": 2.365041732788086,
      "learning_rate": 6.4665561083471536e-06,
      "epoch": 9.701492537313433
    },
    {
      "loss": 6.0775,
      "grad_norm": 2.40736722946167,
      "learning_rate": 6.5218352681039254e-06,
      "epoch": 9.78441127694859
    },
    {
      "loss": 6.0906,
      "grad_norm": 2.2615773677825928,
      "learning_rate": 6.5771144278606965e-06,
      "epoch": 9.867330016583749
    },
    {
      "loss": 6.0672,
      "grad_norm": 2.287912607192993,
      "learning_rate": 6.632393587617468e-06,
      "epoch": 9.950248756218905
    },
    {
      "loss": 6.0704,
      "grad_norm": 2.3913731575012207,
      "learning_rate": 6.687672747374241e-06,
      "epoch": 10.033167495854062
    },
    {
      "loss": 6.0438,
      "grad_norm": 2.26442813873291,
      "learning_rate": 6.742951907131012e-06,
      "epoch": 10.11608623548922
    },
    {
      "loss": 6.0168,
      "grad_norm": 2.401289463043213,
      "learning_rate": 6.798231066887784e-06,
      "epoch": 10.199004975124378
    },
    {
      "loss": 6.0195,
      "grad_norm": 2.3927481174468994,
      "learning_rate": 6.853510226644555e-06,
      "epoch": 10.281923714759536
    },
    {
      "loss": 5.9736,
      "grad_norm": 2.389756441116333,
      "learning_rate": 6.908789386401328e-06,
      "epoch": 10.364842454394694
    },
    {
      "loss": 5.9814,
      "grad_norm": 2.4473352432250977,
      "learning_rate": 6.9640685461580995e-06,
      "epoch": 10.447761194029852
    },
    {
      "loss": 5.971,
      "grad_norm": 2.4850597381591797,
      "learning_rate": 7.0193477059148706e-06,
      "epoch": 10.530679933665008
    },
    {
      "loss": 5.9575,
      "grad_norm": 2.2944717407226562,
      "learning_rate": 7.0746268656716424e-06,
      "epoch": 10.613598673300165
    },
    {
      "loss": 5.9475,
      "grad_norm": 2.392334222793579,
      "learning_rate": 7.1299060254284134e-06,
      "epoch": 10.696517412935323
    },
    {
      "loss": 5.9097,
      "grad_norm": 2.1638238430023193,
      "learning_rate": 7.185185185185186e-06,
      "epoch": 10.779436152570481
    },
    {
      "loss": 5.923,
      "grad_norm": 2.5088016986846924,
      "learning_rate": 7.240464344941957e-06,
      "epoch": 10.862354892205639
    },
    {
      "loss": 5.8914,
      "grad_norm": 2.6537327766418457,
      "learning_rate": 7.295743504698729e-06,
      "epoch": 10.945273631840797
    },
    {
      "loss": 5.8703,
      "grad_norm": 2.300137758255005,
      "learning_rate": 7.351022664455501e-06,
      "epoch": 11.028192371475953
    },
    {
      "loss": 5.8798,
      "grad_norm": 2.436129570007324,
      "learning_rate": 7.406301824212273e-06,
      "epoch": 11.11111111111111
    },
    {
      "loss": 5.8248,
      "grad_norm": 2.4752395153045654,
      "learning_rate": 7.461580983969045e-06,
      "epoch": 11.194029850746269
    },
    {
      "loss": 5.8148,
      "grad_norm": 2.404580593109131,
      "learning_rate": 7.516860143725816e-06,
      "epoch": 11.276948590381426
    },
    {
      "loss": 5.823,
      "grad_norm": 2.4921717643737793,
      "learning_rate": 7.5721393034825875e-06,
      "epoch": 11.359867330016584
    },
    {
      "loss": 5.8022,
      "grad_norm": 2.4516446590423584,
      "learning_rate": 7.6274184632393586e-06,
      "epoch": 11.442786069651742
    },
    {
      "loss": 5.7439,
      "grad_norm": 2.549602746963501,
      "learning_rate": 7.682697622996131e-06,
      "epoch": 11.525704809286898
    },
    {
      "loss": 5.8003,
      "grad_norm": 2.8656153678894043,
      "learning_rate": 7.737976782752903e-06,
      "epoch": 11.608623548922056
    },
    {
      "loss": 5.7476,
      "grad_norm": 2.855980396270752,
      "learning_rate": 7.793255942509675e-06,
      "epoch": 11.691542288557214
    },
    {
      "loss": 5.7274,
      "grad_norm": 2.7095847129821777,
      "learning_rate": 7.848535102266445e-06,
      "epoch": 11.774461028192372
    },
    {
      "loss": 5.7334,
      "grad_norm": 2.508673906326294,
      "learning_rate": 7.903814262023219e-06,
      "epoch": 11.85737976782753
    },
    {
      "loss": 5.6997,
      "grad_norm": 2.6512959003448486,
      "learning_rate": 7.95909342177999e-06,
      "epoch": 11.940298507462687
    },
    {
      "loss": 5.6878,
      "grad_norm": 2.42924165725708,
      "learning_rate": 8.01437258153676e-06,
      "epoch": 12.023217247097843
    },
    {
      "loss": 5.6668,
      "grad_norm": 2.498377799987793,
      "learning_rate": 8.069651741293533e-06,
      "epoch": 12.106135986733001
    },
    {
      "loss": 5.6633,
      "grad_norm": 2.4608662128448486,
      "learning_rate": 8.124930901050305e-06,
      "epoch": 12.189054726368159
    },
    {
      "loss": 5.6324,
      "grad_norm": 2.3502681255340576,
      "learning_rate": 8.180210060807076e-06,
      "epoch": 12.271973466003317
    },
    {
      "loss": 5.6224,
      "grad_norm": 2.4463815689086914,
      "learning_rate": 8.235489220563848e-06,
      "epoch": 12.354892205638475
    },
    {
      "loss": 5.6523,
      "grad_norm": 2.413620710372925,
      "learning_rate": 8.29076838032062e-06,
      "epoch": 12.437810945273633
    },
    {
      "loss": 5.6421,
      "grad_norm": 2.4983043670654297,
      "learning_rate": 8.34604754007739e-06,
      "epoch": 12.520729684908789
    },
    {
      "loss": 5.6258,
      "grad_norm": 2.5846569538116455,
      "learning_rate": 8.401326699834164e-06,
      "epoch": 12.603648424543946
    },
    {
      "loss": 5.5907,
      "grad_norm": 2.3645215034484863,
      "learning_rate": 8.456605859590936e-06,
      "epoch": 12.686567164179104
    },
    {
      "loss": 5.5878,
      "grad_norm": 2.3220860958099365,
      "learning_rate": 8.511885019347706e-06,
      "epoch": 12.769485903814262
    },
    {
      "loss": 5.5442,
      "grad_norm": 2.541339159011841,
      "learning_rate": 8.567164179104478e-06,
      "epoch": 12.85240464344942
    },
    {
      "loss": 5.5272,
      "grad_norm": 2.4082655906677246,
      "learning_rate": 8.622443338861251e-06,
      "epoch": 12.935323383084578
    },
    {
      "loss": 5.5382,
      "grad_norm": 2.4534928798675537,
      "learning_rate": 8.677722498618022e-06,
      "epoch": 13.018242122719734
    },
    {
      "loss": 5.5114,
      "grad_norm": 2.5675547122955322,
      "learning_rate": 8.733001658374793e-06,
      "epoch": 13.101160862354892
    },
    {
      "loss": 5.4866,
      "grad_norm": 2.4529683589935303,
      "learning_rate": 8.788280818131565e-06,
      "epoch": 13.18407960199005
    },
    {
      "loss": 5.4763,
      "grad_norm": 2.5588016510009766,
      "learning_rate": 8.843559977888337e-06,
      "epoch": 13.266998341625207
    },
    {
      "loss": 5.4455,
      "grad_norm": 2.44097900390625,
      "learning_rate": 8.898839137645109e-06,
      "epoch": 13.349917081260365
    },
    {
      "loss": 5.4555,
      "grad_norm": 2.9660534858703613,
      "learning_rate": 8.954118297401881e-06,
      "epoch": 13.432835820895523
    },
    {
      "loss": 5.4657,
      "grad_norm": 2.6088316440582275,
      "learning_rate": 9.009397457158651e-06,
      "epoch": 13.51575456053068
    },
    {
      "loss": 5.4424,
      "grad_norm": 2.4225690364837646,
      "learning_rate": 9.064676616915423e-06,
      "epoch": 13.598673300165837
    },
    {
      "loss": 5.4505,
      "grad_norm": 2.6229488849639893,
      "learning_rate": 9.119955776672196e-06,
      "epoch": 13.681592039800995
    },
    {
      "loss": 5.4305,
      "grad_norm": 2.5143179893493652,
      "learning_rate": 9.175234936428967e-06,
      "epoch": 13.764510779436153
    },
    {
      "loss": 5.3949,
      "grad_norm": 2.5874409675598145,
      "learning_rate": 9.230514096185739e-06,
      "epoch": 13.84742951907131
    },
    {
      "loss": 5.3673,
      "grad_norm": 2.3625547885894775,
      "learning_rate": 9.28579325594251e-06,
      "epoch": 13.930348258706468
    },
    {
      "loss": 5.3688,
      "grad_norm": 2.505998134613037,
      "learning_rate": 9.341072415699282e-06,
      "epoch": 14.013266998341626
    },
    {
      "loss": 5.3647,
      "grad_norm": 2.626058578491211,
      "learning_rate": 9.396351575456054e-06,
      "epoch": 14.096185737976782
    },
    {
      "loss": 5.364,
      "grad_norm": 2.4693286418914795,
      "learning_rate": 9.451630735212826e-06,
      "epoch": 14.17910447761194
    },
    {
      "loss": 5.3046,
      "grad_norm": 2.444755792617798,
      "learning_rate": 9.506909894969598e-06,
      "epoch": 14.262023217247098
    },
    {
      "loss": 5.3435,
      "grad_norm": 2.416337251663208,
      "learning_rate": 9.562189054726368e-06,
      "epoch": 14.344941956882256
    },
    {
      "loss": 5.3101,
      "grad_norm": 2.583552122116089,
      "learning_rate": 9.617468214483142e-06,
      "epoch": 14.427860696517413
    },
    {
      "loss": 5.258,
      "grad_norm": 2.705463409423828,
      "learning_rate": 9.672747374239912e-06,
      "epoch": 14.510779436152571
    },
    {
      "loss": 5.2932,
      "grad_norm": 2.92818284034729,
      "learning_rate": 9.728026533996684e-06,
      "epoch": 14.593698175787727
    },
    {
      "loss": 5.2822,
      "grad_norm": 2.5978965759277344,
      "learning_rate": 9.783305693753456e-06,
      "epoch": 14.676616915422885
    },
    {
      "loss": 5.246,
      "grad_norm": 2.404097557067871,
      "learning_rate": 9.838584853510227e-06,
      "epoch": 14.759535655058043
    },
    {
      "loss": 5.2505,
      "grad_norm": 2.46894907951355,
      "learning_rate": 9.893864013267e-06,
      "epoch": 14.8424543946932
    },
    {
      "loss": 5.2314,
      "grad_norm": 2.4202334880828857,
      "learning_rate": 9.949143173023771e-06,
      "epoch": 14.925373134328359
    },
    {
      "loss": 5.2521,
      "grad_norm": 2.3481662273406982,
      "learning_rate": 1.0004422332780543e-05,
      "epoch": 15.008291873963516
    },
    {
      "loss": 5.1649,
      "grad_norm": 2.403796434402466,
      "learning_rate": 1.0059701492537315e-05,
      "epoch": 15.091210613598673
    },
    {
      "loss": 5.2003,
      "grad_norm": 2.358527898788452,
      "learning_rate": 1.0114980652294087e-05,
      "epoch": 15.17412935323383
    },
    {
      "loss": 5.1765,
      "grad_norm": 2.644507646560669,
      "learning_rate": 1.0170259812050859e-05,
      "epoch": 15.257048092868988
    },
    {
      "loss": 5.1793,
      "grad_norm": 2.765592575073242,
      "learning_rate": 1.0225538971807629e-05,
      "epoch": 15.339966832504146
    },
    {
      "loss": 5.1649,
      "grad_norm": 2.4003446102142334,
      "learning_rate": 1.02808181315644e-05,
      "epoch": 15.422885572139304
    },
    {
      "loss": 5.1523,
      "grad_norm": 2.499962091445923,
      "learning_rate": 1.0336097291321172e-05,
      "epoch": 15.505804311774462
    },
    {
      "loss": 5.174,
      "grad_norm": 2.365406036376953,
      "learning_rate": 1.0391376451077944e-05,
      "epoch": 15.588723051409618
    },
    {
      "loss": 5.0915,
      "grad_norm": 2.5528688430786133,
      "learning_rate": 1.0446655610834718e-05,
      "epoch": 15.671641791044776
    },
    {
      "loss": 5.1216,
      "grad_norm": 2.496272087097168,
      "learning_rate": 1.0501934770591488e-05,
      "epoch": 15.754560530679933
    },
    {
      "loss": 5.1102,
      "grad_norm": 2.8393497467041016,
      "learning_rate": 1.055721393034826e-05,
      "epoch": 15.837479270315091
    },
    {
      "loss": 5.0831,
      "grad_norm": 2.57500958442688,
      "learning_rate": 1.0612493090105032e-05,
      "epoch": 15.92039800995025
    },
    {
      "loss": 5.1148,
      "grad_norm": 2.554312229156494,
      "learning_rate": 1.0667772249861804e-05,
      "epoch": 16.003316749585405
    },
    {
      "loss": 5.0897,
      "grad_norm": 2.3558995723724365,
      "learning_rate": 1.0723051409618574e-05,
      "epoch": 16.086235489220563
    },
    {
      "loss": 5.0725,
      "grad_norm": 2.622974157333374,
      "learning_rate": 1.0778330569375346e-05,
      "epoch": 16.16915422885572
    },
    {
      "loss": 5.0691,
      "grad_norm": 2.668390989303589,
      "learning_rate": 1.0833609729132118e-05,
      "epoch": 16.25207296849088
    },
    {
      "loss": 5.0325,
      "grad_norm": 2.4059512615203857,
      "learning_rate": 1.088888888888889e-05,
      "epoch": 16.334991708126037
    },
    {
      "loss": 5.024,
      "grad_norm": 2.691390037536621,
      "learning_rate": 1.0944168048645663e-05,
      "epoch": 16.417910447761194
    },
    {
      "loss": 4.9935,
      "grad_norm": 2.388420343399048,
      "learning_rate": 1.0999447208402433e-05,
      "epoch": 16.500829187396352
    },
    {
      "loss": 4.991,
      "grad_norm": 2.4687843322753906,
      "learning_rate": 1.1054726368159205e-05,
      "epoch": 16.58374792703151
    },
    {
      "loss": 5.0076,
      "grad_norm": 2.4945247173309326,
      "learning_rate": 1.1110005527915977e-05,
      "epoch": 16.666666666666668
    },
    {
      "loss": 4.9923,
      "grad_norm": 2.435224771499634,
      "learning_rate": 1.1165284687672749e-05,
      "epoch": 16.749585406301826
    },
    {
      "loss": 4.98,
      "grad_norm": 2.3674659729003906,
      "learning_rate": 1.1220563847429519e-05,
      "epoch": 16.83250414593698
    },
    {
      "loss": 4.9648,
      "grad_norm": 2.590312957763672,
      "learning_rate": 1.1275843007186291e-05,
      "epoch": 16.915422885572138
    },
    {
      "loss": 4.9362,
      "grad_norm": 2.6689085960388184,
      "learning_rate": 1.1331122166943063e-05,
      "epoch": 16.998341625207296
    },
    {
      "loss": 4.9245,
      "grad_norm": 2.420269727706909,
      "learning_rate": 1.1386401326699835e-05,
      "epoch": 17.081260364842453
    },
    {
      "loss": 4.9339,
      "grad_norm": 2.6594974994659424,
      "learning_rate": 1.1441680486456608e-05,
      "epoch": 17.16417910447761
    },
    {
      "loss": 4.9294,
      "grad_norm": 2.548335552215576,
      "learning_rate": 1.149695964621338e-05,
      "epoch": 17.24709784411277
    },
    {
      "loss": 4.9167,
      "grad_norm": 2.2593352794647217,
      "learning_rate": 1.155223880597015e-05,
      "epoch": 17.330016583747927
    },
    {
      "loss": 4.8912,
      "grad_norm": 2.3336780071258545,
      "learning_rate": 1.1607517965726922e-05,
      "epoch": 17.412935323383085
    },
    {
      "loss": 4.8924,
      "grad_norm": 2.541323661804199,
      "learning_rate": 1.1662797125483694e-05,
      "epoch": 17.495854063018243
    },
    {
      "loss": 4.8747,
      "grad_norm": 2.4015250205993652,
      "learning_rate": 1.1718076285240466e-05,
      "epoch": 17.5787728026534
    },
    {
      "loss": 4.888,
      "grad_norm": 2.588642120361328,
      "learning_rate": 1.1773355444997236e-05,
      "epoch": 17.66169154228856
    },
    {
      "loss": 4.8749,
      "grad_norm": 2.5440609455108643,
      "learning_rate": 1.1828634604754008e-05,
      "epoch": 17.744610281923716
    },
    {
      "loss": 4.8883,
      "grad_norm": 2.641991376876831,
      "learning_rate": 1.188391376451078e-05,
      "epoch": 17.827529021558874
    },
    {
      "loss": 4.8599,
      "grad_norm": 2.532076597213745,
      "learning_rate": 1.1939192924267553e-05,
      "epoch": 17.91044776119403
    },
    {
      "loss": 4.8099,
      "grad_norm": 2.6154420375823975,
      "learning_rate": 1.1994472084024325e-05,
      "epoch": 17.993366500829186
    },
    {
      "loss": 4.8229,
      "grad_norm": 2.464808225631714,
      "learning_rate": 1.2049751243781095e-05,
      "epoch": 18.076285240464344
    },
    {
      "loss": 4.8226,
      "grad_norm": 2.484250545501709,
      "learning_rate": 1.2105030403537867e-05,
      "epoch": 18.1592039800995
    },
    {
      "loss": 4.8103,
      "grad_norm": 2.596205711364746,
      "learning_rate": 1.2160309563294639e-05,
      "epoch": 18.24212271973466
    },
    {
      "loss": 4.7602,
      "grad_norm": 2.465076208114624,
      "learning_rate": 1.2215588723051411e-05,
      "epoch": 18.325041459369817
    },
    {
      "loss": 4.7722,
      "grad_norm": 2.3708977699279785,
      "learning_rate": 1.2270867882808181e-05,
      "epoch": 18.407960199004975
    },
    {
      "loss": 4.7811,
      "grad_norm": 2.4525890350341797,
      "learning_rate": 1.2326147042564953e-05,
      "epoch": 18.490878938640133
    },
    {
      "loss": 4.7656,
      "grad_norm": 2.4264073371887207,
      "learning_rate": 1.2381426202321725e-05,
      "epoch": 18.57379767827529
    },
    {
      "loss": 4.7306,
      "grad_norm": 2.602304220199585,
      "learning_rate": 1.2436705362078498e-05,
      "epoch": 18.65671641791045
    },
    {
      "loss": 4.7215,
      "grad_norm": 2.381988286972046,
      "learning_rate": 1.249198452183527e-05,
      "epoch": 18.739635157545607
    },
    {
      "loss": 4.7437,
      "grad_norm": 2.3801825046539307,
      "learning_rate": 1.254726368159204e-05,
      "epoch": 18.822553897180764
    },
    {
      "loss": 4.7055,
      "grad_norm": 2.42865252494812,
      "learning_rate": 1.2602542841348812e-05,
      "epoch": 18.90547263681592
    },
    {
      "loss": 4.7336,
      "grad_norm": 2.500257730484009,
      "learning_rate": 1.2657822001105584e-05,
      "epoch": 18.988391376451077
    },
    {
      "loss": 4.7017,
      "grad_norm": 2.42043399810791,
      "learning_rate": 1.2713101160862356e-05,
      "epoch": 19.071310116086234
    },
    {
      "loss": 4.7151,
      "grad_norm": 2.499932289123535,
      "learning_rate": 1.2768380320619126e-05,
      "epoch": 19.154228855721392
    },
    {
      "loss": 4.6865,
      "grad_norm": 2.4089722633361816,
      "learning_rate": 1.2823659480375898e-05,
      "epoch": 19.23714759535655
    },
    {
      "loss": 4.6668,
      "grad_norm": 2.4539783000946045,
      "learning_rate": 1.2878938640132672e-05,
      "epoch": 19.320066334991708
    },
    {
      "loss": 4.6016,
      "grad_norm": 2.5757546424865723,
      "learning_rate": 1.2934217799889444e-05,
      "epoch": 19.402985074626866
    },
    {
      "loss": 4.6664,
      "grad_norm": 2.386625289916992,
      "learning_rate": 1.2989496959646215e-05,
      "epoch": 19.485903814262024
    },
    {
      "loss": 4.6324,
      "grad_norm": 2.61678147315979,
      "learning_rate": 1.3044776119402987e-05,
      "epoch": 19.56882255389718
    },
    {
      "loss": 4.637,
      "grad_norm": 2.473463773727417,
      "learning_rate": 1.3100055279159757e-05,
      "epoch": 19.65174129353234
    },
    {
      "loss": 4.6472,
      "grad_norm": 2.4244322776794434,
      "learning_rate": 1.315533443891653e-05,
      "epoch": 19.734660033167497
    },
    {
      "loss": 4.6046,
      "grad_norm": 2.4534265995025635,
      "learning_rate": 1.3210613598673301e-05,
      "epoch": 19.817578772802655
    },
    {
      "loss": 4.6078,
      "grad_norm": 2.5532517433166504,
      "learning_rate": 1.3265892758430073e-05,
      "epoch": 19.90049751243781
    },
    {
      "loss": 4.5704,
      "grad_norm": 2.3849074840545654,
      "learning_rate": 1.3321171918186843e-05,
      "epoch": 19.983416252072967
    },
    {
      "loss": 4.5874,
      "grad_norm": 2.422785758972168,
      "learning_rate": 1.3376451077943617e-05,
      "epoch": 20.066334991708125
    },
    {
      "loss": 4.6008,
      "grad_norm": 2.4619555473327637,
      "learning_rate": 1.3431730237700389e-05,
      "epoch": 20.149253731343283
    },
    {
      "loss": 4.591,
      "grad_norm": 2.5759036540985107,
      "learning_rate": 1.348700939745716e-05,
      "epoch": 20.23217247097844
    },
    {
      "loss": 4.5503,
      "grad_norm": 2.7162184715270996,
      "learning_rate": 1.3542288557213932e-05,
      "epoch": 20.3150912106136
    },
    {
      "loss": 4.5631,
      "grad_norm": 2.331199884414673,
      "learning_rate": 1.3597567716970703e-05,
      "epoch": 20.398009950248756
    },
    {
      "loss": 4.5501,
      "grad_norm": 2.4275221824645996,
      "learning_rate": 1.3652846876727474e-05,
      "epoch": 20.480928689883914
    },
    {
      "loss": 4.5443,
      "grad_norm": 2.5075459480285645,
      "learning_rate": 1.3708126036484246e-05,
      "epoch": 20.563847429519072
    },
    {
      "loss": 4.5519,
      "grad_norm": 2.3960485458374023,
      "learning_rate": 1.3763405196241018e-05,
      "epoch": 20.64676616915423
    },
    {
      "loss": 4.5067,
      "grad_norm": 2.375393867492676,
      "learning_rate": 1.3818684355997788e-05,
      "epoch": 20.729684908789388
    },
    {
      "loss": 4.4879,
      "grad_norm": 2.4964687824249268,
      "learning_rate": 1.3873963515754562e-05,
      "epoch": 20.812603648424545
    },
    {
      "loss": 4.4871,
      "grad_norm": 2.486602783203125,
      "learning_rate": 1.3929242675511334e-05,
      "epoch": 20.895522388059703
    },
    {
      "loss": 4.5046,
      "grad_norm": 2.3462483882904053,
      "learning_rate": 1.3984521835268106e-05,
      "epoch": 20.978441127694857
    },
    {
      "loss": 4.483,
      "grad_norm": 2.2760062217712402,
      "learning_rate": 1.4039800995024878e-05,
      "epoch": 21.061359867330015
    },
    {
      "loss": 4.4821,
      "grad_norm": 2.344048023223877,
      "learning_rate": 1.4095080154781648e-05,
      "epoch": 21.144278606965173
    },
    {
      "loss": 4.4594,
      "grad_norm": 2.6548593044281006,
      "learning_rate": 1.415035931453842e-05,
      "epoch": 21.22719734660033
    },
    {
      "loss": 4.4595,
      "grad_norm": 2.5257480144500732,
      "learning_rate": 1.4205638474295191e-05,
      "epoch": 21.31011608623549
    },
    {
      "loss": 4.4443,
      "grad_norm": 2.4036812782287598,
      "learning_rate": 1.4260917634051963e-05,
      "epoch": 21.393034825870647
    },
    {
      "loss": 4.48,
      "grad_norm": 2.643904447555542,
      "learning_rate": 1.4316196793808733e-05,
      "epoch": 21.475953565505804
    },
    {
      "loss": 4.4306,
      "grad_norm": 2.3303933143615723,
      "learning_rate": 1.4371475953565509e-05,
      "epoch": 21.558872305140962
    },
    {
      "loss": 4.4462,
      "grad_norm": 2.4412693977355957,
      "learning_rate": 1.4426755113322279e-05,
      "epoch": 21.64179104477612
    },
    {
      "loss": 4.4755,
      "grad_norm": 2.5598995685577393,
      "learning_rate": 1.448203427307905e-05,
      "epoch": 21.724709784411278
    },
    {
      "loss": 4.4005,
      "grad_norm": 2.3902432918548584,
      "learning_rate": 1.4537313432835823e-05,
      "epoch": 21.807628524046436
    },
    {
      "loss": 4.4086,
      "grad_norm": 2.4452872276306152,
      "learning_rate": 1.4592592592592594e-05,
      "epoch": 21.890547263681594
    },
    {
      "loss": 4.361,
      "grad_norm": 2.489471435546875,
      "learning_rate": 1.4647871752349365e-05,
      "epoch": 21.973466003316748
    },
    {
      "loss": 4.3933,
      "grad_norm": 2.494231939315796,
      "learning_rate": 1.4703150912106137e-05,
      "epoch": 22.056384742951906
    },
    {
      "loss": 4.3772,
      "grad_norm": 2.6109962463378906,
      "learning_rate": 1.4758430071862908e-05,
      "epoch": 22.139303482587064
    },
    {
      "loss": 4.349,
      "grad_norm": 2.4000630378723145,
      "learning_rate": 1.481370923161968e-05,
      "epoch": 22.22222222222222
    },
    {
      "loss": 4.3928,
      "grad_norm": 2.358504295349121,
      "learning_rate": 1.4868988391376454e-05,
      "epoch": 22.30514096185738
    },
    {
      "loss": 4.3824,
      "grad_norm": 2.430241346359253,
      "learning_rate": 1.4924267551133224e-05,
      "epoch": 22.388059701492537
    },
    {
      "loss": 4.3262,
      "grad_norm": 2.447984218597412,
      "learning_rate": 1.4979546710889996e-05,
      "epoch": 22.470978441127695
    },
    {
      "loss": 4.3347,
      "grad_norm": 2.3876588344573975,
      "learning_rate": 1.5034825870646768e-05,
      "epoch": 22.553897180762853
    },
    {
      "loss": 4.3795,
      "grad_norm": 2.3419296741485596,
      "learning_rate": 1.509010503040354e-05,
      "epoch": 22.63681592039801
    },
    {
      "loss": 4.3074,
      "grad_norm": 2.5417256355285645,
      "learning_rate": 1.514538419016031e-05,
      "epoch": 22.71973466003317
    },
    {
      "loss": 4.2926,
      "grad_norm": 2.4196600914001465,
      "learning_rate": 1.5200663349917082e-05,
      "epoch": 22.802653399668326
    },
    {
      "loss": 4.2922,
      "grad_norm": 2.614643096923828,
      "learning_rate": 1.5255942509673854e-05,
      "epoch": 22.885572139303484
    },
    {
      "loss": 4.2843,
      "grad_norm": 2.449038028717041,
      "learning_rate": 1.5311221669430624e-05,
      "epoch": 22.96849087893864
    },
    {
      "loss": 4.2761,
      "grad_norm": 2.5855932235717773,
      "learning_rate": 1.53665008291874e-05,
      "epoch": 23.051409618573796
    },
    {
      "loss": 4.2782,
      "grad_norm": 2.552445888519287,
      "learning_rate": 1.542177998894417e-05,
      "epoch": 23.134328358208954
    },
    {
      "loss": 4.2266,
      "grad_norm": 2.7205705642700195,
      "learning_rate": 1.5477059148700943e-05,
      "epoch": 23.217247097844112
    },
    {
      "loss": 4.2641,
      "grad_norm": 2.3774492740631104,
      "learning_rate": 1.553233830845771e-05,
      "epoch": 23.30016583747927
    },
    {
      "loss": 4.266,
      "grad_norm": 2.447430372238159,
      "learning_rate": 1.5587617468214483e-05,
      "epoch": 23.383084577114428
    },
    {
      "loss": 4.2599,
      "grad_norm": 2.397613763809204,
      "learning_rate": 1.5642896627971255e-05,
      "epoch": 23.466003316749585
    },
    {
      "loss": 4.2743,
      "grad_norm": 2.5649847984313965,
      "learning_rate": 1.5698175787728027e-05,
      "epoch": 23.548922056384743
    },
    {
      "loss": 4.2206,
      "grad_norm": 2.497708559036255,
      "learning_rate": 1.57534549474848e-05,
      "epoch": 23.6318407960199
    },
    {
      "loss": 4.2428,
      "grad_norm": 2.4241695404052734,
      "learning_rate": 1.5808734107241574e-05,
      "epoch": 23.71475953565506
    },
    {
      "loss": 4.2253,
      "grad_norm": 2.4780423641204834,
      "learning_rate": 1.5864013266998342e-05,
      "epoch": 23.797678275290217
    },
    {
      "loss": 4.2378,
      "grad_norm": 2.5025904178619385,
      "learning_rate": 1.5919292426755114e-05,
      "epoch": 23.880597014925375
    },
    {
      "loss": 4.213,
      "grad_norm": 2.4291787147521973,
      "learning_rate": 1.5974571586511886e-05,
      "epoch": 23.963515754560532
    },
    {
      "loss": 4.1813,
      "grad_norm": 2.323483467102051,
      "learning_rate": 1.6029850746268658e-05,
      "epoch": 24.046434494195687
    },
    {
      "loss": 4.2236,
      "grad_norm": 2.334641218185425,
      "learning_rate": 1.608512990602543e-05,
      "epoch": 24.129353233830845
    },
    {
      "loss": 4.206,
      "grad_norm": 2.330864906311035,
      "learning_rate": 1.6140409065782202e-05,
      "epoch": 24.212271973466002
    },
    {
      "loss": 4.1684,
      "grad_norm": 2.3944478034973145,
      "learning_rate": 1.6195688225538974e-05,
      "epoch": 24.29519071310116
    },
    {
      "loss": 4.151,
      "grad_norm": 2.443751335144043,
      "learning_rate": 1.6250967385295745e-05,
      "epoch": 24.378109452736318
    },
    {
      "loss": 4.1365,
      "grad_norm": 2.370575428009033,
      "learning_rate": 1.6306246545052517e-05,
      "epoch": 24.461028192371476
    },
    {
      "loss": 4.1679,
      "grad_norm": 2.611891269683838,
      "learning_rate": 1.636152570480929e-05,
      "epoch": 24.543946932006634
    },
    {
      "loss": 4.1295,
      "grad_norm": 2.297877788543701,
      "learning_rate": 1.641680486456606e-05,
      "epoch": 24.62686567164179
    },
    {
      "loss": 4.1643,
      "grad_norm": 2.6132757663726807,
      "learning_rate": 1.6472084024322833e-05,
      "epoch": 24.70978441127695
    },
    {
      "loss": 4.1413,
      "grad_norm": 2.537510633468628,
      "learning_rate": 1.6527363184079605e-05,
      "epoch": 24.792703150912107
    },
    {
      "loss": 4.1437,
      "grad_norm": 2.390181064605713,
      "learning_rate": 1.6582642343836373e-05,
      "epoch": 24.875621890547265
    },
    {
      "loss": 4.1395,
      "grad_norm": 2.493807554244995,
      "learning_rate": 1.6637921503593145e-05,
      "epoch": 24.958540630182423
    },
    {
      "loss": 4.1002,
      "grad_norm": 2.3271546363830566,
      "learning_rate": 1.6693200663349917e-05,
      "epoch": 25.041459369817577
    },
    {
      "loss": 4.1018,
      "grad_norm": 2.309774875640869,
      "learning_rate": 1.674847982310669e-05,
      "epoch": 25.124378109452735
    },
    {
      "loss": 4.1104,
      "grad_norm": 2.3431193828582764,
      "learning_rate": 1.6803758982863464e-05,
      "epoch": 25.207296849087893
    },
    {
      "loss": 4.0903,
      "grad_norm": 2.4706239700317383,
      "learning_rate": 1.6859038142620233e-05,
      "epoch": 25.29021558872305
    },
    {
      "loss": 4.101,
      "grad_norm": 2.3763668537139893,
      "learning_rate": 1.6914317302377004e-05,
      "epoch": 25.37313432835821
    },
    {
      "loss": 4.0654,
      "grad_norm": 2.383195161819458,
      "learning_rate": 1.6969596462133776e-05,
      "epoch": 25.456053067993366
    },
    {
      "loss": 4.0643,
      "grad_norm": 2.4399983882904053,
      "learning_rate": 1.7024875621890548e-05,
      "epoch": 25.538971807628524
    },
    {
      "loss": 4.0954,
      "grad_norm": 2.368049144744873,
      "learning_rate": 1.708015478164732e-05,
      "epoch": 25.621890547263682
    },
    {
      "loss": 4.0769,
      "grad_norm": 2.428744316101074,
      "learning_rate": 1.7135433941404092e-05,
      "epoch": 25.70480928689884
    },
    {
      "loss": 4.0907,
      "grad_norm": 2.3882944583892822,
      "learning_rate": 1.7190713101160864e-05,
      "epoch": 25.787728026533998
    },
    {
      "loss": 4.0047,
      "grad_norm": 2.29382061958313,
      "learning_rate": 1.7245992260917636e-05,
      "epoch": 25.870646766169155
    },
    {
      "loss": 4.0249,
      "grad_norm": 2.3886849880218506,
      "learning_rate": 1.7301271420674408e-05,
      "epoch": 25.953565505804313
    },
    {
      "loss": 4.0378,
      "grad_norm": 2.1973612308502197,
      "learning_rate": 1.735655058043118e-05,
      "epoch": 26.036484245439468
    },
    {
      "loss": 4.0591,
      "grad_norm": 2.289424180984497,
      "learning_rate": 1.741182974018795e-05,
      "epoch": 26.119402985074625
    },
    {
      "loss": 4.0087,
      "grad_norm": 2.5589821338653564,
      "learning_rate": 1.7467108899944723e-05,
      "epoch": 26.202321724709783
    },
    {
      "loss": 4.0401,
      "grad_norm": 2.3648934364318848,
      "learning_rate": 1.7522388059701495e-05,
      "epoch": 26.28524046434494
    },
    {
      "loss": 3.9866,
      "grad_norm": 2.6145639419555664,
      "learning_rate": 1.7577667219458267e-05,
      "epoch": 26.3681592039801
    },
    {
      "loss": 4.0074,
      "grad_norm": 2.3682613372802734,
      "learning_rate": 1.7632946379215035e-05,
      "epoch": 26.451077943615257
    },
    {
      "loss": 3.9988,
      "grad_norm": 2.376734495162964,
      "learning_rate": 1.7688225538971807e-05,
      "epoch": 26.533996683250415
    },
    {
      "loss": 3.9721,
      "grad_norm": 2.483973264694214,
      "learning_rate": 1.774350469872858e-05,
      "epoch": 26.616915422885572
    },
    {
      "loss": 4.0282,
      "grad_norm": 2.428921937942505,
      "learning_rate": 1.7798783858485354e-05,
      "epoch": 26.69983416252073
    },
    {
      "loss": 3.9959,
      "grad_norm": 2.352424144744873,
      "learning_rate": 1.7854063018242126e-05,
      "epoch": 26.782752902155888
    },
    {
      "loss": 3.9968,
      "grad_norm": 2.385869026184082,
      "learning_rate": 1.7909342177998895e-05,
      "epoch": 26.865671641791046
    },
    {
      "loss": 3.9536,
      "grad_norm": 2.3331429958343506,
      "learning_rate": 1.7964621337755667e-05,
      "epoch": 26.948590381426204
    },
    {
      "loss": 3.942,
      "grad_norm": 2.5928118228912354,
      "learning_rate": 1.801990049751244e-05,
      "epoch": 27.03150912106136
    },
    {
      "loss": 3.946,
      "grad_norm": 2.4168989658355713,
      "learning_rate": 1.807517965726921e-05,
      "epoch": 27.114427860696516
    },
    {
      "loss": 3.9567,
      "grad_norm": 2.643573760986328,
      "learning_rate": 1.8130458817025982e-05,
      "epoch": 27.197346600331674
    },
    {
      "loss": 3.9451,
      "grad_norm": 2.417168617248535,
      "learning_rate": 1.8185737976782754e-05,
      "epoch": 27.28026533996683
    },
    {
      "loss": 3.9509,
      "grad_norm": 2.493354558944702,
      "learning_rate": 1.8241017136539526e-05,
      "epoch": 27.36318407960199
    },
    {
      "loss": 3.9256,
      "grad_norm": 2.425187587738037,
      "learning_rate": 1.8296296296296298e-05,
      "epoch": 27.446102819237147
    },
    {
      "loss": 3.9313,
      "grad_norm": 2.413353443145752,
      "learning_rate": 1.835157545605307e-05,
      "epoch": 27.529021558872305
    },
    {
      "loss": 3.9203,
      "grad_norm": 2.3711278438568115,
      "learning_rate": 1.840685461580984e-05,
      "epoch": 27.611940298507463
    },
    {
      "loss": 3.8963,
      "grad_norm": 2.425295352935791,
      "learning_rate": 1.8462133775566613e-05,
      "epoch": 27.69485903814262
    },
    {
      "loss": 3.9136,
      "grad_norm": 2.3475637435913086,
      "learning_rate": 1.8517412935323385e-05,
      "epoch": 27.77777777777778
    },
    {
      "loss": 3.8807,
      "grad_norm": 2.340080499649048,
      "learning_rate": 1.8572692095080157e-05,
      "epoch": 27.860696517412936
    },
    {
      "loss": 3.9076,
      "grad_norm": 2.5394327640533447,
      "learning_rate": 1.862797125483693e-05,
      "epoch": 27.943615257048094
    },
    {
      "loss": 3.8835,
      "grad_norm": 2.401271104812622,
      "learning_rate": 1.8683250414593697e-05,
      "epoch": 28.026533996683252
    },
    {
      "loss": 3.8797,
      "grad_norm": 2.400451898574829,
      "learning_rate": 1.8738529574350473e-05,
      "epoch": 28.109452736318406
    },
    {
      "loss": 3.8614,
      "grad_norm": 2.5798561573028564,
      "learning_rate": 1.8793808734107245e-05,
      "epoch": 28.192371475953564
    },
    {
      "loss": 3.903,
      "grad_norm": 2.348581552505493,
      "learning_rate": 1.8849087893864016e-05,
      "epoch": 28.275290215588722
    },
    {
      "loss": 3.8487,
      "grad_norm": 2.3866543769836426,
      "learning_rate": 1.890436705362079e-05,
      "epoch": 28.35820895522388
    },
    {
      "loss": 3.8679,
      "grad_norm": 2.449525833129883,
      "learning_rate": 1.8959646213377557e-05,
      "epoch": 28.441127694859038
    },
    {
      "loss": 3.8592,
      "grad_norm": 2.2881598472595215,
      "learning_rate": 1.901492537313433e-05,
      "epoch": 28.524046434494196
    },
    {
      "loss": 3.8392,
      "grad_norm": 2.4416239261627197,
      "learning_rate": 1.90702045328911e-05,
      "epoch": 28.606965174129353
    },
    {
      "loss": 3.8417,
      "grad_norm": 2.5665783882141113,
      "learning_rate": 1.9125483692647872e-05,
      "epoch": 28.68988391376451
    },
    {
      "loss": 3.8357,
      "grad_norm": 2.523000955581665,
      "learning_rate": 1.9180762852404644e-05,
      "epoch": 28.77280265339967
    },
    {
      "loss": 3.8196,
      "grad_norm": 2.4705467224121094,
      "learning_rate": 1.9236042012161416e-05,
      "epoch": 28.855721393034827
    },
    {
      "loss": 3.825,
      "grad_norm": 2.459768056869507,
      "learning_rate": 1.9291321171918188e-05,
      "epoch": 28.938640132669985
    },
    {
      "loss": 3.8176,
      "grad_norm": 2.3627724647521973,
      "learning_rate": 1.934660033167496e-05,
      "epoch": 29.021558872305143
    },
    {
      "loss": 3.8239,
      "grad_norm": 2.442950487136841,
      "learning_rate": 1.9401879491431732e-05,
      "epoch": 29.104477611940297
    },
    {
      "loss": 3.7923,
      "grad_norm": 2.3955976963043213,
      "learning_rate": 1.9457158651188504e-05,
      "epoch": 29.187396351575455
    },
    {
      "loss": 3.8137,
      "grad_norm": 2.4912803173065186,
      "learning_rate": 1.9512437810945276e-05,
      "epoch": 29.270315091210612
    },
    {
      "loss": 3.8083,
      "grad_norm": 2.420145034790039,
      "learning_rate": 1.9567716970702047e-05,
      "epoch": 29.35323383084577
    },
    {
      "loss": 3.7905,
      "grad_norm": 2.361961841583252,
      "learning_rate": 1.962299613045882e-05,
      "epoch": 29.436152570480928
    },
    {
      "loss": 3.792,
      "grad_norm": 2.3383665084838867,
      "learning_rate": 1.9678275290215588e-05,
      "epoch": 29.519071310116086
    },
    {
      "loss": 3.7625,
      "grad_norm": 2.3615784645080566,
      "learning_rate": 1.9733554449972363e-05,
      "epoch": 29.601990049751244
    },
    {
      "loss": 3.7685,
      "grad_norm": 2.3778154850006104,
      "learning_rate": 1.9788833609729135e-05,
      "epoch": 29.6849087893864
    },
    {
      "loss": 3.7701,
      "grad_norm": 2.352156639099121,
      "learning_rate": 1.9844112769485907e-05,
      "epoch": 29.76782752902156
    },
    {
      "loss": 3.7475,
      "grad_norm": 2.3692665100097656,
      "learning_rate": 1.989939192924268e-05,
      "epoch": 29.850746268656717
    },
    {
      "loss": 3.7379,
      "grad_norm": 2.658752918243408,
      "learning_rate": 1.995467108899945e-05,
      "epoch": 29.933665008291875
    },
    {
      "loss": 3.7271,
      "grad_norm": 2.3654587268829346,
      "learning_rate": 1.9999997756510465e-05,
      "epoch": 30.016583747927033
    },
    {
      "loss": 3.7502,
      "grad_norm": 2.332385301589966,
      "learning_rate": 1.9999903585496125e-05,
      "epoch": 30.09950248756219
    },
    {
      "loss": 3.6948,
      "grad_norm": 2.304621696472168,
      "learning_rate": 1.999967092896331e-05,
      "epoch": 30.182421227197345
    },
    {
      "loss": 3.7248,
      "grad_norm": 2.3681912422180176,
      "learning_rate": 1.999929979013401e-05,
      "epoch": 30.265339966832503
    },
    {
      "loss": 3.7375,
      "grad_norm": 2.38773775100708,
      "learning_rate": 1.9998790174148007e-05,
      "epoch": 30.34825870646766
    },
    {
      "loss": 3.7327,
      "grad_norm": 2.455322265625,
      "learning_rate": 1.999814208806281e-05,
      "epoch": 30.43117744610282
    },
    {
      "loss": 3.701,
      "grad_norm": 2.331031560897827,
      "learning_rate": 1.9997355540853566e-05,
      "epoch": 30.514096185737976
    },
    {
      "loss": 3.71,
      "grad_norm": 2.385133743286133,
      "learning_rate": 1.999643054341292e-05,
      "epoch": 30.597014925373134
    },
    {
      "loss": 3.7269,
      "grad_norm": 2.3733341693878174,
      "learning_rate": 1.9995367108550865e-05,
      "epoch": 30.679933665008292
    },
    {
      "loss": 3.7414,
      "grad_norm": 2.3276448249816895,
      "learning_rate": 1.9994165250994575e-05,
      "epoch": 30.76285240464345
    },
    {
      "loss": 3.703,
      "grad_norm": 2.451648235321045,
      "learning_rate": 1.9992824987388202e-05,
      "epoch": 30.845771144278608
    },
    {
      "loss": 3.7067,
      "grad_norm": 2.293205499649048,
      "learning_rate": 1.9991346336292638e-05,
      "epoch": 30.928689883913766
    },
    {
      "loss": 3.7285,
      "grad_norm": 2.2971363067626953,
      "learning_rate": 1.9989729318185245e-05,
      "epoch": 31.011608623548923
    },
    {
      "loss": 3.621,
      "grad_norm": 2.3267343044281006,
      "learning_rate": 1.9987973955459607e-05,
      "epoch": 31.09452736318408
    },
    {
      "loss": 3.6575,
      "grad_norm": 2.490583896636963,
      "learning_rate": 1.998608027242519e-05,
      "epoch": 31.177446102819236
    },
    {
      "loss": 3.6531,
      "grad_norm": 2.360797882080078,
      "learning_rate": 1.998404829530701e-05,
      "epoch": 31.260364842454393
    },
    {
      "loss": 3.6752,
      "grad_norm": 2.4584665298461914,
      "learning_rate": 1.9981878052245283e-05,
      "epoch": 31.34328358208955
    },
    {
      "loss": 3.6596,
      "grad_norm": 2.5448648929595947,
      "learning_rate": 1.9979569573295022e-05,
      "epoch": 31.42620232172471
    },
    {
      "loss": 3.6508,
      "grad_norm": 2.366032361984253,
      "learning_rate": 1.997712289042562e-05,
      "epoch": 31.509121061359867
    },
    {
      "loss": 3.6315,
      "grad_norm": 2.3530337810516357,
      "learning_rate": 1.997453803752043e-05,
      "epoch": 31.592039800995025
    },
    {
      "loss": 3.6461,
      "grad_norm": 2.2648468017578125,
      "learning_rate": 1.9971815050376253e-05,
      "epoch": 31.674958540630183
    },
    {
      "loss": 3.6779,
      "grad_norm": 2.371666669845581,
      "learning_rate": 1.996895396670289e-05,
      "epoch": 31.75787728026534
    },
    {
      "loss": 3.6393,
      "grad_norm": 2.3013854026794434,
      "learning_rate": 1.9965954826122583e-05,
      "epoch": 31.8407960199005
    },
    {
      "loss": 3.6548,
      "grad_norm": 2.328594446182251,
      "learning_rate": 1.9962817670169494e-05,
      "epoch": 31.923714759535656
    },
    {
      "loss": 3.6234,
      "grad_norm": 2.43747878074646,
      "learning_rate": 1.9959542542289103e-05,
      "epoch": 32.00663349917081
    },
    {
      "loss": 3.5941,
      "grad_norm": 2.35422420501709,
      "learning_rate": 1.9956129487837624e-05,
      "epoch": 32.08955223880597
    },
    {
      "loss": 3.6423,
      "grad_norm": 2.404144048690796,
      "learning_rate": 1.995257855408138e-05,
      "epoch": 32.172470978441126
    },
    {
      "loss": 3.5772,
      "grad_norm": 2.3173201084136963,
      "learning_rate": 1.994888979019613e-05,
      "epoch": 32.25538971807629
    },
    {
      "loss": 3.6178,
      "grad_norm": 2.431769371032715,
      "learning_rate": 1.994506324726641e-05,
      "epoch": 32.33830845771144
    },
    {
      "loss": 3.59,
      "grad_norm": 2.3257641792297363,
      "learning_rate": 1.9941098978284805e-05,
      "epoch": 32.4212271973466
    },
    {
      "loss": 3.5963,
      "grad_norm": 2.3310163021087646,
      "learning_rate": 1.9936997038151225e-05,
      "epoch": 32.50414593698176
    },
    {
      "loss": 3.5872,
      "grad_norm": 2.3442628383636475,
      "learning_rate": 1.993275748367216e-05,
      "epoch": 32.58706467661692
    },
    {
      "loss": 3.5827,
      "grad_norm": 2.3202691078186035,
      "learning_rate": 1.9928380373559855e-05,
      "epoch": 32.66998341625207
    },
    {
      "loss": 3.5769,
      "grad_norm": 2.4601800441741943,
      "learning_rate": 1.9923865768431534e-05,
      "epoch": 32.75290215588723
    },
    {
      "loss": 3.5958,
      "grad_norm": 2.417060375213623,
      "learning_rate": 1.9919213730808544e-05,
      "epoch": 32.83582089552239
    },
    {
      "loss": 3.5752,
      "grad_norm": 2.387460231781006,
      "learning_rate": 1.9914424325115493e-05,
      "epoch": 32.91873963515754
    },
    {
      "loss": 3.6035,
      "grad_norm": 2.468843460083008,
      "learning_rate": 1.990949761767935e-05,
      "epoch": 33.001658374792704
    },
    {
      "loss": 3.5436,
      "grad_norm": 2.4428789615631104,
      "learning_rate": 1.9904433676728536e-05,
      "epoch": 33.08457711442786
    },
    {
      "loss": 3.5683,
      "grad_norm": 2.3211660385131836,
      "learning_rate": 1.989923257239198e-05,
      "epoch": 33.16749585406302
    },
    {
      "loss": 3.5442,
      "grad_norm": 2.325471878051758,
      "learning_rate": 1.989389437669813e-05,
      "epoch": 33.250414593698174
    },
    {
      "loss": 3.5863,
      "grad_norm": 2.433650493621826,
      "learning_rate": 1.9888419163573988e-05,
      "epoch": 33.333333333333336
    },
    {
      "loss": 3.5459,
      "grad_norm": 2.3963406085968018,
      "learning_rate": 1.9882807008844055e-05,
      "epoch": 33.41625207296849
    },
    {
      "loss": 3.519,
      "grad_norm": 2.372513771057129,
      "learning_rate": 1.9877057990229297e-05,
      "epoch": 33.49917081260365
    },
    {
      "loss": 3.5545,
      "grad_norm": 2.3225343227386475,
      "learning_rate": 1.9871172187346063e-05,
      "epoch": 33.582089552238806
    },
    {
      "loss": 3.5505,
      "grad_norm": 2.4132578372955322,
      "learning_rate": 1.9865149681704982e-05,
      "epoch": 33.66500829187396
    },
    {
      "loss": 3.5551,
      "grad_norm": 2.3435428142547607,
      "learning_rate": 1.9858990556709847e-05,
      "epoch": 33.74792703150912
    },
    {
      "loss": 3.5307,
      "grad_norm": 2.42558217048645,
      "learning_rate": 1.9852694897656435e-05,
      "epoch": 33.830845771144276
    },
    {
      "loss": 3.5328,
      "grad_norm": 2.401134967803955,
      "learning_rate": 1.9846262791731355e-05,
      "epoch": 33.91376451077944
    },
    {
      "loss": 3.4986,
      "grad_norm": 2.352684736251831,
      "learning_rate": 1.983969432801081e-05,
      "epoch": 33.99668325041459
    },
    {
      "loss": 3.4889,
      "grad_norm": 2.436408519744873,
      "learning_rate": 1.9832989597459393e-05,
      "epoch": 34.07960199004975
    },
    {
      "loss": 3.5346,
      "grad_norm": 2.3243398666381836,
      "learning_rate": 1.9826148692928805e-05,
      "epoch": 34.16252072968491
    },
    {
      "loss": 3.5707,
      "grad_norm": 2.4258933067321777,
      "learning_rate": 1.9819171709156582e-05,
      "epoch": 34.24543946932007
    },
    {
      "loss": 3.4831,
      "grad_norm": 2.581127405166626,
      "learning_rate": 1.981205874276478e-05,
      "epoch": 34.32835820895522
    },
    {
      "loss": 3.5075,
      "grad_norm": 2.373438835144043,
      "learning_rate": 1.9804809892258623e-05,
      "epoch": 34.411276948590384
    },
    {
      "loss": 3.4858,
      "grad_norm": 2.4306390285491943,
      "learning_rate": 1.979742525802517e-05,
      "epoch": 34.49419568822554
    },
    {
      "loss": 3.4922,
      "grad_norm": 2.3858866691589355,
      "learning_rate": 1.97899049423319e-05,
      "epoch": 34.5771144278607
    },
    {
      "loss": 3.4934,
      "grad_norm": 2.4272451400756836,
      "learning_rate": 1.9782249049325282e-05,
      "epoch": 34.660033167495854
    },
    {
      "loss": 3.5015,
      "grad_norm": 2.4914662837982178,
      "learning_rate": 1.9774457685029386e-05,
      "epoch": 34.74295190713101
    },
    {
      "loss": 3.4793,
      "grad_norm": 2.377329111099243,
      "learning_rate": 1.9766530957344362e-05,
      "epoch": 34.82587064676617
    },
    {
      "loss": 3.4815,
      "grad_norm": 2.392423152923584,
      "learning_rate": 1.9758468976044966e-05,
      "epoch": 34.908789386401324
    },
    {
      "loss": 3.4601,
      "grad_norm": 2.3998560905456543,
      "learning_rate": 1.9750271852779033e-05,
      "epoch": 34.991708126036485
    },
    {
      "loss": 3.4573,
      "grad_norm": 2.3143117427825928,
      "learning_rate": 1.9741939701065956e-05,
      "epoch": 35.07462686567164
    },
    {
      "loss": 3.4822,
      "grad_norm": 2.385258436203003,
      "learning_rate": 1.9733472636295077e-05,
      "epoch": 35.1575456053068
    },
    {
      "loss": 3.4779,
      "grad_norm": 2.535111665725708,
      "learning_rate": 1.972487077572411e-05,
      "epoch": 35.240464344941955
    },
    {
      "loss": 3.4461,
      "grad_norm": 2.2457199096679688,
      "learning_rate": 1.9716134238477517e-05,
      "epoch": 35.32338308457712
    },
    {
      "loss": 3.4464,
      "grad_norm": 2.4782230854034424,
      "learning_rate": 1.9707263145544857e-05,
      "epoch": 35.40630182421227
    },
    {
      "loss": 3.4346,
      "grad_norm": 2.301630735397339,
      "learning_rate": 1.96982576197791e-05,
      "epoch": 35.48922056384743
    },
    {
      "loss": 3.4337,
      "grad_norm": 2.411799669265747,
      "learning_rate": 1.9689117785894943e-05,
      "epoch": 35.57213930348259
    },
    {
      "loss": 3.4462,
      "grad_norm": 2.550781488418579,
      "learning_rate": 1.967984377046707e-05,
      "epoch": 35.65505804311775
    },
    {
      "loss": 3.4644,
      "grad_norm": 2.443511486053467,
      "learning_rate": 1.9670435701928397e-05,
      "epoch": 35.7379767827529
    },
    {
      "loss": 3.4088,
      "grad_norm": 2.4824233055114746,
      "learning_rate": 1.966089371056831e-05,
      "epoch": 35.82089552238806
    },
    {
      "loss": 3.4368,
      "grad_norm": 2.4285504817962646,
      "learning_rate": 1.965121792853084e-05,
      "epoch": 35.90381426202322
    },
    {
      "loss": 3.4141,
      "grad_norm": 2.3574371337890625,
      "learning_rate": 1.9641408489812858e-05,
      "epoch": 35.98673300165837
    },
    {
      "loss": 3.4231,
      "grad_norm": 2.4479215145111084,
      "learning_rate": 1.9631465530262186e-05,
      "epoch": 36.069651741293534
    },
    {
      "loss": 3.4073,
      "grad_norm": 2.4113893508911133,
      "learning_rate": 1.9621389187575746e-05,
      "epoch": 36.15257048092869
    },
    {
      "loss": 3.3959,
      "grad_norm": 2.4827635288238525,
      "learning_rate": 1.9611179601297636e-05,
      "epoch": 36.23548922056385
    },
    {
      "loss": 3.4117,
      "grad_norm": 2.280956983566284,
      "learning_rate": 1.9600836912817204e-05,
      "epoch": 36.318407960199
    },
    {
      "loss": 3.3969,
      "grad_norm": 2.4196996688842773,
      "learning_rate": 1.959036126536709e-05,
      "epoch": 36.401326699834165
    },
    {
      "loss": 3.4325,
      "grad_norm": 2.3852553367614746,
      "learning_rate": 1.9579752804021238e-05,
      "epoch": 36.48424543946932
    },
    {
      "loss": 3.4361,
      "grad_norm": 2.3675568103790283,
      "learning_rate": 1.9569011675692895e-05,
      "epoch": 36.56716417910448
    },
    {
      "loss": 3.3894,
      "grad_norm": 2.436154365539551,
      "learning_rate": 1.9558138029132562e-05,
      "epoch": 36.650082918739635
    },
    {
      "loss": 3.3752,
      "grad_norm": 2.4855878353118896,
      "learning_rate": 1.9547132014925953e-05,
      "epoch": 36.73300165837479
    },
    {
      "loss": 3.3783,
      "grad_norm": 2.3800125122070312,
      "learning_rate": 1.9535993785491895e-05,
      "epoch": 36.81592039800995
    },
    {
      "loss": 3.3644,
      "grad_norm": 2.506363868713379,
      "learning_rate": 1.9524723495080223e-05,
      "epoch": 36.898839137645105
    },
    {
      "loss": 3.3712,
      "grad_norm": 2.460200548171997,
      "learning_rate": 1.951332129976964e-05,
      "epoch": 36.981757877280266
    },
    {
      "loss": 3.3592,
      "grad_norm": 2.4118852615356445,
      "learning_rate": 1.950178735746557e-05,
      "epoch": 37.06467661691542
    },
    {
      "loss": 3.3833,
      "grad_norm": 2.529684066772461,
      "learning_rate": 1.9490121827897943e-05,
      "epoch": 37.14759535655058
    },
    {
      "loss": 3.3384,
      "grad_norm": 2.545466423034668,
      "learning_rate": 1.947832487261901e-05,
      "epoch": 37.230514096185736
    },
    {
      "loss": 3.3431,
      "grad_norm": 2.3754167556762695,
      "learning_rate": 1.946639665500109e-05,
      "epoch": 37.3134328358209
    },
    {
      "loss": 3.3364,
      "grad_norm": 2.3933229446411133,
      "learning_rate": 1.945433734023432e-05,
      "epoch": 37.39635157545605
    },
    {
      "loss": 3.3657,
      "grad_norm": 2.3742759227752686,
      "learning_rate": 1.9442147095324356e-05,
      "epoch": 37.47927031509121
    },
    {
      "loss": 3.3439,
      "grad_norm": 2.37912917137146,
      "learning_rate": 1.9429826089090064e-05,
      "epoch": 37.56218905472637
    },
    {
      "loss": 3.3777,
      "grad_norm": 2.335615873336792,
      "learning_rate": 1.941737449216118e-05,
      "epoch": 37.64510779436153
    },
    {
      "loss": 3.3585,
      "grad_norm": 2.5720608234405518,
      "learning_rate": 1.9404792476975954e-05,
      "epoch": 37.72802653399668
    },
    {
      "loss": 3.3502,
      "grad_norm": 2.435619831085205,
      "learning_rate": 1.9392080217778756e-05,
      "epoch": 37.81094527363184
    },
    {
      "loss": 3.3232,
      "grad_norm": 2.4470396041870117,
      "learning_rate": 1.9379237890617658e-05,
      "epoch": 37.893864013267
    },
    {
      "loss": 3.3352,
      "grad_norm": 2.4501030445098877,
      "learning_rate": 1.936626567334202e-05,
      "epoch": 37.97678275290215
    },
    {
      "loss": 3.3275,
      "grad_norm": 2.325359344482422,
      "learning_rate": 1.935316374559999e-05,
      "epoch": 38.059701492537314
    },
    {
      "loss": 3.3407,
      "grad_norm": 2.507514476776123,
      "learning_rate": 1.9339932288836045e-05,
      "epoch": 38.14262023217247
    },
    {
      "loss": 3.3054,
      "grad_norm": 2.5598387718200684,
      "learning_rate": 1.9326571486288465e-05,
      "epoch": 38.22553897180763
    },
    {
      "loss": 3.323,
      "grad_norm": 2.4058144092559814,
      "learning_rate": 1.9313081522986806e-05,
      "epoch": 38.308457711442784
    },
    {
      "loss": 3.3067,
      "grad_norm": 2.4213852882385254,
      "learning_rate": 1.9299462585749317e-05,
      "epoch": 38.391376451077946
    },
    {
      "loss": 3.302,
      "grad_norm": 2.3657138347625732,
      "learning_rate": 1.928571486318038e-05,
      "epoch": 38.4742951907131
    },
    {
      "loss": 3.33,
      "grad_norm": 2.38112735748291,
      "learning_rate": 1.9271838545667876e-05,
      "epoch": 38.55721393034826
    },
    {
      "loss": 3.3251,
      "grad_norm": 2.495853900909424,
      "learning_rate": 1.9257833825380567e-05,
      "epoch": 38.640132669983416
    },
    {
      "loss": 3.3062,
      "grad_norm": 2.468045711517334,
      "learning_rate": 1.924370089626541e-05,
      "epoch": 38.72305140961858
    },
    {
      "loss": 3.3245,
      "grad_norm": 2.402966022491455,
      "learning_rate": 1.9229439954044897e-05,
      "epoch": 38.80597014925373
    },
    {
      "loss": 3.3125,
      "grad_norm": 2.39715576171875,
      "learning_rate": 1.9215051196214333e-05,
      "epoch": 38.888888888888886
    },
    {
      "loss": 3.2885,
      "grad_norm": 2.3446426391601562,
      "learning_rate": 1.9200534822039095e-05,
      "epoch": 38.97180762852405
    },
    {
      "loss": 3.2823,
      "grad_norm": 2.66845703125,
      "learning_rate": 1.918589103255188e-05,
      "epoch": 39.0547263681592
    },
    {
      "loss": 3.279,
      "grad_norm": 2.5368242263793945,
      "learning_rate": 1.9171120030549925e-05,
      "epoch": 39.13764510779436
    },
    {
      "loss": 3.2571,
      "grad_norm": 2.4397189617156982,
      "learning_rate": 1.915622202059219e-05,
      "epoch": 39.22056384742952
    },
    {
      "loss": 3.2656,
      "grad_norm": 2.4082958698272705,
      "learning_rate": 1.914119720899652e-05,
      "epoch": 39.30348258706468
    },
    {
      "loss": 3.3084,
      "grad_norm": 2.4544620513916016,
      "learning_rate": 1.912604580383681e-05,
      "epoch": 39.38640132669983
    },
    {
      "loss": 3.297,
      "grad_norm": 2.3524560928344727,
      "learning_rate": 1.91107680149401e-05,
      "epoch": 39.469320066334994
    },
    {
      "loss": 3.2803,
      "grad_norm": 2.423609495162964,
      "learning_rate": 1.9095364053883688e-05,
      "epoch": 39.55223880597015
    },
    {
      "loss": 3.2694,
      "grad_norm": 2.4105472564697266,
      "learning_rate": 1.9079834133992175e-05,
      "epoch": 39.63515754560531
    },
    {
      "loss": 3.2572,
      "grad_norm": 2.511579990386963,
      "learning_rate": 1.906417847033454e-05,
      "epoch": 39.718076285240464
    },
    {
      "loss": 3.2308,
      "grad_norm": 2.386014699935913,
      "learning_rate": 1.9048397279721142e-05,
      "epoch": 39.80099502487562
    },
    {
      "loss": 3.2967,
      "grad_norm": 2.380253553390503,
      "learning_rate": 1.9032490780700727e-05,
      "epoch": 39.88391376451078
    },
    {
      "loss": 3.2299,
      "grad_norm": 2.421903133392334,
      "learning_rate": 1.9016459193557394e-05,
      "epoch": 39.966832504145934
    },
    {
      "loss": 3.2809,
      "grad_norm": 2.413421869277954,
      "learning_rate": 1.900030274030755e-05,
      "epoch": 40.049751243781095
    },
    {
      "loss": 3.2196,
      "grad_norm": 2.391941785812378,
      "learning_rate": 1.8984021644696828e-05,
      "epoch": 40.13266998341625
    },
    {
      "loss": 3.2321,
      "grad_norm": 2.472442865371704,
      "learning_rate": 1.8967616132197003e-05,
      "epoch": 40.21558872305141
    },
    {
      "loss": 3.2553,
      "grad_norm": 2.467775821685791,
      "learning_rate": 1.8951086430002856e-05,
      "epoch": 40.298507462686565
    },
    {
      "loss": 3.2868,
      "grad_norm": 2.426863431930542,
      "learning_rate": 1.893443276702903e-05,
      "epoch": 40.38142620232173
    },
    {
      "loss": 3.2417,
      "grad_norm": 2.4445955753326416,
      "learning_rate": 1.8917655373906866e-05,
      "epoch": 40.46434494195688
    },
    {
      "loss": 3.2264,
      "grad_norm": 2.446000337600708,
      "learning_rate": 1.89007544829812e-05,
      "epoch": 40.54726368159204
    },
    {
      "loss": 3.2344,
      "grad_norm": 2.4171671867370605,
      "learning_rate": 1.8883730328307154e-05,
      "epoch": 40.6301824212272
    },
    {
      "loss": 3.208,
      "grad_norm": 2.4660379886627197,
      "learning_rate": 1.8866583145646892e-05,
      "epoch": 40.71310116086236
    },
    {
      "loss": 3.2393,
      "grad_norm": 2.3445682525634766,
      "learning_rate": 1.884931317246635e-05,
      "epoch": 40.79601990049751
    },
    {
      "loss": 3.2341,
      "grad_norm": 2.4120490550994873,
      "learning_rate": 1.883192064793195e-05,
      "epoch": 40.87893864013267
    },
    {
      "loss": 3.2153,
      "grad_norm": 2.377427339553833,
      "learning_rate": 1.8814405812907295e-05,
      "epoch": 40.96185737976783
    },
    {
      "loss": 3.2445,
      "grad_norm": 2.40509033203125,
      "learning_rate": 1.8796768909949828e-05,
      "epoch": 41.04477611940298
    },
    {
      "loss": 3.1928,
      "grad_norm": 2.4241034984588623,
      "learning_rate": 1.877901018330746e-05,
      "epoch": 41.127694859038144
    },
    {
      "loss": 3.2157,
      "grad_norm": 2.524165391921997,
      "learning_rate": 1.8761129878915215e-05,
      "epoch": 41.2106135986733
    },
    {
      "loss": 3.1989,
      "grad_norm": 2.4564461708068848,
      "learning_rate": 1.8743128244391804e-05,
      "epoch": 41.29353233830846
    },
    {
      "loss": 3.2282,
      "grad_norm": 2.315159797668457,
      "learning_rate": 1.8725005529036197e-05,
      "epoch": 41.376451077943614
    },
    {
      "loss": 3.1967,
      "grad_norm": 2.3985023498535156,
      "learning_rate": 1.870676198382418e-05,
      "epoch": 41.459369817578775
    },
    {
      "loss": 3.1825,
      "grad_norm": 2.297605276107788,
      "learning_rate": 1.868839786140486e-05,
      "epoch": 41.54228855721393
    },
    {
      "loss": 3.1692,
      "grad_norm": 2.4106104373931885,
      "learning_rate": 1.8669913416097205e-05,
      "epoch": 41.62520729684909
    },
    {
      "loss": 3.1827,
      "grad_norm": 2.4527127742767334,
      "learning_rate": 1.8651308903886474e-05,
      "epoch": 41.708126036484245
    },
    {
      "loss": 3.1824,
      "grad_norm": 2.435291051864624,
      "learning_rate": 1.8632584582420706e-05,
      "epoch": 41.791044776119406
    },
    {
      "loss": 3.1964,
      "grad_norm": 2.455458164215088,
      "learning_rate": 1.8613740711007134e-05,
      "epoch": 41.87396351575456
    },
    {
      "loss": 3.2003,
      "grad_norm": 2.3663389682769775,
      "learning_rate": 1.8594777550608608e-05,
      "epoch": 41.956882255389715
    },
    {
      "loss": 3.2054,
      "grad_norm": 2.3903872966766357,
      "learning_rate": 1.857569536383997e-05,
      "epoch": 42.039800995024876
    },
    {
      "loss": 3.1776,
      "grad_norm": 2.4678943157196045,
      "learning_rate": 1.855649441496442e-05,
      "epoch": 42.12271973466003
    },
    {
      "loss": 3.1892,
      "grad_norm": 2.496342182159424,
      "learning_rate": 1.8537174969889853e-05,
      "epoch": 42.20563847429519
    },
    {
      "loss": 3.1835,
      "grad_norm": 2.3626387119293213,
      "learning_rate": 1.8517737296165194e-05,
      "epoch": 42.288557213930346
    },
    {
      "loss": 3.1524,
      "grad_norm": 2.4664013385772705,
      "learning_rate": 1.8498181662976665e-05,
      "epoch": 42.37147595356551
    },
    {
      "loss": 3.2106,
      "grad_norm": 2.392324686050415,
      "learning_rate": 1.8478508341144076e-05,
      "epoch": 42.45439469320066
    },
    {
      "loss": 3.1843,
      "grad_norm": 2.406912088394165,
      "learning_rate": 1.8458717603117077e-05,
      "epoch": 42.53731343283582
    },
    {
      "loss": 3.1477,
      "grad_norm": 2.4091169834136963,
      "learning_rate": 1.843880972297137e-05,
      "epoch": 42.62023217247098
    },
    {
      "loss": 3.1471,
      "grad_norm": 2.435919761657715,
      "learning_rate": 1.841878497640492e-05,
      "epoch": 42.70315091210614
    },
    {
      "loss": 3.1571,
      "grad_norm": 2.510671615600586,
      "learning_rate": 1.8398643640734147e-05,
      "epoch": 42.78606965174129
    },
    {
      "loss": 3.1543,
      "grad_norm": 2.4633376598358154,
      "learning_rate": 1.8378385994890065e-05,
      "epoch": 42.86898839137645
    },
    {
      "loss": 3.137,
      "grad_norm": 2.473207712173462,
      "learning_rate": 1.8358012319414443e-05,
      "epoch": 42.95190713101161
    },
    {
      "loss": 3.1514,
      "grad_norm": 2.3790817260742188,
      "learning_rate": 1.83375228964559e-05,
      "epoch": 43.03482587064676
    },
    {
      "loss": 3.1311,
      "grad_norm": 2.397679328918457,
      "learning_rate": 1.8316918009766012e-05,
      "epoch": 43.117744610281925
    },
    {
      "loss": 3.149,
      "grad_norm": 2.4453516006469727,
      "learning_rate": 1.8296197944695366e-05,
      "epoch": 43.20066334991708
    },
    {
      "loss": 3.137,
      "grad_norm": 2.4021637439727783,
      "learning_rate": 1.8275362988189627e-05,
      "epoch": 43.28358208955224
    },
    {
      "loss": 3.1488,
      "grad_norm": 2.437716245651245,
      "learning_rate": 1.8254413428785552e-05,
      "epoch": 43.366500829187395
    },
    {
      "loss": 3.1164,
      "grad_norm": 2.4281985759735107,
      "learning_rate": 1.8233349556606998e-05,
      "epoch": 43.449419568822556
    },
    {
      "loss": 3.1264,
      "grad_norm": 2.3554961681365967,
      "learning_rate": 1.8212171663360902e-05,
      "epoch": 43.53233830845771
    },
    {
      "loss": 3.1427,
      "grad_norm": 2.422006368637085,
      "learning_rate": 1.8190880042333247e-05,
      "epoch": 43.61525704809287
    },
    {
      "loss": 3.1234,
      "grad_norm": 2.4000394344329834,
      "learning_rate": 1.8169474988384994e-05,
      "epoch": 43.698175787728026
    },
    {
      "loss": 3.1065,
      "grad_norm": 2.4145500659942627,
      "learning_rate": 1.8147956797947994e-05,
      "epoch": 43.78109452736319
    },
    {
      "loss": 3.1319,
      "grad_norm": 2.395054340362549,
      "learning_rate": 1.8126325769020906e-05,
      "epoch": 43.86401326699834
    },
    {
      "loss": 3.1167,
      "grad_norm": 2.3701558113098145,
      "learning_rate": 1.8104582201165036e-05,
      "epoch": 43.946932006633496
    },
    {
      "loss": 3.125,
      "grad_norm": 2.472158193588257,
      "learning_rate": 1.8082726395500215e-05,
      "epoch": 44.02985074626866
    },
    {
      "loss": 3.072,
      "grad_norm": 2.4652793407440186,
      "learning_rate": 1.8060758654700622e-05,
      "epoch": 44.11276948590381
    },
    {
      "loss": 3.0959,
      "grad_norm": 2.4526524543762207,
      "learning_rate": 1.8038679282990587e-05,
      "epoch": 44.19568822553897
    },
    {
      "loss": 3.1229,
      "grad_norm": 2.432250499725342,
      "learning_rate": 1.801648858614038e-05,
      "epoch": 44.27860696517413
    },
    {
      "loss": 3.0843,
      "grad_norm": 2.4447271823883057,
      "learning_rate": 1.799418687146198e-05,
      "epoch": 44.36152570480929
    },
    {
      "loss": 3.0978,
      "grad_norm": 2.445629358291626,
      "learning_rate": 1.797177444780482e-05,
      "epoch": 44.44444444444444
    },
    {
      "loss": 3.1013,
      "grad_norm": 2.4817051887512207,
      "learning_rate": 1.79492516255515e-05,
      "epoch": 44.527363184079604
    },
    {
      "loss": 3.1131,
      "grad_norm": 2.4662842750549316,
      "learning_rate": 1.7926618716613504e-05,
      "epoch": 44.61028192371476
    },
    {
      "loss": 3.1144,
      "grad_norm": 2.5037825107574463,
      "learning_rate": 1.790387603442686e-05,
      "epoch": 44.69320066334992
    },
    {
      "loss": 3.0812,
      "grad_norm": 2.528698444366455,
      "learning_rate": 1.788102389394782e-05,
      "epoch": 44.776119402985074
    },
    {
      "loss": 3.1019,
      "grad_norm": 2.386319637298584,
      "learning_rate": 1.785806261164849e-05,
      "epoch": 44.859038142620236
    },
    {
      "loss": 3.1112,
      "grad_norm": 2.5193352699279785,
      "learning_rate": 1.7834992505512444e-05,
      "epoch": 44.94195688225539
    },
    {
      "loss": 3.0666,
      "grad_norm": 2.482898235321045,
      "learning_rate": 1.781181389503032e-05,
      "epoch": 45.024875621890544
    },
    {
      "loss": 3.0545,
      "grad_norm": 2.5540575981140137,
      "learning_rate": 1.7788527101195407e-05,
      "epoch": 45.107794361525706
    },
    {
      "loss": 3.101,
      "grad_norm": 2.4201231002807617,
      "learning_rate": 1.776513244649918e-05,
      "epoch": 45.19071310116086
    },
    {
      "loss": 3.0856,
      "grad_norm": 2.520721673965454,
      "learning_rate": 1.7741630254926856e-05,
      "epoch": 45.27363184079602
    },
    {
      "loss": 3.0661,
      "grad_norm": 2.604250907897949,
      "learning_rate": 1.7718020851952894e-05,
      "epoch": 45.356550580431175
    },
    {
      "loss": 3.0771,
      "grad_norm": 2.413618564605713,
      "learning_rate": 1.7694304564536477e-05,
      "epoch": 45.43946932006634
    },
    {
      "loss": 3.0534,
      "grad_norm": 2.516815662384033,
      "learning_rate": 1.7670481721117018e-05,
      "epoch": 45.52238805970149
    },
    {
      "loss": 3.0449,
      "grad_norm": 2.4418821334838867,
      "learning_rate": 1.7646552651609575e-05,
      "epoch": 45.60530679933665
    },
    {
      "loss": 3.098,
      "grad_norm": 2.5883944034576416,
      "learning_rate": 1.76225176874003e-05,
      "epoch": 45.68822553897181
    },
    {
      "loss": 3.0636,
      "grad_norm": 2.433311939239502,
      "learning_rate": 1.759837716134186e-05,
      "epoch": 45.77114427860697
    },
    {
      "loss": 3.0824,
      "grad_norm": 2.3760573863983154,
      "learning_rate": 1.7574131407748796e-05,
      "epoch": 45.85406301824212
    },
    {
      "loss": 3.0852,
      "grad_norm": 2.440382242202759,
      "learning_rate": 1.7549780762392927e-05,
      "epoch": 45.93698175787728
    },
    {
      "loss": 3.0497,
      "grad_norm": 2.4376394748687744,
      "learning_rate": 1.752532556249867e-05,
      "epoch": 46.01990049751244
    },
    {
      "loss": 3.018,
      "grad_norm": 2.3384571075439453,
      "learning_rate": 1.750076614673841e-05,
      "epoch": 46.10281923714759
    },
    {
      "loss": 3.0578,
      "grad_norm": 2.5811679363250732,
      "learning_rate": 1.7476102855227753e-05,
      "epoch": 46.185737976782754
    },
    {
      "loss": 3.028,
      "grad_norm": 2.4876203536987305,
      "learning_rate": 1.745133602952087e-05,
      "epoch": 46.26865671641791
    },
    {
      "loss": 3.0403,
      "grad_norm": 2.4626145362854004,
      "learning_rate": 1.7426466012605738e-05,
      "epoch": 46.35157545605307
    },
    {
      "loss": 3.04,
      "grad_norm": 2.494115114212036,
      "learning_rate": 1.740149314889939e-05,
      "epoch": 46.434494195688224
    },
    {
      "loss": 3.0379,
      "grad_norm": 2.451528787612915,
      "learning_rate": 1.737641778424317e-05,
      "epoch": 46.517412935323385
    },
    {
      "loss": 3.0639,
      "grad_norm": 2.432790517807007,
      "learning_rate": 1.7351240265897903e-05,
      "epoch": 46.60033167495854
    },
    {
      "loss": 3.0707,
      "grad_norm": 2.467133045196533,
      "learning_rate": 1.7325960942539124e-05,
      "epoch": 46.6832504145937
    },
    {
      "loss": 3.0475,
      "grad_norm": 2.4451563358306885,
      "learning_rate": 1.730058016425223e-05,
      "epoch": 46.766169154228855
    },
    {
      "loss": 3.025,
      "grad_norm": 2.450732946395874,
      "learning_rate": 1.727509828252763e-05,
      "epoch": 46.84908789386402
    },
    {
      "loss": 3.0305,
      "grad_norm": 2.548595905303955,
      "learning_rate": 1.724951565025589e-05,
      "epoch": 46.93200663349917
    },
    {
      "loss": 3.0352,
      "grad_norm": 2.465212821960449,
      "learning_rate": 1.722383262172283e-05,
      "epoch": 47.014925373134325
    },
    {
      "loss": 2.9999,
      "grad_norm": 2.44566011428833,
      "learning_rate": 1.7198049552604638e-05,
      "epoch": 47.09784411276949
    },
    {
      "loss": 3.0492,
      "grad_norm": 2.4835731983184814,
      "learning_rate": 1.717216679996292e-05,
      "epoch": 47.18076285240464
    },
    {
      "loss": 2.9868,
      "grad_norm": 2.48227596282959,
      "learning_rate": 1.714618472223978e-05,
      "epoch": 47.2636815920398
    },
    {
      "loss": 3.0217,
      "grad_norm": 2.3772072792053223,
      "learning_rate": 1.7120103679252834e-05,
      "epoch": 47.346600331674956
    },
    {
      "loss": 3.0159,
      "grad_norm": 2.4975876808166504,
      "learning_rate": 1.709392403219024e-05,
      "epoch": 47.42951907131012
    },
    {
      "loss": 3.0332,
      "grad_norm": 2.465428113937378,
      "learning_rate": 1.7067646143605693e-05,
      "epoch": 47.51243781094527
    },
    {
      "loss": 3.0296,
      "grad_norm": 2.5278615951538086,
      "learning_rate": 1.704127037741341e-05,
      "epoch": 47.59535655058043
    },
    {
      "loss": 2.9877,
      "grad_norm": 2.5118701457977295,
      "learning_rate": 1.701479709888307e-05,
      "epoch": 47.67827529021559
    },
    {
      "loss": 3.015,
      "grad_norm": 2.4546666145324707,
      "learning_rate": 1.698822667463478e-05,
      "epoch": 47.76119402985075
    },
    {
      "loss": 3.0351,
      "grad_norm": 2.539360284805298,
      "learning_rate": 1.6961559472633985e-05,
      "epoch": 47.8441127694859
    },
    {
      "loss": 3.0419,
      "grad_norm": 2.5462965965270996,
      "learning_rate": 1.6934795862186382e-05,
      "epoch": 47.927031509121065
    },
    {
      "loss": 2.9759,
      "grad_norm": 2.556544303894043,
      "learning_rate": 1.6907936213932787e-05,
      "epoch": 48.00995024875622
    },
    {
      "loss": 2.9831,
      "grad_norm": 2.528665065765381,
      "learning_rate": 1.6880980899844013e-05,
      "epoch": 48.09286898839137
    },
    {
      "loss": 3.0037,
      "grad_norm": 2.507071018218994,
      "learning_rate": 1.6853930293215735e-05,
      "epoch": 48.175787728026535
    },
    {
      "loss": 2.9989,
      "grad_norm": 2.356647253036499,
      "learning_rate": 1.6826784768663283e-05,
      "epoch": 48.25870646766169
    },
    {
      "loss": 2.9946,
      "grad_norm": 2.5230414867401123,
      "learning_rate": 1.6799544702116487e-05,
      "epoch": 48.34162520729685
    },
    {
      "loss": 3.0072,
      "grad_norm": 2.5416152477264404,
      "learning_rate": 1.6772210470814464e-05,
      "epoch": 48.424543946932005
    },
    {
      "loss": 3.0112,
      "grad_norm": 2.5149576663970947,
      "learning_rate": 1.6744782453300378e-05,
      "epoch": 48.507462686567166
    },
    {
      "loss": 3.0084,
      "grad_norm": 2.525012254714966,
      "learning_rate": 1.6717261029416218e-05,
      "epoch": 48.59038142620232
    },
    {
      "loss": 3.0229,
      "grad_norm": 2.4823036193847656,
      "learning_rate": 1.6689646580297518e-05,
      "epoch": 48.67330016583748
    },
    {
      "loss": 2.9931,
      "grad_norm": 2.6646981239318848,
      "learning_rate": 1.6661939488368098e-05,
      "epoch": 48.756218905472636
    },
    {
      "loss": 2.9668,
      "grad_norm": 2.534306049346924,
      "learning_rate": 1.663414013733476e-05,
      "epoch": 48.8391376451078
    },
    {
      "loss": 2.9706,
      "grad_norm": 2.572962522506714,
      "learning_rate": 1.660624891218197e-05,
      "epoch": 48.92205638474295
    },
    {
      "loss": 2.9612,
      "grad_norm": 2.543949604034424,
      "learning_rate": 1.6578266199166523e-05,
      "epoch": 49.004975124378106
    },
    {
      "loss": 2.9885,
      "grad_norm": 2.394331932067871,
      "learning_rate": 1.6550192385812212e-05,
      "epoch": 49.08789386401327
    },
    {
      "loss": 2.9756,
      "grad_norm": 2.518510580062866,
      "learning_rate": 1.652202786090445e-05,
      "epoch": 49.17081260364842
    },
    {
      "loss": 2.9577,
      "grad_norm": 2.5125629901885986,
      "learning_rate": 1.6493773014484867e-05,
      "epoch": 49.25373134328358
    },
    {
      "loss": 2.9612,
      "grad_norm": 2.613164186477661,
      "learning_rate": 1.6465428237845954e-05,
      "epoch": 49.33665008291874
    },
    {
      "loss": 2.9911,
      "grad_norm": 2.4945499897003174,
      "learning_rate": 1.6436993923525606e-05,
      "epoch": 49.4195688225539
    },
    {
      "loss": 2.983,
      "grad_norm": 2.4711897373199463,
      "learning_rate": 1.640847046530168e-05,
      "epoch": 49.50248756218905
    },
    {
      "loss": 2.9607,
      "grad_norm": 2.4786531925201416,
      "learning_rate": 1.6379858258186593e-05,
      "epoch": 49.585406301824214
    },
    {
      "loss": 2.9677,
      "grad_norm": 2.5878283977508545,
      "learning_rate": 1.635115769842179e-05,
      "epoch": 49.66832504145937
    },
    {
      "loss": 2.9452,
      "grad_norm": 2.543532371520996,
      "learning_rate": 1.6322369183472293e-05,
      "epoch": 49.75124378109453
    },
    {
      "loss": 2.9639,
      "grad_norm": 2.5302019119262695,
      "learning_rate": 1.6293493112021188e-05,
      "epoch": 49.834162520729684
    },
    {
      "loss": 2.9311,
      "grad_norm": 2.383892059326172,
      "learning_rate": 1.62645298839641e-05,
      "epoch": 49.917081260364846
    },
    {
      "loss": 2.9562,
      "grad_norm": 2.6648802757263184,
      "learning_rate": 1.623547990040367e-05,
      "epoch": 50.0
    },
    {
      "loss": 2.9429,
      "grad_norm": 2.5349347591400146,
      "learning_rate": 1.6206343563643975e-05,
      "epoch": 50.082918739635154
    },
    {
      "loss": 2.9494,
      "grad_norm": 2.568498373031616,
      "learning_rate": 1.6177121277184978e-05,
      "epoch": 50.165837479270316
    },
    {
      "loss": 2.9022,
      "grad_norm": 2.634613275527954,
      "learning_rate": 1.6147813445716926e-05,
      "epoch": 50.24875621890547
    },
    {
      "loss": 2.9474,
      "grad_norm": 2.3747339248657227,
      "learning_rate": 1.611842047511476e-05,
      "epoch": 50.33167495854063
    },
    {
      "loss": 2.9657,
      "grad_norm": 2.555813789367676,
      "learning_rate": 1.6088942772432478e-05,
      "epoch": 50.414593698175786
    },
    {
      "loss": 2.9246,
      "grad_norm": 2.5512185096740723,
      "learning_rate": 1.6059380745897514e-05,
      "epoch": 50.49751243781095
    },
    {
      "loss": 2.9424,
      "grad_norm": 2.5129876136779785,
      "learning_rate": 1.6029734804905072e-05,
      "epoch": 50.5804311774461
    },
    {
      "loss": 2.9518,
      "grad_norm": 2.7016046047210693,
      "learning_rate": 1.6000005360012455e-05,
      "epoch": 50.66334991708126
    },
    {
      "loss": 2.9389,
      "grad_norm": 2.5099101066589355,
      "learning_rate": 1.59701928229334e-05,
      "epoch": 50.74626865671642
    },
    {
      "loss": 2.927,
      "grad_norm": 2.4500513076782227,
      "learning_rate": 1.5940297606532347e-05,
      "epoch": 50.82918739635158
    },
    {
      "loss": 2.9042,
      "grad_norm": 2.5788252353668213,
      "learning_rate": 1.5910320124818745e-05,
      "epoch": 50.91210613598673
    },
    {
      "loss": 2.9466,
      "grad_norm": 2.5610272884368896,
      "learning_rate": 1.5880260792941303e-05,
      "epoch": 50.995024875621894
    },
    {
      "loss": 2.9128,
      "grad_norm": 2.6072025299072266,
      "learning_rate": 1.5850120027182257e-05,
      "epoch": 51.07794361525705
    },
    {
      "loss": 2.9199,
      "grad_norm": 2.343956470489502,
      "learning_rate": 1.5819898244951583e-05,
      "epoch": 51.1608623548922
    },
    {
      "loss": 2.9169,
      "grad_norm": 2.5094082355499268,
      "learning_rate": 1.5789595864781237e-05,
      "epoch": 51.243781094527364
    },
    {
      "loss": 2.9109,
      "grad_norm": 2.539949893951416,
      "learning_rate": 1.5759213306319344e-05,
      "epoch": 51.32669983416252
    },
    {
      "loss": 2.9377,
      "grad_norm": 2.563650131225586,
      "learning_rate": 1.5728750990324404e-05,
      "epoch": 51.40961857379768
    },
    {
      "loss": 2.9109,
      "grad_norm": 2.5788350105285645,
      "learning_rate": 1.5698209338659442e-05,
      "epoch": 51.492537313432834
    },
    {
      "loss": 2.9463,
      "grad_norm": 2.521111011505127,
      "learning_rate": 1.5667588774286183e-05,
      "epoch": 51.575456053067995
    },
    {
      "loss": 2.8894,
      "grad_norm": 2.4885871410369873,
      "learning_rate": 1.563688972125919e-05,
      "epoch": 51.65837479270315
    },
    {
      "loss": 2.9117,
      "grad_norm": 2.5623326301574707,
      "learning_rate": 1.5606112604719985e-05,
      "epoch": 51.74129353233831
    },
    {
      "loss": 2.9209,
      "grad_norm": 2.5480642318725586,
      "learning_rate": 1.5575257850891178e-05,
      "epoch": 51.824212271973465
    },
    {
      "loss": 2.9052,
      "grad_norm": 2.4937663078308105,
      "learning_rate": 1.5544325887070546e-05,
      "epoch": 51.90713101160863
    },
    {
      "loss": 2.913,
      "grad_norm": 2.5054116249084473,
      "learning_rate": 1.551331714162512e-05,
      "epoch": 51.99004975124378
    },
    {
      "loss": 2.8999,
      "grad_norm": 2.5907294750213623,
      "learning_rate": 1.548223204398526e-05,
      "epoch": 52.072968490878935
    },
    {
      "loss": 2.9056,
      "grad_norm": 2.5789270401000977,
      "learning_rate": 1.5451071024638707e-05,
      "epoch": 52.1558872305141
    },
    {
      "loss": 2.9129,
      "grad_norm": 2.5710175037384033,
      "learning_rate": 1.541983451512461e-05,
      "epoch": 52.23880597014925
    },
    {
      "loss": 2.901,
      "grad_norm": 2.520054817199707,
      "learning_rate": 1.538852294802756e-05,
      "epoch": 52.32172470978441
    },
    {
      "loss": 2.8739,
      "grad_norm": 2.5241520404815674,
      "learning_rate": 1.53571367569716e-05,
      "epoch": 52.40464344941957
    },
    {
      "loss": 2.8857,
      "grad_norm": 2.4072535037994385,
      "learning_rate": 1.5325676376614215e-05,
      "epoch": 52.48756218905473
    },
    {
      "loss": 2.9151,
      "grad_norm": 2.557680130004883,
      "learning_rate": 1.5294142242640317e-05,
      "epoch": 52.57048092868988
    },
    {
      "loss": 2.8962,
      "grad_norm": 2.5504798889160156,
      "learning_rate": 1.5262534791756202e-05,
      "epoch": 52.653399668325044
    },
    {
      "loss": 2.8675,
      "grad_norm": 2.614145278930664,
      "learning_rate": 1.5230854461683513e-05,
      "epoch": 52.7363184079602
    },
    {
      "loss": 2.8887,
      "grad_norm": 2.56950306892395,
      "learning_rate": 1.5199101691153175e-05,
      "epoch": 52.81923714759536
    },
    {
      "loss": 2.9056,
      "grad_norm": 2.5375986099243164,
      "learning_rate": 1.5167276919899316e-05,
      "epoch": 52.90215588723051
    },
    {
      "loss": 2.8693,
      "grad_norm": 2.484983444213867,
      "learning_rate": 1.5135380588653176e-05,
      "epoch": 52.985074626865675
    },
    {
      "loss": 2.8791,
      "grad_norm": 2.5699238777160645,
      "learning_rate": 1.5103413139137019e-05,
      "epoch": 53.06799336650083
    },
    {
      "loss": 2.902,
      "grad_norm": 2.48618483543396,
      "learning_rate": 1.507137501405799e-05,
      "epoch": 53.15091210613598
    },
    {
      "loss": 2.8969,
      "grad_norm": 2.517817258834839,
      "learning_rate": 1.5039266657102005e-05,
      "epoch": 53.233830845771145
    },
    {
      "loss": 2.8857,
      "grad_norm": 2.560030698776245,
      "learning_rate": 1.5007088512927594e-05,
      "epoch": 53.3167495854063
    },
    {
      "loss": 2.8532,
      "grad_norm": 2.500488758087158,
      "learning_rate": 1.4974841027159757e-05,
      "epoch": 53.39966832504146
    },
    {
      "loss": 2.8677,
      "grad_norm": 2.6393163204193115,
      "learning_rate": 1.4942524646383773e-05,
      "epoch": 53.482587064676615
    },
    {
      "loss": 2.8637,
      "grad_norm": 2.684361457824707,
      "learning_rate": 1.4910139818139035e-05,
      "epoch": 53.565505804311776
    },
    {
      "loss": 2.8688,
      "grad_norm": 2.492476224899292,
      "learning_rate": 1.4877686990912837e-05,
      "epoch": 53.64842454394693
    },
    {
      "loss": 2.8497,
      "grad_norm": 2.5351498126983643,
      "learning_rate": 1.4845166614134174e-05,
      "epoch": 53.73134328358209
    },
    {
      "loss": 2.8582,
      "grad_norm": 2.5832200050354004,
      "learning_rate": 1.4812579138167515e-05,
      "epoch": 53.814262023217246
    },
    {
      "loss": 2.8607,
      "grad_norm": 2.5878021717071533,
      "learning_rate": 1.477992501430656e-05,
      "epoch": 53.89718076285241
    },
    {
      "loss": 2.8806,
      "grad_norm": 2.5666322708129883,
      "learning_rate": 1.4747204694767993e-05,
      "epoch": 53.98009950248756
    },
    {
      "loss": 2.8542,
      "grad_norm": 2.5242598056793213,
      "learning_rate": 1.4714418632685226e-05,
      "epoch": 54.06301824212272
    },
    {
      "loss": 2.8398,
      "grad_norm": 2.5231773853302,
      "learning_rate": 1.4681567282102119e-05,
      "epoch": 54.14593698175788
    },
    {
      "loss": 2.838,
      "grad_norm": 2.6738457679748535,
      "learning_rate": 1.4648651097966689e-05,
      "epoch": 54.22885572139303
    },
    {
      "loss": 2.888,
      "grad_norm": 2.5851171016693115,
      "learning_rate": 1.4615670536124816e-05,
      "epoch": 54.31177446102819
    },
    {
      "loss": 2.8591,
      "grad_norm": 2.5065882205963135,
      "learning_rate": 1.4582626053313917e-05,
      "epoch": 54.39469320066335
    },
    {
      "loss": 2.829,
      "grad_norm": 2.5043113231658936,
      "learning_rate": 1.454951810715665e-05,
      "epoch": 54.47761194029851
    },
    {
      "loss": 2.8286,
      "grad_norm": 2.5150234699249268,
      "learning_rate": 1.4516347156154536e-05,
      "epoch": 54.56053067993366
    },
    {
      "loss": 2.8595,
      "grad_norm": 2.550483465194702,
      "learning_rate": 1.4483113659681639e-05,
      "epoch": 54.643449419568825
    },
    {
      "loss": 2.8573,
      "grad_norm": 2.5297393798828125,
      "learning_rate": 1.4449818077978193e-05,
      "epoch": 54.72636815920398
    },
    {
      "loss": 2.8553,
      "grad_norm": 2.60417103767395,
      "learning_rate": 1.4416460872144237e-05,
      "epoch": 54.80928689883914
    },
    {
      "loss": 2.8685,
      "grad_norm": 2.4599697589874268,
      "learning_rate": 1.4383042504133222e-05,
      "epoch": 54.892205638474294
    },
    {
      "loss": 2.855,
      "grad_norm": 2.670830726623535,
      "learning_rate": 1.434956343674561e-05,
      "epoch": 54.975124378109456
    },
    {
      "loss": 2.815,
      "grad_norm": 2.4789793491363525,
      "learning_rate": 1.431602413362247e-05,
      "epoch": 55.05804311774461
    },
    {
      "loss": 2.8352,
      "grad_norm": 2.453433036804199,
      "learning_rate": 1.4282425059239057e-05,
      "epoch": 55.140961857379764
    },
    {
      "loss": 2.8291,
      "grad_norm": 2.574197769165039,
      "learning_rate": 1.4248766678898386e-05,
      "epoch": 55.223880597014926
    },
    {
      "loss": 2.8014,
      "grad_norm": 2.54305362701416,
      "learning_rate": 1.4215049458724773e-05,
      "epoch": 55.30679933665008
    },
    {
      "loss": 2.8328,
      "grad_norm": 2.4847981929779053,
      "learning_rate": 1.418127386565739e-05,
      "epoch": 55.38971807628524
    },
    {
      "loss": 2.8268,
      "grad_norm": 2.6550791263580322,
      "learning_rate": 1.4147440367443803e-05,
      "epoch": 55.472636815920396
    },
    {
      "loss": 2.8104,
      "grad_norm": 2.546527862548828,
      "learning_rate": 1.411354943263348e-05,
      "epoch": 55.55555555555556
    },
    {
      "loss": 2.8472,
      "grad_norm": 2.589482545852661,
      "learning_rate": 1.4079601530571317e-05,
      "epoch": 55.63847429519071
    },
    {
      "loss": 2.8436,
      "grad_norm": 2.6712701320648193,
      "learning_rate": 1.4045597131391127e-05,
      "epoch": 55.72139303482587
    },
    {
      "loss": 2.8534,
      "grad_norm": 2.4942383766174316,
      "learning_rate": 1.4011536706009134e-05,
      "epoch": 55.80431177446103
    },
    {
      "loss": 2.8446,
      "grad_norm": 2.618297815322876,
      "learning_rate": 1.3977420726117457e-05,
      "epoch": 55.88723051409619
    },
    {
      "loss": 2.8304,
      "grad_norm": 2.5162734985351562,
      "learning_rate": 1.3943249664177563e-05,
      "epoch": 55.97014925373134
    },
    {
      "loss": 2.831,
      "grad_norm": 2.5390002727508545,
      "learning_rate": 1.3909023993413737e-05,
      "epoch": 56.053067993366504
    },
    {
      "loss": 2.7823,
      "grad_norm": 2.655472755432129,
      "learning_rate": 1.3874744187806528e-05,
      "epoch": 56.13598673300166
    },
    {
      "loss": 2.8286,
      "grad_norm": 2.5716376304626465,
      "learning_rate": 1.3840410722086182e-05,
      "epoch": 56.21890547263681
    },
    {
      "loss": 2.8347,
      "grad_norm": 2.556330919265747,
      "learning_rate": 1.3806024071726054e-05,
      "epoch": 56.301824212271974
    },
    {
      "loss": 2.8089,
      "grad_norm": 2.545499801635742,
      "learning_rate": 1.3771584712936053e-05,
      "epoch": 56.38474295190713
    },
    {
      "loss": 2.7908,
      "grad_norm": 2.4983608722686768,
      "learning_rate": 1.3737093122656017e-05,
      "epoch": 56.46766169154229
    },
    {
      "loss": 2.8219,
      "grad_norm": 2.48659610748291,
      "learning_rate": 1.3702549778549134e-05,
      "epoch": 56.550580431177444
    },
    {
      "loss": 2.7944,
      "grad_norm": 2.496427536010742,
      "learning_rate": 1.3667955158995305e-05,
      "epoch": 56.633499170812605
    },
    {
      "loss": 2.791,
      "grad_norm": 2.638976573944092,
      "learning_rate": 1.3633309743084535e-05,
      "epoch": 56.71641791044776
    },
    {
      "loss": 2.8304,
      "grad_norm": 2.555149793624878,
      "learning_rate": 1.3598614010610282e-05,
      "epoch": 56.79933665008292
    },
    {
      "loss": 2.8382,
      "grad_norm": 2.649564266204834,
      "learning_rate": 1.3563868442062836e-05,
      "epoch": 56.882255389718075
    },
    {
      "loss": 2.8108,
      "grad_norm": 2.714327096939087,
      "learning_rate": 1.3529073518622637e-05,
      "epoch": 56.96517412935324
    },
    {
      "loss": 2.8111,
      "grad_norm": 2.571256399154663,
      "learning_rate": 1.3494229722153638e-05,
      "epoch": 57.04809286898839
    },
    {
      "loss": 2.758,
      "grad_norm": 2.506542444229126,
      "learning_rate": 1.3459337535196607e-05,
      "epoch": 57.13101160862355
    },
    {
      "loss": 2.8052,
      "grad_norm": 2.619424343109131,
      "learning_rate": 1.3424397440962467e-05,
      "epoch": 57.21393034825871
    },
    {
      "loss": 2.7832,
      "grad_norm": 2.651806354522705,
      "learning_rate": 1.3389409923325592e-05,
      "epoch": 57.29684908789386
    },
    {
      "loss": 2.7928,
      "grad_norm": 2.597675323486328,
      "learning_rate": 1.3354375466817101e-05,
      "epoch": 57.37976782752902
    },
    {
      "loss": 2.8055,
      "grad_norm": 2.563458204269409,
      "learning_rate": 1.3319294556618164e-05,
      "epoch": 57.46268656716418
    },
    {
      "loss": 2.784,
      "grad_norm": 2.6206090450286865,
      "learning_rate": 1.3284167678553263e-05,
      "epoch": 57.54560530679934
    },
    {
      "loss": 2.7915,
      "grad_norm": 2.594430446624756,
      "learning_rate": 1.3248995319083484e-05,
      "epoch": 57.62852404643449
    },
    {
      "loss": 2.8192,
      "grad_norm": 2.712989330291748,
      "learning_rate": 1.3213777965299768e-05,
      "epoch": 57.711442786069654
    },
    {
      "loss": 2.8045,
      "grad_norm": 2.787637233734131,
      "learning_rate": 1.3178516104916162e-05,
      "epoch": 57.79436152570481
    },
    {
      "loss": 2.7579,
      "grad_norm": 2.454118251800537,
      "learning_rate": 1.3143210226263082e-05,
      "epoch": 57.87728026533997
    },
    {
      "loss": 2.8001,
      "grad_norm": 2.577587127685547,
      "learning_rate": 1.3107860818280533e-05,
      "epoch": 57.960199004975124
    },
    {
      "loss": 2.7777,
      "grad_norm": 2.5629026889801025,
      "learning_rate": 1.307246837051134e-05,
      "epoch": 58.043117744610285
    },
    {
      "loss": 2.7602,
      "grad_norm": 2.7232589721679688,
      "learning_rate": 1.3037033373094377e-05,
      "epoch": 58.12603648424544
    },
    {
      "loss": 2.7924,
      "grad_norm": 2.5203351974487305,
      "learning_rate": 1.3001556316757773e-05,
      "epoch": 58.208955223880594
    },
    {
      "loss": 2.7603,
      "grad_norm": 2.709641218185425,
      "learning_rate": 1.2966037692812124e-05,
      "epoch": 58.291873963515755
    },
    {
      "loss": 2.7478,
      "grad_norm": 2.572925090789795,
      "learning_rate": 1.2930477993143676e-05,
      "epoch": 58.37479270315091
    },
    {
      "loss": 2.8009,
      "grad_norm": 2.6554410457611084,
      "learning_rate": 1.2894877710207519e-05,
      "epoch": 58.45771144278607
    },
    {
      "loss": 2.7902,
      "grad_norm": 2.5887107849121094,
      "learning_rate": 1.2859237337020774e-05,
      "epoch": 58.540630182421225
    },
    {
      "loss": 2.7765,
      "grad_norm": 2.5305638313293457,
      "learning_rate": 1.2823557367155753e-05,
      "epoch": 58.623548922056386
    },
    {
      "loss": 2.7611,
      "grad_norm": 2.7038261890411377,
      "learning_rate": 1.2787838294733132e-05,
      "epoch": 58.70646766169154
    },
    {
      "loss": 2.771,
      "grad_norm": 2.5383334159851074,
      "learning_rate": 1.2752080614415113e-05,
      "epoch": 58.7893864013267
    },
    {
      "loss": 2.7796,
      "grad_norm": 2.5609655380249023,
      "learning_rate": 1.2716284821398547e-05,
      "epoch": 58.872305140961856
    },
    {
      "loss": 2.7761,
      "grad_norm": 2.498276710510254,
      "learning_rate": 1.2680451411408123e-05,
      "epoch": 58.95522388059702
    },
    {
      "loss": 2.7476,
      "grad_norm": 2.5842247009277344,
      "learning_rate": 1.2644580880689457e-05,
      "epoch": 59.03814262023217
    },
    {
      "loss": 2.7797,
      "grad_norm": 2.6011807918548584,
      "learning_rate": 1.260867372600224e-05,
      "epoch": 59.12106135986733
    },
    {
      "loss": 2.7824,
      "grad_norm": 2.5924949645996094,
      "learning_rate": 1.2572730444613363e-05,
      "epoch": 59.20398009950249
    },
    {
      "loss": 2.7609,
      "grad_norm": 2.6564574241638184,
      "learning_rate": 1.2536751534290019e-05,
      "epoch": 59.28689883913764
    },
    {
      "loss": 2.8021,
      "grad_norm": 2.6182076930999756,
      "learning_rate": 1.2500737493292818e-05,
      "epoch": 59.3698175787728
    },
    {
      "loss": 2.7381,
      "grad_norm": 2.6116135120391846,
      "learning_rate": 1.2464688820368884e-05,
      "epoch": 59.45273631840796
    },
    {
      "loss": 2.77,
      "grad_norm": 2.714517593383789,
      "learning_rate": 1.2428606014744943e-05,
      "epoch": 59.53565505804312
    },
    {
      "loss": 2.7742,
      "grad_norm": 2.7161996364593506,
      "learning_rate": 1.2392489576120417e-05,
      "epoch": 59.61857379767827
    },
    {
      "loss": 2.7711,
      "grad_norm": 2.6376402378082275,
      "learning_rate": 1.2356340004660507e-05,
      "epoch": 59.701492537313435
    },
    {
      "loss": 2.7545,
      "grad_norm": 2.7101027965545654,
      "learning_rate": 1.232015780098925e-05,
      "epoch": 59.78441127694859
    },
    {
      "loss": 2.7444,
      "grad_norm": 2.578376293182373,
      "learning_rate": 1.22839434661826e-05,
      "epoch": 59.86733001658375
    },
    {
      "loss": 2.7507,
      "grad_norm": 2.5386998653411865,
      "learning_rate": 1.2247697501761484e-05,
      "epoch": 59.950248756218905
    },
    {
      "loss": 2.7701,
      "grad_norm": 2.643906831741333,
      "learning_rate": 1.2211420409684867e-05,
      "epoch": 60.033167495854066
    },
    {
      "loss": 2.7417,
      "grad_norm": 2.5577454566955566,
      "learning_rate": 1.2175112692342773e-05,
      "epoch": 60.11608623548922
    },
    {
      "loss": 2.7274,
      "grad_norm": 2.5493829250335693,
      "learning_rate": 1.2138774852549366e-05,
      "epoch": 60.19900497512438
    },
    {
      "loss": 2.7439,
      "grad_norm": 2.62650990486145,
      "learning_rate": 1.210240739353595e-05,
      "epoch": 60.281923714759536
    },
    {
      "loss": 2.7317,
      "grad_norm": 2.626532554626465,
      "learning_rate": 1.2066010818944024e-05,
      "epoch": 60.36484245439469
    },
    {
      "loss": 2.7415,
      "grad_norm": 2.553262233734131,
      "learning_rate": 1.2029585632818294e-05,
      "epoch": 60.44776119402985
    },
    {
      "loss": 2.7502,
      "grad_norm": 2.5994648933410645,
      "learning_rate": 1.1993132339599711e-05,
      "epoch": 60.530679933665006
    },
    {
      "loss": 2.7176,
      "grad_norm": 2.503819227218628,
      "learning_rate": 1.1956651444118454e-05,
      "epoch": 60.61359867330017
    },
    {
      "loss": 2.7513,
      "grad_norm": 2.5096678733825684,
      "learning_rate": 1.1920143451586973e-05,
      "epoch": 60.69651741293532
    },
    {
      "loss": 2.773,
      "grad_norm": 2.6284029483795166,
      "learning_rate": 1.188360886759297e-05,
      "epoch": 60.77943615257048
    },
    {
      "loss": 2.7524,
      "grad_norm": 2.593637704849243,
      "learning_rate": 1.1847048198092402e-05,
      "epoch": 60.86235489220564
    },
    {
      "loss": 2.7353,
      "grad_norm": 2.546921491622925,
      "learning_rate": 1.1810461949402474e-05,
      "epoch": 60.9452736318408
    },
    {
      "loss": 2.7475,
      "grad_norm": 2.8388683795928955,
      "learning_rate": 1.177385062819464e-05,
      "epoch": 61.02819237147595
    },
    {
      "loss": 2.7046,
      "grad_norm": 2.6506094932556152,
      "learning_rate": 1.1737214741487563e-05,
      "epoch": 61.111111111111114
    },
    {
      "loss": 2.7286,
      "grad_norm": 2.527695894241333,
      "learning_rate": 1.1700554796640113e-05,
      "epoch": 61.19402985074627
    },
    {
      "loss": 2.7313,
      "grad_norm": 2.5835044384002686,
      "learning_rate": 1.1663871301344332e-05,
      "epoch": 61.27694859038142
    },
    {
      "loss": 2.7436,
      "grad_norm": 2.708101272583008,
      "learning_rate": 1.1627164763618406e-05,
      "epoch": 61.359867330016584
    },
    {
      "loss": 2.7261,
      "grad_norm": 2.493542194366455,
      "learning_rate": 1.1590435691799624e-05,
      "epoch": 61.44278606965174
    },
    {
      "loss": 2.7474,
      "grad_norm": 2.664074182510376,
      "learning_rate": 1.1553684594537349e-05,
      "epoch": 61.5257048092869
    },
    {
      "loss": 2.7292,
      "grad_norm": 2.5926294326782227,
      "learning_rate": 1.1516911980785958e-05,
      "epoch": 61.608623548922054
    },
    {
      "loss": 2.7261,
      "grad_norm": 2.606707811355591,
      "learning_rate": 1.1480118359797817e-05,
      "epoch": 61.691542288557216
    },
    {
      "loss": 2.7167,
      "grad_norm": 2.621661901473999,
      "learning_rate": 1.1443304241116203e-05,
      "epoch": 61.77446102819237
    },
    {
      "loss": 2.7465,
      "grad_norm": 2.660273313522339,
      "learning_rate": 1.140647013456826e-05,
      "epoch": 61.85737976782753
    },
    {
      "loss": 2.724,
      "grad_norm": 2.679640054702759,
      "learning_rate": 1.1369616550257943e-05,
      "epoch": 61.940298507462686
    },
    {
      "loss": 2.7267,
      "grad_norm": 2.4985883235931396,
      "learning_rate": 1.1332743998558944e-05,
      "epoch": 62.02321724709785
    },
    {
      "loss": 2.7329,
      "grad_norm": 2.765932321548462,
      "learning_rate": 1.1295852990107634e-05,
      "epoch": 62.106135986733
    },
    {
      "loss": 2.7053,
      "grad_norm": 2.6678519248962402,
      "learning_rate": 1.125894403579598e-05,
      "epoch": 62.18905472636816
    },
    {
      "loss": 2.7036,
      "grad_norm": 2.5786983966827393,
      "learning_rate": 1.1222017646764475e-05,
      "epoch": 62.27197346600332
    },
    {
      "loss": 2.7241,
      "grad_norm": 2.5020642280578613,
      "learning_rate": 1.1185074334395065e-05,
      "epoch": 62.35489220563847
    },
    {
      "loss": 2.7029,
      "grad_norm": 2.698390007019043,
      "learning_rate": 1.1148114610304061e-05,
      "epoch": 62.43781094527363
    },
    {
      "loss": 2.7255,
      "grad_norm": 2.621612548828125,
      "learning_rate": 1.1111138986335052e-05,
      "epoch": 62.52072968490879
    },
    {
      "loss": 2.7178,
      "grad_norm": 2.5771093368530273,
      "learning_rate": 1.107414797455182e-05,
      "epoch": 62.60364842454395
    },
    {
      "loss": 2.7292,
      "grad_norm": 2.4121978282928467,
      "learning_rate": 1.1037142087231247e-05,
      "epoch": 62.6865671641791
    },
    {
      "loss": 2.696,
      "grad_norm": 2.5290143489837646,
      "learning_rate": 1.1000121836856228e-05,
      "epoch": 62.769485903814264
    },
    {
      "loss": 2.6884,
      "grad_norm": 2.541264533996582,
      "learning_rate": 1.096308773610856e-05,
      "epoch": 62.85240464344942
    },
    {
      "loss": 2.6926,
      "grad_norm": 2.606375217437744,
      "learning_rate": 1.0926040297861851e-05,
      "epoch": 62.93532338308458
    },
    {
      "loss": 2.7144,
      "grad_norm": 2.534533977508545,
      "learning_rate": 1.0888980035174414e-05,
      "epoch": 63.018242122719734
    },
    {
      "loss": 2.684,
      "grad_norm": 2.654209613800049,
      "learning_rate": 1.0851907461282174e-05,
      "epoch": 63.101160862354895
    },
    {
      "loss": 2.6995,
      "grad_norm": 2.6150143146514893,
      "learning_rate": 1.0814823089591542e-05,
      "epoch": 63.18407960199005
    },
    {
      "loss": 2.7057,
      "grad_norm": 2.709798574447632,
      "learning_rate": 1.0777727433672311e-05,
      "epoch": 63.26699834162521
    },
    {
      "loss": 2.7207,
      "grad_norm": 2.6051745414733887,
      "learning_rate": 1.0740621007250545e-05,
      "epoch": 63.349917081260365
    },
    {
      "loss": 2.7129,
      "grad_norm": 2.5947763919830322,
      "learning_rate": 1.0703504324201472e-05,
      "epoch": 63.43283582089552
    },
    {
      "loss": 2.6971,
      "grad_norm": 2.603424072265625,
      "learning_rate": 1.066637789854236e-05,
      "epoch": 63.51575456053068
    },
    {
      "loss": 2.69,
      "grad_norm": 2.651726484298706,
      "learning_rate": 1.0629242244425394e-05,
      "epoch": 63.598673300165835
    },
    {
      "loss": 2.6991,
      "grad_norm": 2.6992974281311035,
      "learning_rate": 1.059209787613056e-05,
      "epoch": 63.681592039801
    },
    {
      "loss": 2.6995,
      "grad_norm": 2.594829559326172,
      "learning_rate": 1.0554945308058537e-05,
      "epoch": 63.76451077943615
    },
    {
      "loss": 2.6711,
      "grad_norm": 2.554511785507202,
      "learning_rate": 1.0517785054723546e-05,
      "epoch": 63.84742951907131
    },
    {
      "loss": 2.6921,
      "grad_norm": 2.580731153488159,
      "learning_rate": 1.0480617630746241e-05,
      "epoch": 63.930348258706466
    },
    {
      "loss": 2.6912,
      "grad_norm": 2.734182834625244,
      "learning_rate": 1.0443443550846586e-05,
      "epoch": 64.01326699834162
    },
    {
      "loss": 2.6823,
      "grad_norm": 2.7193052768707275,
      "learning_rate": 1.0406263329836714e-05,
      "epoch": 64.09618573797678
    },
    {
      "loss": 2.6695,
      "grad_norm": 2.7004690170288086,
      "learning_rate": 1.0369077482613812e-05,
      "epoch": 64.17910447761194
    },
    {
      "loss": 2.678,
      "grad_norm": 2.5916712284088135,
      "learning_rate": 1.0331886524152978e-05,
      "epoch": 64.2620232172471
    },
    {
      "loss": 2.6977,
      "grad_norm": 2.6555120944976807,
      "learning_rate": 1.0294690969500092e-05,
      "epoch": 64.34494195688225
    },
    {
      "loss": 2.6809,
      "grad_norm": 2.6786949634552,
      "learning_rate": 1.0257491333764688e-05,
      "epoch": 64.42786069651741
    },
    {
      "loss": 2.7065,
      "grad_norm": 2.591284990310669,
      "learning_rate": 1.022028813211282e-05,
      "epoch": 64.51077943615257
    },
    {
      "loss": 2.6738,
      "grad_norm": 2.558490753173828,
      "learning_rate": 1.0183081879759916e-05,
      "epoch": 64.59369817578772
    },
    {
      "loss": 2.6674,
      "grad_norm": 2.6576380729675293,
      "learning_rate": 1.0145873091963665e-05,
      "epoch": 64.67661691542288
    },
    {
      "loss": 2.6734,
      "grad_norm": 2.8266961574554443,
      "learning_rate": 1.0108662284016857e-05,
      "epoch": 64.75953565505804
    },
    {
      "loss": 2.7016,
      "grad_norm": 2.656938314437866,
      "learning_rate": 1.0071449971240268e-05,
      "epoch": 64.8424543946932
    },
    {
      "loss": 2.6665,
      "grad_norm": 2.581664562225342,
      "learning_rate": 1.0034236668975508e-05,
      "epoch": 64.92537313432835
    },
    {
      "loss": 2.6844,
      "grad_norm": 2.715837240219116,
      "learning_rate": 9.99702289257789e-06,
      "epoch": 65.00829187396351
    },
    {
      "loss": 2.6938,
      "grad_norm": 2.6830215454101562,
      "learning_rate": 9.9598091574093e-06,
      "epoch": 65.09121061359868
    },
    {
      "loss": 2.6807,
      "grad_norm": 2.684473991394043,
      "learning_rate": 9.922595978831039e-06,
      "epoch": 65.17412935323384
    },
    {
      "loss": 2.6849,
      "grad_norm": 2.7103796005249023,
      "learning_rate": 9.885383872196713e-06,
      "epoch": 65.25704809286898
    },
    {
      "loss": 2.6688,
      "grad_norm": 2.5902340412139893,
      "learning_rate": 9.848173352845082e-06,
      "epoch": 65.33996683250415
    },
    {
      "loss": 2.645,
      "grad_norm": 2.666278123855591,
      "learning_rate": 9.810964936092917e-06,
      "epoch": 65.42288557213931
    },
    {
      "loss": 2.6993,
      "grad_norm": 2.5789380073547363,
      "learning_rate": 9.773759137227878e-06,
      "epoch": 65.50580431177445
    },
    {
      "loss": 2.6621,
      "grad_norm": 2.713064193725586,
      "learning_rate": 9.736556471501363e-06,
      "epoch": 65.58872305140962
    },
    {
      "loss": 2.6845,
      "grad_norm": 2.6036365032196045,
      "learning_rate": 9.699357454121385e-06,
      "epoch": 65.67164179104478
    },
    {
      "loss": 2.6449,
      "grad_norm": 2.545564651489258,
      "learning_rate": 9.662162600245437e-06,
      "epoch": 65.75456053067994
    },
    {
      "loss": 2.6712,
      "grad_norm": 2.6406123638153076,
      "learning_rate": 9.62497242497334e-06,
      "epoch": 65.83747927031509
    },
    {
      "loss": 2.6589,
      "grad_norm": 2.526522636413574,
      "learning_rate": 9.587787443340134e-06,
      "epoch": 65.92039800995025
    },
    {
      "loss": 2.6499,
      "grad_norm": 2.535550594329834,
      "learning_rate": 9.550608170308935e-06,
      "epoch": 66.00331674958541
    },
    {
      "loss": 2.6486,
      "grad_norm": 2.499955892562866,
      "learning_rate": 9.513435120763791e-06,
      "epoch": 66.08623548922057
    },
    {
      "loss": 2.6436,
      "grad_norm": 2.6330344676971436,
      "learning_rate": 9.47626880950257e-06,
      "epoch": 66.16915422885572
    },
    {
      "loss": 2.6483,
      "grad_norm": 2.5194027423858643,
      "learning_rate": 9.439109751229825e-06,
      "epoch": 66.25207296849088
    },
    {
      "loss": 2.664,
      "grad_norm": 2.678152322769165,
      "learning_rate": 9.401958460549658e-06,
      "epoch": 66.33499170812604
    },
    {
      "loss": 2.6328,
      "grad_norm": 2.7169365882873535,
      "learning_rate": 9.364815451958615e-06,
      "epoch": 66.41791044776119
    },
    {
      "loss": 2.6441,
      "grad_norm": 2.627577304840088,
      "learning_rate": 9.327681239838523e-06,
      "epoch": 66.50082918739635
    },
    {
      "loss": 2.6513,
      "grad_norm": 2.6837351322174072,
      "learning_rate": 9.29055633844941e-06,
      "epoch": 66.58374792703151
    },
    {
      "loss": 2.6324,
      "grad_norm": 2.6815102100372314,
      "learning_rate": 9.253441261922353e-06,
      "epoch": 66.66666666666667
    },
    {
      "loss": 2.6648,
      "grad_norm": 2.650498628616333,
      "learning_rate": 9.216336524252368e-06,
      "epoch": 66.74958540630182
    },
    {
      "loss": 2.6361,
      "grad_norm": 2.6691997051239014,
      "learning_rate": 9.179242639291297e-06,
      "epoch": 66.83250414593698
    },
    {
      "loss": 2.6715,
      "grad_norm": 2.6693203449249268,
      "learning_rate": 9.142160120740673e-06,
      "epoch": 66.91542288557214
    },
    {
      "loss": 2.6708,
      "grad_norm": 2.6856088638305664,
      "learning_rate": 9.105089482144635e-06,
      "epoch": 66.9983416252073
    },
    {
      "loss": 2.6401,
      "grad_norm": 2.629140615463257,
      "learning_rate": 9.068031236882798e-06,
      "epoch": 67.08126036484245
    },
    {
      "loss": 2.6452,
      "grad_norm": 2.6151883602142334,
      "learning_rate": 9.030985898163128e-06,
      "epoch": 67.16417910447761
    },
    {
      "loss": 2.6622,
      "grad_norm": 2.608490467071533,
      "learning_rate": 8.993953979014877e-06,
      "epoch": 67.24709784411277
    },
    {
      "loss": 2.6268,
      "grad_norm": 2.609046220779419,
      "learning_rate": 8.956935992281435e-06,
      "epoch": 67.33001658374793
    },
    {
      "loss": 2.637,
      "grad_norm": 2.727060079574585,
      "learning_rate": 8.919932450613254e-06,
      "epoch": 67.41293532338308
    },
    {
      "loss": 2.6417,
      "grad_norm": 2.6682188510894775,
      "learning_rate": 8.882943866460746e-06,
      "epoch": 67.49585406301824
    },
    {
      "loss": 2.6363,
      "grad_norm": 2.5543792247772217,
      "learning_rate": 8.845970752067165e-06,
      "epoch": 67.5787728026534
    },
    {
      "loss": 2.6538,
      "grad_norm": 2.6879994869232178,
      "learning_rate": 8.809013619461546e-06,
      "epoch": 67.66169154228855
    },
    {
      "loss": 2.6573,
      "grad_norm": 2.6476290225982666,
      "learning_rate": 8.772072980451594e-06,
      "epoch": 67.74461028192371
    },
    {
      "loss": 2.6669,
      "grad_norm": 2.814992666244507,
      "learning_rate": 8.73514934661659e-06,
      "epoch": 67.82752902155887
    },
    {
      "loss": 2.621,
      "grad_norm": 2.618320941925049,
      "learning_rate": 8.69824322930032e-06,
      "epoch": 67.91044776119404
    },
    {
      "loss": 2.6234,
      "grad_norm": 2.668865203857422,
      "learning_rate": 8.661355139603998e-06,
      "epoch": 67.99336650082918
    },
    {
      "loss": 2.6609,
      "grad_norm": 2.6378605365753174,
      "learning_rate": 8.624485588379164e-06,
      "epoch": 68.07628524046434
    },
    {
      "loss": 2.6154,
      "grad_norm": 2.702723979949951,
      "learning_rate": 8.587635086220642e-06,
      "epoch": 68.1592039800995
    },
    {
      "loss": 2.6162,
      "grad_norm": 2.6275179386138916,
      "learning_rate": 8.550804143459431e-06,
      "epoch": 68.24212271973467
    },
    {
      "loss": 2.6405,
      "grad_norm": 2.7347664833068848,
      "learning_rate": 8.513993270155681e-06,
      "epoch": 68.32504145936981
    },
    {
      "loss": 2.6547,
      "grad_norm": 2.668870449066162,
      "learning_rate": 8.477202976091583e-06,
      "epoch": 68.40796019900498
    },
    {
      "loss": 2.6571,
      "grad_norm": 2.6861395835876465,
      "learning_rate": 8.440433770764352e-06,
      "epoch": 68.49087893864014
    },
    {
      "loss": 2.6388,
      "grad_norm": 2.6949496269226074,
      "learning_rate": 8.403686163379144e-06,
      "epoch": 68.57379767827528
    },
    {
      "loss": 2.6236,
      "grad_norm": 2.6134684085845947,
      "learning_rate": 8.366960662842004e-06,
      "epoch": 68.65671641791045
    },
    {
      "loss": 2.6115,
      "grad_norm": 2.7658510208129883,
      "learning_rate": 8.330257777752841e-06,
      "epoch": 68.7396351575456
    },
    {
      "loss": 2.6387,
      "grad_norm": 2.6879663467407227,
      "learning_rate": 8.293578016398364e-06,
      "epoch": 68.82255389718077
    },
    {
      "loss": 2.6409,
      "grad_norm": 2.7814276218414307,
      "learning_rate": 8.256921886745041e-06,
      "epoch": 68.90547263681592
    },
    {
      "loss": 2.6406,
      "grad_norm": 2.724334239959717,
      "learning_rate": 8.220289896432085e-06,
      "epoch": 68.98839137645108
    },
    {
      "loss": 2.6251,
      "grad_norm": 2.7448232173919678,
      "learning_rate": 8.183682552764401e-06,
      "epoch": 69.07131011608624
    },
    {
      "loss": 2.6327,
      "grad_norm": 2.637857437133789,
      "learning_rate": 8.14710036270557e-06,
      "epoch": 69.1542288557214
    },
    {
      "loss": 2.6242,
      "grad_norm": 2.7387027740478516,
      "learning_rate": 8.110543832870838e-06,
      "epoch": 69.23714759535655
    },
    {
      "loss": 2.6243,
      "grad_norm": 2.611849784851074,
      "learning_rate": 8.07401346952008e-06,
      "epoch": 69.32006633499171
    },
    {
      "loss": 2.6257,
      "grad_norm": 2.70796275138855,
      "learning_rate": 8.037509778550808e-06,
      "epoch": 69.40298507462687
    },
    {
      "loss": 2.6287,
      "grad_norm": 2.597116470336914,
      "learning_rate": 8.001033265491156e-06,
      "epoch": 69.48590381426202
    },
    {
      "loss": 2.6333,
      "grad_norm": 2.7035934925079346,
      "learning_rate": 7.96458443549287e-06,
      "epoch": 69.56882255389718
    },
    {
      "loss": 2.6472,
      "grad_norm": 2.6586973667144775,
      "learning_rate": 7.928163793324331e-06,
      "epoch": 69.65174129353234
    },
    {
      "loss": 2.6138,
      "grad_norm": 2.738905429840088,
      "learning_rate": 7.891771843363557e-06,
      "epoch": 69.7346600331675
    },
    {
      "loss": 2.6346,
      "grad_norm": 2.648172616958618,
      "learning_rate": 7.855409089591206e-06,
      "epoch": 69.81757877280265
    },
    {
      "loss": 2.587,
      "grad_norm": 2.773963689804077,
      "learning_rate": 7.81907603558362e-06,
      "epoch": 69.90049751243781
    },
    {
      "loss": 2.6129,
      "grad_norm": 2.7465689182281494,
      "learning_rate": 7.78277318450583e-06,
      "epoch": 69.98341625207297
    },
    {
      "loss": 2.6231,
      "grad_norm": 2.6270785331726074,
      "learning_rate": 7.746501039104596e-06,
      "epoch": 70.06633499170813
    },
    {
      "loss": 2.5902,
      "grad_norm": 2.6625618934631348,
      "learning_rate": 7.710260101701457e-06,
      "epoch": 70.14925373134328
    },
    {
      "loss": 2.6142,
      "grad_norm": 2.6762704849243164,
      "learning_rate": 7.674050874185742e-06,
      "epoch": 70.23217247097844
    },
    {
      "loss": 2.6554,
      "grad_norm": 2.689724922180176,
      "learning_rate": 7.637873858007658e-06,
      "epoch": 70.3150912106136
    },
    {
      "loss": 2.6111,
      "grad_norm": 2.6568408012390137,
      "learning_rate": 7.6017295541713106e-06,
      "epoch": 70.39800995024876
    },
    {
      "loss": 2.6021,
      "grad_norm": 2.5900585651397705,
      "learning_rate": 7.565618463227797e-06,
      "epoch": 70.48092868988391
    },
    {
      "loss": 2.6067,
      "grad_norm": 2.703669786453247,
      "learning_rate": 7.529541085268257e-06,
      "epoch": 70.56384742951907
    },
    {
      "loss": 2.6034,
      "grad_norm": 2.8062336444854736,
      "learning_rate": 7.493497919916941e-06,
      "epoch": 70.64676616915423
    },
    {
      "loss": 2.6451,
      "grad_norm": 2.716012954711914,
      "learning_rate": 7.457489466324313e-06,
      "epoch": 70.72968490878938
    },
    {
      "loss": 2.6164,
      "grad_norm": 2.6901638507843018,
      "learning_rate": 7.421516223160108e-06,
      "epoch": 70.81260364842454
    },
    {
      "loss": 2.6074,
      "grad_norm": 2.697892427444458,
      "learning_rate": 7.385578688606456e-06,
      "epoch": 70.8955223880597
    },
    {
      "loss": 2.609,
      "grad_norm": 2.6864304542541504,
      "learning_rate": 7.349677360350972e-06,
      "epoch": 70.97844112769486
    },
    {
      "loss": 2.6114,
      "grad_norm": 2.721158504486084,
      "learning_rate": 7.313812735579846e-06,
      "epoch": 71.06135986733001
    },
    {
      "loss": 2.5955,
      "grad_norm": 2.7050278186798096,
      "learning_rate": 7.277985310970985e-06,
      "epoch": 71.14427860696517
    },
    {
      "loss": 2.6085,
      "grad_norm": 2.643568992614746,
      "learning_rate": 7.2421955826871285e-06,
      "epoch": 71.22719734660033
    },
    {
      "loss": 2.5863,
      "grad_norm": 2.6618735790252686,
      "learning_rate": 7.20644404636895e-06,
      "epoch": 71.3101160862355
    },
    {
      "loss": 2.5996,
      "grad_norm": 2.6532039642333984,
      "learning_rate": 7.170731197128238e-06,
      "epoch": 71.39303482587064
    },
    {
      "loss": 2.5989,
      "grad_norm": 2.547654867172241,
      "learning_rate": 7.135057529540998e-06,
      "epoch": 71.4759535655058
    },
    {
      "loss": 2.5898,
      "grad_norm": 2.71529221534729,
      "learning_rate": 7.099423537640631e-06,
      "epoch": 71.55887230514097
    },
    {
      "loss": 2.6288,
      "grad_norm": 2.7938344478607178,
      "learning_rate": 7.063829714911081e-06,
      "epoch": 71.64179104477611
    },
    {
      "loss": 2.5986,
      "grad_norm": 2.7372617721557617,
      "learning_rate": 7.028276554279996e-06,
      "epoch": 71.72470978441127
    },
    {
      "loss": 2.6196,
      "grad_norm": 2.622724771499634,
      "learning_rate": 6.9927645481119125e-06,
      "epoch": 71.80762852404644
    },
    {
      "loss": 2.6141,
      "grad_norm": 2.765181064605713,
      "learning_rate": 6.957294188201438e-06,
      "epoch": 71.8905472636816
    },
    {
      "loss": 2.6112,
      "grad_norm": 2.564234972000122,
      "learning_rate": 6.9218659657664164e-06,
      "epoch": 71.97346600331674
    },
    {
      "loss": 2.5973,
      "grad_norm": 2.7912979125976562,
      "learning_rate": 6.886480371441162e-06,
      "epoch": 72.0563847429519
    },
    {
      "loss": 2.5904,
      "grad_norm": 2.7319869995117188,
      "learning_rate": 6.851137895269629e-06,
      "epoch": 72.13930348258707
    },
    {
      "loss": 2.5859,
      "grad_norm": 2.670576333999634,
      "learning_rate": 6.815839026698653e-06,
      "epoch": 72.22222222222223
    },
    {
      "loss": 2.5901,
      "grad_norm": 2.7021846771240234,
      "learning_rate": 6.780584254571164e-06,
      "epoch": 72.30514096185738
    },
    {
      "loss": 2.6015,
      "grad_norm": 2.6387991905212402,
      "learning_rate": 6.745374067119401e-06,
      "epoch": 72.38805970149254
    },
    {
      "loss": 2.6026,
      "grad_norm": 2.8301031589508057,
      "learning_rate": 6.71020895195818e-06,
      "epoch": 72.4709784411277
    },
    {
      "loss": 2.6027,
      "grad_norm": 2.664266586303711,
      "learning_rate": 6.675089396078108e-06,
      "epoch": 72.55389718076285
    },
    {
      "loss": 2.6174,
      "grad_norm": 2.761650562286377,
      "learning_rate": 6.640015885838872e-06,
      "epoch": 72.636815920398
    },
    {
      "loss": 2.5799,
      "grad_norm": 2.677297830581665,
      "learning_rate": 6.604988906962478e-06,
      "epoch": 72.71973466003317
    },
    {
      "loss": 2.5796,
      "grad_norm": 2.717806100845337,
      "learning_rate": 6.570008944526531e-06,
      "epoch": 72.80265339966833
    },
    {
      "loss": 2.6234,
      "grad_norm": 2.7263338565826416,
      "learning_rate": 6.53507648295753e-06,
      "epoch": 72.88557213930348
    },
    {
      "loss": 2.6032,
      "grad_norm": 2.623961925506592,
      "learning_rate": 6.500192006024146e-06,
      "epoch": 72.96849087893864
    },
    {
      "loss": 2.5752,
      "grad_norm": 2.665296792984009,
      "learning_rate": 6.46535599683052e-06,
      "epoch": 73.0514096185738
    },
    {
      "loss": 2.597,
      "grad_norm": 2.771176815032959,
      "learning_rate": 6.430568937809587e-06,
      "epoch": 73.13432835820896
    },
    {
      "loss": 2.5878,
      "grad_norm": 2.68989896774292,
      "learning_rate": 6.395831310716378e-06,
      "epoch": 73.21724709784411
    },
    {
      "loss": 2.604,
      "grad_norm": 2.7165040969848633,
      "learning_rate": 6.361143596621362e-06,
      "epoch": 73.30016583747927
    },
    {
      "loss": 2.5939,
      "grad_norm": 2.6148743629455566,
      "learning_rate": 6.326506275903781e-06,
      "epoch": 73.38308457711443
    },
    {
      "loss": 2.5962,
      "grad_norm": 2.7487242221832275,
      "learning_rate": 6.291919828244986e-06,
      "epoch": 73.46600331674958
    },
    {
      "loss": 2.5922,
      "grad_norm": 2.6500847339630127,
      "learning_rate": 6.257384732621812e-06,
      "epoch": 73.54892205638474
    },
    {
      "loss": 2.5999,
      "grad_norm": 2.704500436782837,
      "learning_rate": 6.2229014672999366e-06,
      "epoch": 73.6318407960199
    },
    {
      "loss": 2.602,
      "grad_norm": 2.7686655521392822,
      "learning_rate": 6.188470509827244e-06,
      "epoch": 73.71475953565506
    },
    {
      "loss": 2.5758,
      "grad_norm": 2.807804822921753,
      "learning_rate": 6.1540923370272394e-06,
      "epoch": 73.79767827529021
    },
    {
      "loss": 2.5635,
      "grad_norm": 2.6159536838531494,
      "learning_rate": 6.119767424992411e-06,
      "epoch": 73.88059701492537
    },
    {
      "loss": 2.5918,
      "grad_norm": 2.737560749053955,
      "learning_rate": 6.085496249077671e-06,
      "epoch": 73.96351575456053
    },
    {
      "loss": 2.5581,
      "grad_norm": 2.7129452228546143,
      "learning_rate": 6.051279283893758e-06,
      "epoch": 74.0464344941957
    },
    {
      "loss": 2.5736,
      "grad_norm": 2.7567999362945557,
      "learning_rate": 6.017117003300648e-06,
      "epoch": 74.12935323383084
    },
    {
      "loss": 2.5812,
      "grad_norm": 2.7670106887817383,
      "learning_rate": 5.9830098804010205e-06,
      "epoch": 74.212271973466
    },
    {
      "loss": 2.5737,
      "grad_norm": 2.732140064239502,
      "learning_rate": 5.948958387533688e-06,
      "epoch": 74.29519071310116
    },
    {
      "loss": 2.6104,
      "grad_norm": 2.6274526119232178,
      "learning_rate": 5.914962996267064e-06,
      "epoch": 74.37810945273633
    },
    {
      "loss": 2.6017,
      "grad_norm": 2.773092746734619,
      "learning_rate": 5.881024177392627e-06,
      "epoch": 74.46102819237147
    },
    {
      "loss": 2.6089,
      "grad_norm": 2.666109085083008,
      "learning_rate": 5.8471424009183954e-06,
      "epoch": 74.54394693200663
    },
    {
      "loss": 2.5666,
      "grad_norm": 2.6835076808929443,
      "learning_rate": 5.8133181360624336e-06,
      "epoch": 74.6268656716418
    },
    {
      "loss": 2.5485,
      "grad_norm": 2.742487668991089,
      "learning_rate": 5.779551851246348e-06,
      "epoch": 74.70978441127694
    },
    {
      "loss": 2.5852,
      "grad_norm": 2.7575390338897705,
      "learning_rate": 5.745844014088788e-06,
      "epoch": 74.7927031509121
    },
    {
      "loss": 2.6081,
      "grad_norm": 2.70516037940979,
      "learning_rate": 5.712195091398989e-06,
      "epoch": 74.87562189054727
    },
    {
      "loss": 2.5738,
      "grad_norm": 2.6954588890075684,
      "learning_rate": 5.678605549170288e-06,
      "epoch": 74.95854063018243
    },
    {
      "loss": 2.5678,
      "grad_norm": 2.712873697280884,
      "learning_rate": 5.645075852573692e-06,
      "epoch": 75.04145936981757
    },
    {
      "loss": 2.5715,
      "grad_norm": 2.6906161308288574,
      "learning_rate": 5.611606465951421e-06,
      "epoch": 75.12437810945273
    },
    {
      "loss": 2.5692,
      "grad_norm": 2.8091204166412354,
      "learning_rate": 5.578197852810479e-06,
      "epoch": 75.2072968490879
    },
    {
      "loss": 2.5867,
      "grad_norm": 2.7085142135620117,
      "learning_rate": 5.544850475816232e-06,
      "epoch": 75.29021558872306
    },
    {
      "loss": 2.5754,
      "grad_norm": 2.8477487564086914,
      "learning_rate": 5.511564796786021e-06,
      "epoch": 75.3731343283582
    },
    {
      "loss": 2.5662,
      "grad_norm": 2.5835509300231934,
      "learning_rate": 5.478341276682733e-06,
      "epoch": 75.45605306799337
    },
    {
      "loss": 2.5829,
      "grad_norm": 2.662257194519043,
      "learning_rate": 5.445180375608453e-06,
      "epoch": 75.53897180762853
    },
    {
      "loss": 2.568,
      "grad_norm": 2.7049691677093506,
      "learning_rate": 5.412082552798059e-06,
      "epoch": 75.62189054726367
    },
    {
      "loss": 2.5509,
      "grad_norm": 2.729262351989746,
      "learning_rate": 5.379048266612889e-06,
      "epoch": 75.70480928689884
    },
    {
      "loss": 2.5782,
      "grad_norm": 2.8369548320770264,
      "learning_rate": 5.346077974534384e-06,
      "epoch": 75.787728026534
    },
    {
      "loss": 2.5496,
      "grad_norm": 2.6774046421051025,
      "learning_rate": 5.313172133157736e-06,
      "epoch": 75.87064676616916
    },
    {
      "loss": 2.5467,
      "grad_norm": 2.7489101886749268,
      "learning_rate": 5.280331198185593e-06,
      "epoch": 75.9535655058043
    },
    {
      "loss": 2.5809,
      "grad_norm": 2.7505369186401367,
      "learning_rate": 5.247555624421736e-06,
      "epoch": 76.03648424543947
    },
    {
      "loss": 2.5588,
      "grad_norm": 2.7050042152404785,
      "learning_rate": 5.214845865764766e-06,
      "epoch": 76.11940298507463
    },
    {
      "loss": 2.5844,
      "grad_norm": 2.754795551300049,
      "learning_rate": 5.1822023752018445e-06,
      "epoch": 76.20232172470979
    },
    {
      "loss": 2.5616,
      "grad_norm": 2.7267231941223145,
      "learning_rate": 5.149625604802396e-06,
      "epoch": 76.28524046434494
    },
    {
      "loss": 2.5573,
      "grad_norm": 2.6510658264160156,
      "learning_rate": 5.117116005711872e-06,
      "epoch": 76.3681592039801
    },
    {
      "loss": 2.5599,
      "grad_norm": 2.7283740043640137,
      "learning_rate": 5.084674028145476e-06,
      "epoch": 76.45107794361526
    },
    {
      "loss": 2.5614,
      "grad_norm": 2.621262550354004,
      "learning_rate": 5.0523001213819464e-06,
      "epoch": 76.53399668325042
    },
    {
      "loss": 2.5712,
      "grad_norm": 2.6701242923736572,
      "learning_rate": 5.0199947337573386e-06,
      "epoch": 76.61691542288557
    },
    {
      "loss": 2.5555,
      "grad_norm": 2.6688222885131836,
      "learning_rate": 4.987758312658795e-06,
      "epoch": 76.69983416252073
    },
    {
      "loss": 2.516,
      "grad_norm": 2.7219057083129883,
      "learning_rate": 4.95559130451837e-06,
      "epoch": 76.78275290215589
    },
    {
      "loss": 2.5685,
      "grad_norm": 2.750861644744873,
      "learning_rate": 4.923494154806845e-06,
      "epoch": 76.86567164179104
    },
    {
      "loss": 2.5838,
      "grad_norm": 2.7492034435272217,
      "learning_rate": 4.891467308027539e-06,
      "epoch": 76.9485903814262
    },
    {
      "loss": 2.5273,
      "grad_norm": 2.6925172805786133,
      "learning_rate": 4.85951120771018e-06,
      "epoch": 77.03150912106136
    },
    {
      "loss": 2.5391,
      "grad_norm": 2.727611541748047,
      "learning_rate": 4.827626296404751e-06,
      "epoch": 77.11442786069652
    },
    {
      "loss": 2.5576,
      "grad_norm": 2.76997709274292,
      "learning_rate": 4.795813015675351e-06,
      "epoch": 77.19734660033167
    },
    {
      "loss": 2.5501,
      "grad_norm": 2.8407013416290283,
      "learning_rate": 4.7640718060941e-06,
      "epoch": 77.28026533996683
    },
    {
      "loss": 2.5576,
      "grad_norm": 2.6784074306488037,
      "learning_rate": 4.732403107235015e-06,
      "epoch": 77.363184079602
    },
    {
      "loss": 2.5489,
      "grad_norm": 2.663088798522949,
      "learning_rate": 4.700807357667953e-06,
      "epoch": 77.44610281923715
    },
    {
      "loss": 2.5821,
      "grad_norm": 2.777245044708252,
      "learning_rate": 4.669284994952499e-06,
      "epoch": 77.5290215588723
    },
    {
      "loss": 2.5591,
      "grad_norm": 2.7685952186584473,
      "learning_rate": 4.637836455631943e-06,
      "epoch": 77.61194029850746
    },
    {
      "loss": 2.5536,
      "grad_norm": 2.698882818222046,
      "learning_rate": 4.606462175227207e-06,
      "epoch": 77.69485903814262
    },
    {
      "loss": 2.5489,
      "grad_norm": 2.7369518280029297,
      "learning_rate": 4.5751625882308335e-06,
      "epoch": 77.77777777777777
    },
    {
      "loss": 2.5621,
      "grad_norm": 2.6725916862487793,
      "learning_rate": 4.5439381281009494e-06,
      "epoch": 77.86069651741293
    },
    {
      "loss": 2.5577,
      "grad_norm": 2.7050423622131348,
      "learning_rate": 4.512789227255285e-06,
      "epoch": 77.9436152570481
    },
    {
      "loss": 2.566,
      "grad_norm": 2.7364232540130615,
      "learning_rate": 4.481716317065163e-06,
      "epoch": 78.02653399668326
    },
    {
      "loss": 2.5294,
      "grad_norm": 2.7243666648864746,
      "learning_rate": 4.450719827849539e-06,
      "epoch": 78.1094527363184
    },
    {
      "loss": 2.5621,
      "grad_norm": 2.665127754211426,
      "learning_rate": 4.419800188869048e-06,
      "epoch": 78.19237147595356
    },
    {
      "loss": 2.549,
      "grad_norm": 2.572354555130005,
      "learning_rate": 4.388957828320032e-06,
      "epoch": 78.27529021558873
    },
    {
      "loss": 2.5448,
      "grad_norm": 2.714120388031006,
      "learning_rate": 4.358193173328642e-06,
      "epoch": 78.35820895522389
    },
    {
      "loss": 2.5431,
      "grad_norm": 2.721668243408203,
      "learning_rate": 4.3275066499449105e-06,
      "epoch": 78.44112769485903
    },
    {
      "loss": 2.5318,
      "grad_norm": 2.7191779613494873,
      "learning_rate": 4.296898683136836e-06,
      "epoch": 78.5240464344942
    },
    {
      "loss": 2.5685,
      "grad_norm": 2.78904390335083,
      "learning_rate": 4.26636969678453e-06,
      "epoch": 78.60696517412936
    },
    {
      "loss": 2.5404,
      "grad_norm": 2.690495729446411,
      "learning_rate": 4.235920113674314e-06,
      "epoch": 78.6898839137645
    },
    {
      "loss": 2.5563,
      "grad_norm": 2.7803003787994385,
      "learning_rate": 4.2055503554928805e-06,
      "epoch": 78.77280265339967
    },
    {
      "loss": 2.5465,
      "grad_norm": 2.699474334716797,
      "learning_rate": 4.175260842821462e-06,
      "epoch": 78.85572139303483
    },
    {
      "loss": 2.5696,
      "grad_norm": 2.7459053993225098,
      "learning_rate": 4.145051995129984e-06,
      "epoch": 78.93864013266999
    },
    {
      "loss": 2.5346,
      "grad_norm": 2.601768732070923,
      "learning_rate": 4.114924230771279e-06,
      "epoch": 79.02155887230514
    },
    {
      "loss": 2.5511,
      "grad_norm": 2.708120346069336,
      "learning_rate": 4.08487796697527e-06,
      "epoch": 79.1044776119403
    },
    {
      "loss": 2.5794,
      "grad_norm": 2.8581981658935547,
      "learning_rate": 4.054913619843215e-06,
      "epoch": 79.18739635157546
    },
    {
      "loss": 2.5409,
      "grad_norm": 2.7884347438812256,
      "learning_rate": 4.025031604341932e-06,
      "epoch": 79.27031509121062
    },
    {
      "loss": 2.5469,
      "grad_norm": 2.8024725914001465,
      "learning_rate": 3.9952323342980456e-06,
      "epoch": 79.35323383084577
    },
    {
      "loss": 2.5527,
      "grad_norm": 2.6730148792266846,
      "learning_rate": 3.965516222392274e-06,
      "epoch": 79.43615257048093
    },
    {
      "loss": 2.5361,
      "grad_norm": 2.762082099914551,
      "learning_rate": 3.935883680153706e-06,
      "epoch": 79.51907131011609
    },
    {
      "loss": 2.5317,
      "grad_norm": 2.7266080379486084,
      "learning_rate": 3.906335117954087e-06,
      "epoch": 79.60199004975124
    },
    {
      "loss": 2.5613,
      "grad_norm": 2.79699969291687,
      "learning_rate": 3.876870945002166e-06,
      "epoch": 79.6849087893864
    },
    {
      "loss": 2.5662,
      "grad_norm": 2.7768912315368652,
      "learning_rate": 3.847491569337997e-06,
      "epoch": 79.76782752902156
    },
    {
      "loss": 2.5291,
      "grad_norm": 2.8113229274749756,
      "learning_rate": 3.8181973978273165e-06,
      "epoch": 79.85074626865672
    },
    {
      "loss": 2.5424,
      "grad_norm": 2.6907992362976074,
      "learning_rate": 3.7889888361558856e-06,
      "epoch": 79.93366500829187
    },
    {
      "loss": 2.5224,
      "grad_norm": 2.682138204574585,
      "learning_rate": 3.7598662888238813e-06,
      "epoch": 80.01658374792703
    },
    {
      "loss": 2.5307,
      "grad_norm": 2.657093048095703,
      "learning_rate": 3.730830159140302e-06,
      "epoch": 80.09950248756219
    },
    {
      "loss": 2.5392,
      "grad_norm": 2.6088106632232666,
      "learning_rate": 3.7018808492173753e-06,
      "epoch": 80.18242122719735
    },
    {
      "loss": 2.5442,
      "grad_norm": 2.7841274738311768,
      "learning_rate": 3.6730187599649804e-06,
      "epoch": 80.2653399668325
    },
    {
      "loss": 2.5504,
      "grad_norm": 2.687060832977295,
      "learning_rate": 3.6442442910851163e-06,
      "epoch": 80.34825870646766
    },
    {
      "loss": 2.5605,
      "grad_norm": 2.7705507278442383,
      "learning_rate": 3.6155578410663418e-06,
      "epoch": 80.43117744610282
    },
    {
      "loss": 2.5844,
      "grad_norm": 2.646543264389038,
      "learning_rate": 3.5869598071782828e-06,
      "epoch": 80.51409618573798
    },
    {
      "loss": 2.5446,
      "grad_norm": 2.802816152572632,
      "learning_rate": 3.5584505854661143e-06,
      "epoch": 80.59701492537313
    },
    {
      "loss": 2.5889,
      "grad_norm": 2.9084763526916504,
      "learning_rate": 3.530030570745072e-06,
      "epoch": 80.67993366500829
    },
    {
      "loss": 2.5467,
      "grad_norm": 2.7459208965301514,
      "learning_rate": 3.5017001565950047e-06,
      "epoch": 80.76285240464345
    },
    {
      "loss": 2.5729,
      "grad_norm": 2.755321979522705,
      "learning_rate": 3.473459735354897e-06,
      "epoch": 80.8457711442786
    },
    {
      "loss": 2.5173,
      "grad_norm": 2.8042330741882324,
      "learning_rate": 3.445309698117465e-06,
      "epoch": 80.92868988391376
    },
    {
      "loss": 2.5228,
      "grad_norm": 2.6049578189849854,
      "learning_rate": 3.41725043472371e-06,
      "epoch": 81.01160862354892
    },
    {
      "loss": 2.5195,
      "grad_norm": 2.6719131469726562,
      "learning_rate": 3.38928233375755e-06,
      "epoch": 81.09452736318408
    },
    {
      "loss": 2.5123,
      "grad_norm": 2.719801902770996,
      "learning_rate": 3.361405782540408e-06,
      "epoch": 81.17744610281923
    },
    {
      "loss": 2.5303,
      "grad_norm": 2.804353713989258,
      "learning_rate": 3.333621167125878e-06,
      "epoch": 81.2603648424544
    },
    {
      "loss": 2.532,
      "grad_norm": 2.832200527191162,
      "learning_rate": 3.3059288722943496e-06,
      "epoch": 81.34328358208955
    },
    {
      "loss": 2.5189,
      "grad_norm": 2.6917812824249268,
      "learning_rate": 3.2783292815477095e-06,
      "epoch": 81.42620232172472
    },
    {
      "loss": 2.5408,
      "grad_norm": 2.7008116245269775,
      "learning_rate": 3.250822777103998e-06,
      "epoch": 81.50912106135986
    },
    {
      "loss": 2.5327,
      "grad_norm": 2.7016794681549072,
      "learning_rate": 3.223409739892145e-06,
      "epoch": 81.59203980099502
    }
  ],
  "eval_logs": [
    {
      "eval_loss": 10.879128456115723,
      "eval_runtime": 46.8108,
      "eval_samples_per_second": 88.206,
      "eval_steps_per_second": 5.533,
      "epoch": 0.16583747927031509
    },
    {
      "eval_loss": 10.635735511779785,
      "eval_runtime": 45.7579,
      "eval_samples_per_second": 90.236,
      "eval_steps_per_second": 5.66,
      "epoch": 0.33167495854063017
    },
    {
      "eval_loss": 10.325429916381836,
      "eval_runtime": 45.9125,
      "eval_samples_per_second": 89.932,
      "eval_steps_per_second": 5.641,
      "epoch": 0.4975124378109453
    },
    {
      "eval_loss": 10.083735466003418,
      "eval_runtime": 45.5034,
      "eval_samples_per_second": 90.74,
      "eval_steps_per_second": 5.692,
      "epoch": 0.6633499170812603
    },
    {
      "eval_loss": 9.907322883605957,
      "eval_runtime": 45.0895,
      "eval_samples_per_second": 91.574,
      "eval_steps_per_second": 5.744,
      "epoch": 0.8291873963515755
    },
    {
      "eval_loss": 9.785597801208496,
      "eval_runtime": 45.1287,
      "eval_samples_per_second": 91.494,
      "eval_steps_per_second": 5.739,
      "epoch": 0.9950248756218906
    },
    {
      "eval_loss": 9.686332702636719,
      "eval_runtime": 45.1166,
      "eval_samples_per_second": 91.519,
      "eval_steps_per_second": 5.741,
      "epoch": 1.1608623548922057
    },
    {
      "eval_loss": 9.582322120666504,
      "eval_runtime": 44.9886,
      "eval_samples_per_second": 91.779,
      "eval_steps_per_second": 5.757,
      "epoch": 1.3266998341625207
    },
    {
      "eval_loss": 9.494952201843262,
      "eval_runtime": 45.0453,
      "eval_samples_per_second": 91.663,
      "eval_steps_per_second": 5.75,
      "epoch": 1.4925373134328357
    },
    {
      "eval_loss": 9.412392616271973,
      "eval_runtime": 46.4866,
      "eval_samples_per_second": 88.821,
      "eval_steps_per_second": 5.571,
      "epoch": 1.658374792703151
    },
    {
      "eval_loss": 9.317459106445312,
      "eval_runtime": 46.2039,
      "eval_samples_per_second": 89.365,
      "eval_steps_per_second": 5.606,
      "epoch": 1.8242122719734661
    },
    {
      "eval_loss": 9.217268943786621,
      "eval_runtime": 45.2414,
      "eval_samples_per_second": 91.266,
      "eval_steps_per_second": 5.725,
      "epoch": 1.9900497512437811
    },
    {
      "eval_loss": 9.122360229492188,
      "eval_runtime": 45.041,
      "eval_samples_per_second": 91.672,
      "eval_steps_per_second": 5.75,
      "epoch": 2.155887230514096
    },
    {
      "eval_loss": 9.00854778289795,
      "eval_runtime": 47.3965,
      "eval_samples_per_second": 87.116,
      "eval_steps_per_second": 5.465,
      "epoch": 2.3217247097844114
    },
    {
      "eval_loss": 8.909120559692383,
      "eval_runtime": 45.4551,
      "eval_samples_per_second": 90.837,
      "eval_steps_per_second": 5.698,
      "epoch": 2.487562189054726
    },
    {
      "eval_loss": 8.78382682800293,
      "eval_runtime": 46.2376,
      "eval_samples_per_second": 89.3,
      "eval_steps_per_second": 5.602,
      "epoch": 2.6533996683250414
    },
    {
      "eval_loss": 8.682120323181152,
      "eval_runtime": 46.0074,
      "eval_samples_per_second": 89.746,
      "eval_steps_per_second": 5.63,
      "epoch": 2.8192371475953566
    },
    {
      "eval_loss": 8.56091594696045,
      "eval_runtime": 48.3588,
      "eval_samples_per_second": 85.383,
      "eval_steps_per_second": 5.356,
      "epoch": 2.9850746268656714
    },
    {
      "eval_loss": 8.438775062561035,
      "eval_runtime": 47.1319,
      "eval_samples_per_second": 87.605,
      "eval_steps_per_second": 5.495,
      "epoch": 3.1509121061359866
    },
    {
      "eval_loss": 8.322005271911621,
      "eval_runtime": 46.9622,
      "eval_samples_per_second": 87.922,
      "eval_steps_per_second": 5.515,
      "epoch": 3.316749585406302
    },
    {
      "eval_loss": 8.196805953979492,
      "eval_runtime": 46.6291,
      "eval_samples_per_second": 88.55,
      "eval_steps_per_second": 5.554,
      "epoch": 3.482587064676617
    },
    {
      "eval_loss": 8.092820167541504,
      "eval_runtime": 46.9412,
      "eval_samples_per_second": 87.961,
      "eval_steps_per_second": 5.518,
      "epoch": 3.6484245439469323
    },
    {
      "eval_loss": 7.970271110534668,
      "eval_runtime": 47.127,
      "eval_samples_per_second": 87.614,
      "eval_steps_per_second": 5.496,
      "epoch": 3.814262023217247
    },
    {
      "eval_loss": 7.851957321166992,
      "eval_runtime": 45.8491,
      "eval_samples_per_second": 90.056,
      "eval_steps_per_second": 5.649,
      "epoch": 3.9800995024875623
    },
    {
      "eval_loss": 7.731200218200684,
      "eval_runtime": 44.939,
      "eval_samples_per_second": 91.88,
      "eval_steps_per_second": 5.763,
      "epoch": 4.1459369817578775
    },
    {
      "eval_loss": 7.646726131439209,
      "eval_runtime": 45.1278,
      "eval_samples_per_second": 91.496,
      "eval_steps_per_second": 5.739,
      "epoch": 4.311774461028192
    },
    {
      "eval_loss": 7.554970741271973,
      "eval_runtime": 45.0108,
      "eval_samples_per_second": 91.734,
      "eval_steps_per_second": 5.754,
      "epoch": 4.477611940298507
    },
    {
      "eval_loss": 7.4625563621521,
      "eval_runtime": 44.8757,
      "eval_samples_per_second": 92.01,
      "eval_steps_per_second": 5.771,
      "epoch": 4.643449419568823
    },
    {
      "eval_loss": 7.393415927886963,
      "eval_runtime": 45.02,
      "eval_samples_per_second": 91.715,
      "eval_steps_per_second": 5.753,
      "epoch": 4.8092868988391375
    },
    {
      "eval_loss": 7.3229217529296875,
      "eval_runtime": 44.9562,
      "eval_samples_per_second": 91.845,
      "eval_steps_per_second": 5.761,
      "epoch": 4.975124378109452
    },
    {
      "eval_loss": 7.268559455871582,
      "eval_runtime": 44.8761,
      "eval_samples_per_second": 92.009,
      "eval_steps_per_second": 5.771,
      "epoch": 5.140961857379768
    },
    {
      "eval_loss": 7.210257053375244,
      "eval_runtime": 44.9207,
      "eval_samples_per_second": 91.917,
      "eval_steps_per_second": 5.766,
      "epoch": 5.306799336650083
    },
    {
      "eval_loss": 7.1743483543396,
      "eval_runtime": 44.8792,
      "eval_samples_per_second": 92.003,
      "eval_steps_per_second": 5.771,
      "epoch": 5.472636815920398
    },
    {
      "eval_loss": 7.118061542510986,
      "eval_runtime": 44.9338,
      "eval_samples_per_second": 91.891,
      "eval_steps_per_second": 5.764,
      "epoch": 5.638474295190713
    },
    {
      "eval_loss": 7.089191436767578,
      "eval_runtime": 44.8901,
      "eval_samples_per_second": 91.98,
      "eval_steps_per_second": 5.77,
      "epoch": 5.804311774461028
    },
    {
      "eval_loss": 7.05344820022583,
      "eval_runtime": 44.8692,
      "eval_samples_per_second": 92.023,
      "eval_steps_per_second": 5.772,
      "epoch": 5.970149253731344
    },
    {
      "eval_loss": 7.0124711990356445,
      "eval_runtime": 44.8168,
      "eval_samples_per_second": 92.131,
      "eval_steps_per_second": 5.779,
      "epoch": 6.135986733001658
    },
    {
      "eval_loss": 6.956892967224121,
      "eval_runtime": 44.8821,
      "eval_samples_per_second": 91.997,
      "eval_steps_per_second": 5.771,
      "epoch": 6.301824212271973
    },
    {
      "eval_loss": 6.893344402313232,
      "eval_runtime": 44.8771,
      "eval_samples_per_second": 92.007,
      "eval_steps_per_second": 5.771,
      "epoch": 6.467661691542289
    },
    {
      "eval_loss": 6.864377498626709,
      "eval_runtime": 44.8818,
      "eval_samples_per_second": 91.997,
      "eval_steps_per_second": 5.771,
      "epoch": 6.633499170812604
    },
    {
      "eval_loss": 6.800537586212158,
      "eval_runtime": 44.845,
      "eval_samples_per_second": 92.073,
      "eval_steps_per_second": 5.775,
      "epoch": 6.799336650082918
    },
    {
      "eval_loss": 6.749076843261719,
      "eval_runtime": 44.9992,
      "eval_samples_per_second": 91.757,
      "eval_steps_per_second": 5.756,
      "epoch": 6.965174129353234
    },
    {
      "eval_loss": 6.702304840087891,
      "eval_runtime": 44.9104,
      "eval_samples_per_second": 91.939,
      "eval_steps_per_second": 5.767,
      "epoch": 7.131011608623549
    },
    {
      "eval_loss": 6.638909816741943,
      "eval_runtime": 44.8138,
      "eval_samples_per_second": 92.137,
      "eval_steps_per_second": 5.779,
      "epoch": 7.296849087893864
    },
    {
      "eval_loss": 6.585209846496582,
      "eval_runtime": 44.9596,
      "eval_samples_per_second": 91.838,
      "eval_steps_per_second": 5.761,
      "epoch": 7.462686567164179
    },
    {
      "eval_loss": 6.540661334991455,
      "eval_runtime": 44.8823,
      "eval_samples_per_second": 91.996,
      "eval_steps_per_second": 5.771,
      "epoch": 7.628524046434494
    },
    {
      "eval_loss": 6.469175338745117,
      "eval_runtime": 44.941,
      "eval_samples_per_second": 91.876,
      "eval_steps_per_second": 5.763,
      "epoch": 7.79436152570481
    },
    {
      "eval_loss": 6.448359966278076,
      "eval_runtime": 44.9463,
      "eval_samples_per_second": 91.865,
      "eval_steps_per_second": 5.762,
      "epoch": 7.960199004975125
    },
    {
      "eval_loss": 6.388474464416504,
      "eval_runtime": 44.7366,
      "eval_samples_per_second": 92.296,
      "eval_steps_per_second": 5.789,
      "epoch": 8.12603648424544
    },
    {
      "eval_loss": 6.346341133117676,
      "eval_runtime": 44.9132,
      "eval_samples_per_second": 91.933,
      "eval_steps_per_second": 5.767,
      "epoch": 8.291873963515755
    },
    {
      "eval_loss": 6.304312705993652,
      "eval_runtime": 44.9788,
      "eval_samples_per_second": 91.799,
      "eval_steps_per_second": 5.758,
      "epoch": 8.457711442786069
    },
    {
      "eval_loss": 6.261873245239258,
      "eval_runtime": 44.8792,
      "eval_samples_per_second": 92.002,
      "eval_steps_per_second": 5.771,
      "epoch": 8.623548922056385
    },
    {
      "eval_loss": 6.2202324867248535,
      "eval_runtime": 44.916,
      "eval_samples_per_second": 91.927,
      "eval_steps_per_second": 5.766,
      "epoch": 8.7893864013267
    },
    {
      "eval_loss": 6.189189434051514,
      "eval_runtime": 44.864,
      "eval_samples_per_second": 92.034,
      "eval_steps_per_second": 5.773,
      "epoch": 8.955223880597014
    },
    {
      "eval_loss": 6.157534122467041,
      "eval_runtime": 44.8457,
      "eval_samples_per_second": 92.071,
      "eval_steps_per_second": 5.775,
      "epoch": 9.12106135986733
    },
    {
      "eval_loss": 6.109443187713623,
      "eval_runtime": 44.9459,
      "eval_samples_per_second": 91.866,
      "eval_steps_per_second": 5.762,
      "epoch": 9.286898839137645
    },
    {
      "eval_loss": 6.08407735824585,
      "eval_runtime": 44.8089,
      "eval_samples_per_second": 92.147,
      "eval_steps_per_second": 5.78,
      "epoch": 9.45273631840796
    },
    {
      "eval_loss": 6.051288604736328,
      "eval_runtime": 44.8004,
      "eval_samples_per_second": 92.164,
      "eval_steps_per_second": 5.781,
      "epoch": 9.618573797678275
    },
    {
      "eval_loss": 6.00562047958374,
      "eval_runtime": 44.9711,
      "eval_samples_per_second": 91.815,
      "eval_steps_per_second": 5.759,
      "epoch": 9.78441127694859
    },
    {
      "eval_loss": 5.955612659454346,
      "eval_runtime": 44.961,
      "eval_samples_per_second": 91.835,
      "eval_steps_per_second": 5.761,
      "epoch": 9.950248756218905
    },
    {
      "eval_loss": 5.942311763763428,
      "eval_runtime": 45.0153,
      "eval_samples_per_second": 91.724,
      "eval_steps_per_second": 5.754,
      "epoch": 10.11608623548922
    },
    {
      "eval_loss": 5.916191101074219,
      "eval_runtime": 44.8846,
      "eval_samples_per_second": 91.992,
      "eval_steps_per_second": 5.77,
      "epoch": 10.281923714759536
    },
    {
      "eval_loss": 5.8641157150268555,
      "eval_runtime": 44.9026,
      "eval_samples_per_second": 91.955,
      "eval_steps_per_second": 5.768,
      "epoch": 10.447761194029852
    },
    {
      "eval_loss": 5.8277058601379395,
      "eval_runtime": 44.8313,
      "eval_samples_per_second": 92.101,
      "eval_steps_per_second": 5.777,
      "epoch": 10.613598673300165
    },
    {
      "eval_loss": 5.816876411437988,
      "eval_runtime": 44.9551,
      "eval_samples_per_second": 91.847,
      "eval_steps_per_second": 5.761,
      "epoch": 10.779436152570481
    },
    {
      "eval_loss": 5.7909932136535645,
      "eval_runtime": 44.8658,
      "eval_samples_per_second": 92.03,
      "eval_steps_per_second": 5.773,
      "epoch": 10.945273631840797
    },
    {
      "eval_loss": 5.760439872741699,
      "eval_runtime": 44.8846,
      "eval_samples_per_second": 91.991,
      "eval_steps_per_second": 5.77,
      "epoch": 11.11111111111111
    },
    {
      "eval_loss": 5.711305141448975,
      "eval_runtime": 44.9097,
      "eval_samples_per_second": 91.94,
      "eval_steps_per_second": 5.767,
      "epoch": 11.276948590381426
    },
    {
      "eval_loss": 5.695173263549805,
      "eval_runtime": 44.9375,
      "eval_samples_per_second": 91.883,
      "eval_steps_per_second": 5.764,
      "epoch": 11.442786069651742
    },
    {
      "eval_loss": 5.657410144805908,
      "eval_runtime": 44.8705,
      "eval_samples_per_second": 92.02,
      "eval_steps_per_second": 5.772,
      "epoch": 11.608623548922056
    },
    {
      "eval_loss": 5.629942893981934,
      "eval_runtime": 44.8995,
      "eval_samples_per_second": 91.961,
      "eval_steps_per_second": 5.768,
      "epoch": 11.774461028192372
    },
    {
      "eval_loss": 5.583545684814453,
      "eval_runtime": 44.8834,
      "eval_samples_per_second": 91.994,
      "eval_steps_per_second": 5.771,
      "epoch": 11.940298507462687
    },
    {
      "eval_loss": 5.568991184234619,
      "eval_runtime": 44.8735,
      "eval_samples_per_second": 92.014,
      "eval_steps_per_second": 5.772,
      "epoch": 12.106135986733001
    },
    {
      "eval_loss": 5.542919635772705,
      "eval_runtime": 44.8804,
      "eval_samples_per_second": 92.0,
      "eval_steps_per_second": 5.771,
      "epoch": 12.271973466003317
    },
    {
      "eval_loss": 5.517384052276611,
      "eval_runtime": 44.6778,
      "eval_samples_per_second": 92.417,
      "eval_steps_per_second": 5.797,
      "epoch": 12.437810945273633
    },
    {
      "eval_loss": 5.4776506423950195,
      "eval_runtime": 44.8763,
      "eval_samples_per_second": 92.008,
      "eval_steps_per_second": 5.771,
      "epoch": 12.603648424543946
    },
    {
      "eval_loss": 5.460651874542236,
      "eval_runtime": 44.8081,
      "eval_samples_per_second": 92.149,
      "eval_steps_per_second": 5.78,
      "epoch": 12.769485903814262
    },
    {
      "eval_loss": 5.443472385406494,
      "eval_runtime": 44.8909,
      "eval_samples_per_second": 91.979,
      "eval_steps_per_second": 5.77,
      "epoch": 12.935323383084578
    },
    {
      "eval_loss": 5.423609256744385,
      "eval_runtime": 44.9998,
      "eval_samples_per_second": 91.756,
      "eval_steps_per_second": 5.756,
      "epoch": 13.101160862354892
    },
    {
      "eval_loss": 5.38193941116333,
      "eval_runtime": 44.901,
      "eval_samples_per_second": 91.958,
      "eval_steps_per_second": 5.768,
      "epoch": 13.266998341625207
    },
    {
      "eval_loss": 5.357533931732178,
      "eval_runtime": 44.8702,
      "eval_samples_per_second": 92.021,
      "eval_steps_per_second": 5.772,
      "epoch": 13.432835820895523
    },
    {
      "eval_loss": 5.334133625030518,
      "eval_runtime": 44.9247,
      "eval_samples_per_second": 91.909,
      "eval_steps_per_second": 5.765,
      "epoch": 13.598673300165837
    },
    {
      "eval_loss": 5.293779373168945,
      "eval_runtime": 44.929,
      "eval_samples_per_second": 91.901,
      "eval_steps_per_second": 5.765,
      "epoch": 13.764510779436153
    },
    {
      "eval_loss": 5.283754348754883,
      "eval_runtime": 44.893,
      "eval_samples_per_second": 91.974,
      "eval_steps_per_second": 5.769,
      "epoch": 13.930348258706468
    },
    {
      "eval_loss": 5.2625651359558105,
      "eval_runtime": 44.9296,
      "eval_samples_per_second": 91.899,
      "eval_steps_per_second": 5.765,
      "epoch": 14.096185737976782
    },
    {
      "eval_loss": 5.207681655883789,
      "eval_runtime": 44.9592,
      "eval_samples_per_second": 91.839,
      "eval_steps_per_second": 5.761,
      "epoch": 14.262023217247098
    },
    {
      "eval_loss": 5.209364891052246,
      "eval_runtime": 44.8933,
      "eval_samples_per_second": 91.974,
      "eval_steps_per_second": 5.769,
      "epoch": 14.427860696517413
    },
    {
      "eval_loss": 5.183217525482178,
      "eval_runtime": 44.8478,
      "eval_samples_per_second": 92.067,
      "eval_steps_per_second": 5.775,
      "epoch": 14.593698175787727
    },
    {
      "eval_loss": 5.160017013549805,
      "eval_runtime": 44.8178,
      "eval_samples_per_second": 92.129,
      "eval_steps_per_second": 5.779,
      "epoch": 14.759535655058043
    },
    {
      "eval_loss": 5.144285678863525,
      "eval_runtime": 44.8485,
      "eval_samples_per_second": 92.066,
      "eval_steps_per_second": 5.775,
      "epoch": 14.925373134328359
    },
    {
      "eval_loss": 5.106461524963379,
      "eval_runtime": 44.8658,
      "eval_samples_per_second": 92.03,
      "eval_steps_per_second": 5.773,
      "epoch": 15.091210613598673
    },
    {
      "eval_loss": 5.09509801864624,
      "eval_runtime": 44.831,
      "eval_samples_per_second": 92.101,
      "eval_steps_per_second": 5.777,
      "epoch": 15.257048092868988
    },
    {
      "eval_loss": 5.080765247344971,
      "eval_runtime": 44.8553,
      "eval_samples_per_second": 92.051,
      "eval_steps_per_second": 5.774,
      "epoch": 15.422885572139304
    },
    {
      "eval_loss": 5.038093090057373,
      "eval_runtime": 44.7793,
      "eval_samples_per_second": 92.208,
      "eval_steps_per_second": 5.784,
      "epoch": 15.588723051409618
    },
    {
      "eval_loss": 5.012759685516357,
      "eval_runtime": 44.776,
      "eval_samples_per_second": 92.215,
      "eval_steps_per_second": 5.784,
      "epoch": 15.754560530679933
    },
    {
      "eval_loss": 5.004238605499268,
      "eval_runtime": 44.861,
      "eval_samples_per_second": 92.04,
      "eval_steps_per_second": 5.773,
      "epoch": 15.92039800995025
    },
    {
      "eval_loss": 4.994377613067627,
      "eval_runtime": 44.8697,
      "eval_samples_per_second": 92.022,
      "eval_steps_per_second": 5.772,
      "epoch": 16.086235489220563
    },
    {
      "eval_loss": 4.970526695251465,
      "eval_runtime": 44.8541,
      "eval_samples_per_second": 92.054,
      "eval_steps_per_second": 5.774,
      "epoch": 16.25207296849088
    },
    {
      "eval_loss": 4.925838470458984,
      "eval_runtime": 44.9371,
      "eval_samples_per_second": 91.884,
      "eval_steps_per_second": 5.764,
      "epoch": 16.417910447761194
    },
    {
      "eval_loss": 4.9372735023498535,
      "eval_runtime": 44.8472,
      "eval_samples_per_second": 92.068,
      "eval_steps_per_second": 5.775,
      "epoch": 16.58374792703151
    },
    {
      "eval_loss": 4.886546611785889,
      "eval_runtime": 44.8675,
      "eval_samples_per_second": 92.027,
      "eval_steps_per_second": 5.773,
      "epoch": 16.749585406301826
    },
    {
      "eval_loss": 4.8834357261657715,
      "eval_runtime": 44.8501,
      "eval_samples_per_second": 92.062,
      "eval_steps_per_second": 5.775,
      "epoch": 16.915422885572138
    },
    {
      "eval_loss": 4.848888397216797,
      "eval_runtime": 44.7834,
      "eval_samples_per_second": 92.199,
      "eval_steps_per_second": 5.783,
      "epoch": 17.081260364842453
    },
    {
      "eval_loss": 4.848678112030029,
      "eval_runtime": 44.7996,
      "eval_samples_per_second": 92.166,
      "eval_steps_per_second": 5.781,
      "epoch": 17.24709784411277
    },
    {
      "eval_loss": 4.827264308929443,
      "eval_runtime": 44.8413,
      "eval_samples_per_second": 92.08,
      "eval_steps_per_second": 5.776,
      "epoch": 17.412935323383085
    },
    {
      "eval_loss": 4.796995639801025,
      "eval_runtime": 44.8743,
      "eval_samples_per_second": 92.013,
      "eval_steps_per_second": 5.772,
      "epoch": 17.5787728026534
    },
    {
      "eval_loss": 4.76686429977417,
      "eval_runtime": 44.765,
      "eval_samples_per_second": 92.237,
      "eval_steps_per_second": 5.786,
      "epoch": 17.744610281923716
    },
    {
      "eval_loss": 4.754540920257568,
      "eval_runtime": 44.937,
      "eval_samples_per_second": 91.884,
      "eval_steps_per_second": 5.764,
      "epoch": 17.91044776119403
    },
    {
      "eval_loss": 4.734476089477539,
      "eval_runtime": 44.8357,
      "eval_samples_per_second": 92.092,
      "eval_steps_per_second": 5.777,
      "epoch": 18.076285240464344
    },
    {
      "eval_loss": 4.730249404907227,
      "eval_runtime": 44.7942,
      "eval_samples_per_second": 92.177,
      "eval_steps_per_second": 5.782,
      "epoch": 18.24212271973466
    },
    {
      "eval_loss": 4.713449954986572,
      "eval_runtime": 44.8743,
      "eval_samples_per_second": 92.013,
      "eval_steps_per_second": 5.772,
      "epoch": 18.407960199004975
    },
    {
      "eval_loss": 4.6951093673706055,
      "eval_runtime": 44.6531,
      "eval_samples_per_second": 92.468,
      "eval_steps_per_second": 5.8,
      "epoch": 18.57379767827529
    },
    {
      "eval_loss": 4.664693355560303,
      "eval_runtime": 44.8778,
      "eval_samples_per_second": 92.005,
      "eval_steps_per_second": 5.771,
      "epoch": 18.739635157545607
    },
    {
      "eval_loss": 4.652712345123291,
      "eval_runtime": 44.8727,
      "eval_samples_per_second": 92.016,
      "eval_steps_per_second": 5.772,
      "epoch": 18.90547263681592
    },
    {
      "eval_loss": 4.634620189666748,
      "eval_runtime": 44.8834,
      "eval_samples_per_second": 91.994,
      "eval_steps_per_second": 5.771,
      "epoch": 19.071310116086234
    },
    {
      "eval_loss": 4.61090612411499,
      "eval_runtime": 44.8072,
      "eval_samples_per_second": 92.15,
      "eval_steps_per_second": 5.78,
      "epoch": 19.23714759535655
    },
    {
      "eval_loss": 4.596757888793945,
      "eval_runtime": 44.8585,
      "eval_samples_per_second": 92.045,
      "eval_steps_per_second": 5.774,
      "epoch": 19.402985074626866
    },
    {
      "eval_loss": 4.58631706237793,
      "eval_runtime": 44.874,
      "eval_samples_per_second": 92.013,
      "eval_steps_per_second": 5.772,
      "epoch": 19.56882255389718
    },
    {
      "eval_loss": 4.557436943054199,
      "eval_runtime": 44.8597,
      "eval_samples_per_second": 92.043,
      "eval_steps_per_second": 5.774,
      "epoch": 19.734660033167497
    },
    {
      "eval_loss": 4.532100677490234,
      "eval_runtime": 44.7839,
      "eval_samples_per_second": 92.198,
      "eval_steps_per_second": 5.783,
      "epoch": 19.90049751243781
    },
    {
      "eval_loss": 4.524337291717529,
      "eval_runtime": 44.9724,
      "eval_samples_per_second": 91.812,
      "eval_steps_per_second": 5.759,
      "epoch": 20.066334991708125
    },
    {
      "eval_loss": 4.499527931213379,
      "eval_runtime": 44.8701,
      "eval_samples_per_second": 92.021,
      "eval_steps_per_second": 5.772,
      "epoch": 20.23217247097844
    },
    {
      "eval_loss": 4.493046760559082,
      "eval_runtime": 44.8124,
      "eval_samples_per_second": 92.14,
      "eval_steps_per_second": 5.78,
      "epoch": 20.398009950248756
    },
    {
      "eval_loss": 4.4688825607299805,
      "eval_runtime": 44.8619,
      "eval_samples_per_second": 92.038,
      "eval_steps_per_second": 5.773,
      "epoch": 20.563847429519072
    },
    {
      "eval_loss": 4.452094078063965,
      "eval_runtime": 44.8402,
      "eval_samples_per_second": 92.082,
      "eval_steps_per_second": 5.776,
      "epoch": 20.729684908789388
    },
    {
      "eval_loss": 4.441656112670898,
      "eval_runtime": 44.9335,
      "eval_samples_per_second": 91.891,
      "eval_steps_per_second": 5.764,
      "epoch": 20.895522388059703
    },
    {
      "eval_loss": 4.434301376342773,
      "eval_runtime": 44.855,
      "eval_samples_per_second": 92.052,
      "eval_steps_per_second": 5.774,
      "epoch": 21.061359867330015
    },
    {
      "eval_loss": 4.410249710083008,
      "eval_runtime": 44.8399,
      "eval_samples_per_second": 92.083,
      "eval_steps_per_second": 5.776,
      "epoch": 21.22719734660033
    },
    {
      "eval_loss": 4.389793395996094,
      "eval_runtime": 44.8741,
      "eval_samples_per_second": 92.013,
      "eval_steps_per_second": 5.772,
      "epoch": 21.393034825870647
    },
    {
      "eval_loss": 4.386097431182861,
      "eval_runtime": 44.9182,
      "eval_samples_per_second": 91.923,
      "eval_steps_per_second": 5.766,
      "epoch": 21.558872305140962
    },
    {
      "eval_loss": 4.369365215301514,
      "eval_runtime": 44.9298,
      "eval_samples_per_second": 91.899,
      "eval_steps_per_second": 5.765,
      "epoch": 21.724709784411278
    },
    {
      "eval_loss": 4.351761341094971,
      "eval_runtime": 44.9733,
      "eval_samples_per_second": 91.81,
      "eval_steps_per_second": 5.759,
      "epoch": 21.890547263681594
    },
    {
      "eval_loss": 4.321926593780518,
      "eval_runtime": 44.8499,
      "eval_samples_per_second": 92.063,
      "eval_steps_per_second": 5.775,
      "epoch": 22.056384742951906
    },
    {
      "eval_loss": 4.320221424102783,
      "eval_runtime": 44.7122,
      "eval_samples_per_second": 92.346,
      "eval_steps_per_second": 5.793,
      "epoch": 22.22222222222222
    },
    {
      "eval_loss": 4.302127361297607,
      "eval_runtime": 44.8248,
      "eval_samples_per_second": 92.114,
      "eval_steps_per_second": 5.778,
      "epoch": 22.388059701492537
    },
    {
      "eval_loss": 4.27600622177124,
      "eval_runtime": 44.7959,
      "eval_samples_per_second": 92.174,
      "eval_steps_per_second": 5.782,
      "epoch": 22.553897180762853
    },
    {
      "eval_loss": 4.26438570022583,
      "eval_runtime": 44.9437,
      "eval_samples_per_second": 91.871,
      "eval_steps_per_second": 5.763,
      "epoch": 22.71973466003317
    },
    {
      "eval_loss": 4.265533924102783,
      "eval_runtime": 44.8926,
      "eval_samples_per_second": 91.975,
      "eval_steps_per_second": 5.769,
      "epoch": 22.885572139303484
    },
    {
      "eval_loss": 4.254349231719971,
      "eval_runtime": 44.768,
      "eval_samples_per_second": 92.231,
      "eval_steps_per_second": 5.785,
      "epoch": 23.051409618573796
    },
    {
      "eval_loss": 4.220759391784668,
      "eval_runtime": 44.8601,
      "eval_samples_per_second": 92.042,
      "eval_steps_per_second": 5.774,
      "epoch": 23.217247097844112
    },
    {
      "eval_loss": 4.222668647766113,
      "eval_runtime": 44.7638,
      "eval_samples_per_second": 92.24,
      "eval_steps_per_second": 5.786,
      "epoch": 23.383084577114428
    },
    {
      "eval_loss": 4.185118675231934,
      "eval_runtime": 44.9008,
      "eval_samples_per_second": 91.958,
      "eval_steps_per_second": 5.768,
      "epoch": 23.548922056384743
    },
    {
      "eval_loss": 4.180415153503418,
      "eval_runtime": 44.9672,
      "eval_samples_per_second": 91.823,
      "eval_steps_per_second": 5.76,
      "epoch": 23.71475953565506
    },
    {
      "eval_loss": 4.173740863800049,
      "eval_runtime": 44.8814,
      "eval_samples_per_second": 91.998,
      "eval_steps_per_second": 5.771,
      "epoch": 23.880597014925375
    },
    {
      "eval_loss": 4.152820110321045,
      "eval_runtime": 44.7893,
      "eval_samples_per_second": 92.187,
      "eval_steps_per_second": 5.783,
      "epoch": 24.046434494195687
    },
    {
      "eval_loss": 4.138526916503906,
      "eval_runtime": 44.8747,
      "eval_samples_per_second": 92.012,
      "eval_steps_per_second": 5.772,
      "epoch": 24.212271973466002
    },
    {
      "eval_loss": 4.115542888641357,
      "eval_runtime": 44.8335,
      "eval_samples_per_second": 92.096,
      "eval_steps_per_second": 5.777,
      "epoch": 24.378109452736318
    },
    {
      "eval_loss": 4.125391483306885,
      "eval_runtime": 44.8082,
      "eval_samples_per_second": 92.148,
      "eval_steps_per_second": 5.78,
      "epoch": 24.543946932006634
    },
    {
      "eval_loss": 4.115341663360596,
      "eval_runtime": 44.7577,
      "eval_samples_per_second": 92.252,
      "eval_steps_per_second": 5.787,
      "epoch": 24.70978441127695
    },
    {
      "eval_loss": 4.07948637008667,
      "eval_runtime": 44.893,
      "eval_samples_per_second": 91.974,
      "eval_steps_per_second": 5.769,
      "epoch": 24.875621890547265
    },
    {
      "eval_loss": 4.091250896453857,
      "eval_runtime": 44.8089,
      "eval_samples_per_second": 92.147,
      "eval_steps_per_second": 5.78,
      "epoch": 25.041459369817577
    },
    {
      "eval_loss": 4.061549186706543,
      "eval_runtime": 44.8269,
      "eval_samples_per_second": 92.11,
      "eval_steps_per_second": 5.778,
      "epoch": 25.207296849087893
    },
    {
      "eval_loss": 4.060634613037109,
      "eval_runtime": 44.8367,
      "eval_samples_per_second": 92.09,
      "eval_steps_per_second": 5.777,
      "epoch": 25.37313432835821
    },
    {
      "eval_loss": 4.0503926277160645,
      "eval_runtime": 44.8088,
      "eval_samples_per_second": 92.147,
      "eval_steps_per_second": 5.78,
      "epoch": 25.538971807628524
    },
    {
      "eval_loss": 4.029573440551758,
      "eval_runtime": 45.3194,
      "eval_samples_per_second": 91.109,
      "eval_steps_per_second": 5.715,
      "epoch": 25.70480928689884
    },
    {
      "eval_loss": 4.021984577178955,
      "eval_runtime": 44.9542,
      "eval_samples_per_second": 91.849,
      "eval_steps_per_second": 5.761,
      "epoch": 25.870646766169155
    },
    {
      "eval_loss": 3.9922399520874023,
      "eval_runtime": 45.0276,
      "eval_samples_per_second": 91.699,
      "eval_steps_per_second": 5.752,
      "epoch": 26.036484245439468
    },
    {
      "eval_loss": 3.9869275093078613,
      "eval_runtime": 45.0365,
      "eval_samples_per_second": 91.681,
      "eval_steps_per_second": 5.751,
      "epoch": 26.202321724709783
    },
    {
      "eval_loss": 3.9877068996429443,
      "eval_runtime": 46.4577,
      "eval_samples_per_second": 88.877,
      "eval_steps_per_second": 5.575,
      "epoch": 26.3681592039801
    },
    {
      "eval_loss": 3.9678125381469727,
      "eval_runtime": 45.0091,
      "eval_samples_per_second": 91.737,
      "eval_steps_per_second": 5.754,
      "epoch": 26.533996683250415
    },
    {
      "eval_loss": 3.9659388065338135,
      "eval_runtime": 44.8424,
      "eval_samples_per_second": 92.078,
      "eval_steps_per_second": 5.776,
      "epoch": 26.69983416252073
    },
    {
      "eval_loss": 3.936940908432007,
      "eval_runtime": 44.7397,
      "eval_samples_per_second": 92.289,
      "eval_steps_per_second": 5.789,
      "epoch": 26.865671641791046
    },
    {
      "eval_loss": 3.933619737625122,
      "eval_runtime": 44.9501,
      "eval_samples_per_second": 91.857,
      "eval_steps_per_second": 5.762,
      "epoch": 27.03150912106136
    },
    {
      "eval_loss": 3.935817003250122,
      "eval_runtime": 44.8749,
      "eval_samples_per_second": 92.011,
      "eval_steps_per_second": 5.772,
      "epoch": 27.197346600331674
    },
    {
      "eval_loss": 3.910212993621826,
      "eval_runtime": 44.875,
      "eval_samples_per_second": 92.011,
      "eval_steps_per_second": 5.772,
      "epoch": 27.36318407960199
    },
    {
      "eval_loss": 3.905930280685425,
      "eval_runtime": 44.7404,
      "eval_samples_per_second": 92.288,
      "eval_steps_per_second": 5.789,
      "epoch": 27.529021558872305
    },
    {
      "eval_loss": 3.883333683013916,
      "eval_runtime": 44.9112,
      "eval_samples_per_second": 91.937,
      "eval_steps_per_second": 5.767,
      "epoch": 27.69485903814262
    },
    {
      "eval_loss": 3.872774839401245,
      "eval_runtime": 44.8641,
      "eval_samples_per_second": 92.033,
      "eval_steps_per_second": 5.773,
      "epoch": 27.860696517412936
    },
    {
      "eval_loss": 3.868746519088745,
      "eval_runtime": 44.7935,
      "eval_samples_per_second": 92.179,
      "eval_steps_per_second": 5.782,
      "epoch": 28.026533996683252
    },
    {
      "eval_loss": 3.8724300861358643,
      "eval_runtime": 44.9315,
      "eval_samples_per_second": 91.895,
      "eval_steps_per_second": 5.764,
      "epoch": 28.192371475953564
    },
    {
      "eval_loss": 3.8509654998779297,
      "eval_runtime": 44.7701,
      "eval_samples_per_second": 92.227,
      "eval_steps_per_second": 5.785,
      "epoch": 28.35820895522388
    },
    {
      "eval_loss": 3.8216724395751953,
      "eval_runtime": 44.8014,
      "eval_samples_per_second": 92.162,
      "eval_steps_per_second": 5.781,
      "epoch": 28.524046434494196
    },
    {
      "eval_loss": 3.8185088634490967,
      "eval_runtime": 44.8692,
      "eval_samples_per_second": 92.023,
      "eval_steps_per_second": 5.772,
      "epoch": 28.68988391376451
    },
    {
      "eval_loss": 3.822977066040039,
      "eval_runtime": 44.8996,
      "eval_samples_per_second": 91.961,
      "eval_steps_per_second": 5.768,
      "epoch": 28.855721393034827
    },
    {
      "eval_loss": 3.7919721603393555,
      "eval_runtime": 44.7781,
      "eval_samples_per_second": 92.21,
      "eval_steps_per_second": 5.784,
      "epoch": 29.021558872305143
    },
    {
      "eval_loss": 3.794698715209961,
      "eval_runtime": 44.9381,
      "eval_samples_per_second": 91.882,
      "eval_steps_per_second": 5.763,
      "epoch": 29.187396351575455
    },
    {
      "eval_loss": 3.7908804416656494,
      "eval_runtime": 44.7215,
      "eval_samples_per_second": 92.327,
      "eval_steps_per_second": 5.791,
      "epoch": 29.35323383084577
    },
    {
      "eval_loss": 3.7672083377838135,
      "eval_runtime": 45.0197,
      "eval_samples_per_second": 91.715,
      "eval_steps_per_second": 5.753,
      "epoch": 29.519071310116086
    },
    {
      "eval_loss": 3.7560667991638184,
      "eval_runtime": 44.8089,
      "eval_samples_per_second": 92.147,
      "eval_steps_per_second": 5.78,
      "epoch": 29.6849087893864
    },
    {
      "eval_loss": 3.7604968547821045,
      "eval_runtime": 44.9886,
      "eval_samples_per_second": 91.779,
      "eval_steps_per_second": 5.757,
      "epoch": 29.850746268656717
    },
    {
      "eval_loss": 3.7441766262054443,
      "eval_runtime": 44.8897,
      "eval_samples_per_second": 91.981,
      "eval_steps_per_second": 5.77,
      "epoch": 30.016583747927033
    },
    {
      "eval_loss": 3.7156827449798584,
      "eval_runtime": 44.9204,
      "eval_samples_per_second": 91.918,
      "eval_steps_per_second": 5.766,
      "epoch": 30.182421227197345
    },
    {
      "eval_loss": 3.7316970825195312,
      "eval_runtime": 44.8281,
      "eval_samples_per_second": 92.107,
      "eval_steps_per_second": 5.778,
      "epoch": 30.34825870646766
    },
    {
      "eval_loss": 3.6949868202209473,
      "eval_runtime": 44.7943,
      "eval_samples_per_second": 92.177,
      "eval_steps_per_second": 5.782,
      "epoch": 30.514096185737976
    },
    {
      "eval_loss": 3.6969614028930664,
      "eval_runtime": 44.8333,
      "eval_samples_per_second": 92.097,
      "eval_steps_per_second": 5.777,
      "epoch": 30.679933665008292
    },
    {
      "eval_loss": 3.694748878479004,
      "eval_runtime": 44.8174,
      "eval_samples_per_second": 92.129,
      "eval_steps_per_second": 5.779,
      "epoch": 30.845771144278608
    },
    {
      "eval_loss": 3.6815686225891113,
      "eval_runtime": 44.7944,
      "eval_samples_per_second": 92.177,
      "eval_steps_per_second": 5.782,
      "epoch": 31.011608623548923
    },
    {
      "eval_loss": 3.667508840560913,
      "eval_runtime": 44.9226,
      "eval_samples_per_second": 91.914,
      "eval_steps_per_second": 5.765,
      "epoch": 31.177446102819236
    },
    {
      "eval_loss": 3.657566785812378,
      "eval_runtime": 45.0012,
      "eval_samples_per_second": 91.753,
      "eval_steps_per_second": 5.755,
      "epoch": 31.34328358208955
    },
    {
      "eval_loss": 3.6693782806396484,
      "eval_runtime": 44.9537,
      "eval_samples_per_second": 91.85,
      "eval_steps_per_second": 5.761,
      "epoch": 31.509121061359867
    },
    {
      "eval_loss": 3.6473681926727295,
      "eval_runtime": 44.7167,
      "eval_samples_per_second": 92.337,
      "eval_steps_per_second": 5.792,
      "epoch": 31.674958540630183
    },
    {
      "eval_loss": 3.639246702194214,
      "eval_runtime": 44.8243,
      "eval_samples_per_second": 92.115,
      "eval_steps_per_second": 5.778,
      "epoch": 31.8407960199005
    },
    {
      "eval_loss": 3.6180613040924072,
      "eval_runtime": 44.8782,
      "eval_samples_per_second": 92.005,
      "eval_steps_per_second": 5.771,
      "epoch": 32.00663349917081
    },
    {
      "eval_loss": 3.6100614070892334,
      "eval_runtime": 44.9134,
      "eval_samples_per_second": 91.932,
      "eval_steps_per_second": 5.767,
      "epoch": 32.172470978441126
    },
    {
      "eval_loss": 3.6150455474853516,
      "eval_runtime": 44.8883,
      "eval_samples_per_second": 91.984,
      "eval_steps_per_second": 5.77,
      "epoch": 32.33830845771144
    },
    {
      "eval_loss": 3.6033637523651123,
      "eval_runtime": 44.7316,
      "eval_samples_per_second": 92.306,
      "eval_steps_per_second": 5.79,
      "epoch": 32.50414593698176
    },
    {
      "eval_loss": 3.5966522693634033,
      "eval_runtime": 44.8766,
      "eval_samples_per_second": 92.008,
      "eval_steps_per_second": 5.771,
      "epoch": 32.66998341625207
    },
    {
      "eval_loss": 3.5793328285217285,
      "eval_runtime": 44.7344,
      "eval_samples_per_second": 92.3,
      "eval_steps_per_second": 5.79,
      "epoch": 32.83582089552239
    },
    {
      "eval_loss": 3.571507453918457,
      "eval_runtime": 44.7829,
      "eval_samples_per_second": 92.2,
      "eval_steps_per_second": 5.783,
      "epoch": 33.001658374792704
    },
    {
      "eval_loss": 3.5554604530334473,
      "eval_runtime": 44.7894,
      "eval_samples_per_second": 92.187,
      "eval_steps_per_second": 5.783,
      "epoch": 33.16749585406302
    },
    {
      "eval_loss": 3.562905788421631,
      "eval_runtime": 44.8208,
      "eval_samples_per_second": 92.122,
      "eval_steps_per_second": 5.779,
      "epoch": 33.333333333333336
    },
    {
      "eval_loss": 3.5444228649139404,
      "eval_runtime": 44.7625,
      "eval_samples_per_second": 92.242,
      "eval_steps_per_second": 5.786,
      "epoch": 33.49917081260365
    },
    {
      "eval_loss": 3.553972005844116,
      "eval_runtime": 44.795,
      "eval_samples_per_second": 92.175,
      "eval_steps_per_second": 5.782,
      "epoch": 33.66500829187396
    },
    {
      "eval_loss": 3.5422511100769043,
      "eval_runtime": 44.9084,
      "eval_samples_per_second": 91.943,
      "eval_steps_per_second": 5.767,
      "epoch": 33.830845771144276
    },
    {
      "eval_loss": 3.5425634384155273,
      "eval_runtime": 44.8206,
      "eval_samples_per_second": 92.123,
      "eval_steps_per_second": 5.779,
      "epoch": 33.99668325041459
    },
    {
      "eval_loss": 3.5281667709350586,
      "eval_runtime": 44.7735,
      "eval_samples_per_second": 92.22,
      "eval_steps_per_second": 5.785,
      "epoch": 34.16252072968491
    },
    {
      "eval_loss": 3.5268945693969727,
      "eval_runtime": 44.8166,
      "eval_samples_per_second": 92.131,
      "eval_steps_per_second": 5.779,
      "epoch": 34.32835820895522
    },
    {
      "eval_loss": 3.5163333415985107,
      "eval_runtime": 44.7544,
      "eval_samples_per_second": 92.259,
      "eval_steps_per_second": 5.787,
      "epoch": 34.49419568822554
    },
    {
      "eval_loss": 3.4883246421813965,
      "eval_runtime": 44.7733,
      "eval_samples_per_second": 92.22,
      "eval_steps_per_second": 5.785,
      "epoch": 34.660033167495854
    },
    {
      "eval_loss": 3.4946136474609375,
      "eval_runtime": 44.9144,
      "eval_samples_per_second": 91.93,
      "eval_steps_per_second": 5.767,
      "epoch": 34.82587064676617
    },
    {
      "eval_loss": 3.502460241317749,
      "eval_runtime": 44.8067,
      "eval_samples_per_second": 92.151,
      "eval_steps_per_second": 5.78,
      "epoch": 34.991708126036485
    },
    {
      "eval_loss": 3.486471652984619,
      "eval_runtime": 44.791,
      "eval_samples_per_second": 92.184,
      "eval_steps_per_second": 5.782,
      "epoch": 35.1575456053068
    },
    {
      "eval_loss": 3.469921350479126,
      "eval_runtime": 44.9486,
      "eval_samples_per_second": 91.861,
      "eval_steps_per_second": 5.762,
      "epoch": 35.32338308457712
    },
    {
      "eval_loss": 3.47245192527771,
      "eval_runtime": 44.9224,
      "eval_samples_per_second": 91.914,
      "eval_steps_per_second": 5.765,
      "epoch": 35.48922056384743
    },
    {
      "eval_loss": 3.466874361038208,
      "eval_runtime": 44.7671,
      "eval_samples_per_second": 92.233,
      "eval_steps_per_second": 5.785,
      "epoch": 35.65505804311775
    },
    {
      "eval_loss": 3.452944040298462,
      "eval_runtime": 44.9284,
      "eval_samples_per_second": 91.902,
      "eval_steps_per_second": 5.765,
      "epoch": 35.82089552238806
    },
    {
      "eval_loss": 3.4550275802612305,
      "eval_runtime": 44.7528,
      "eval_samples_per_second": 92.262,
      "eval_steps_per_second": 5.787,
      "epoch": 35.98673300165837
    },
    {
      "eval_loss": 3.4616622924804688,
      "eval_runtime": 44.7599,
      "eval_samples_per_second": 92.248,
      "eval_steps_per_second": 5.786,
      "epoch": 36.15257048092869
    },
    {
      "eval_loss": 3.435023069381714,
      "eval_runtime": 44.6469,
      "eval_samples_per_second": 92.481,
      "eval_steps_per_second": 5.801,
      "epoch": 36.318407960199
    },
    {
      "eval_loss": 3.4266960620880127,
      "eval_runtime": 44.7484,
      "eval_samples_per_second": 92.271,
      "eval_steps_per_second": 5.788,
      "epoch": 36.48424543946932
    },
    {
      "eval_loss": 3.431136131286621,
      "eval_runtime": 44.9171,
      "eval_samples_per_second": 91.925,
      "eval_steps_per_second": 5.766,
      "epoch": 36.650082918739635
    },
    {
      "eval_loss": 3.4136292934417725,
      "eval_runtime": 44.921,
      "eval_samples_per_second": 91.917,
      "eval_steps_per_second": 5.766,
      "epoch": 36.81592039800995
    },
    {
      "eval_loss": 3.414602756500244,
      "eval_runtime": 44.7801,
      "eval_samples_per_second": 92.206,
      "eval_steps_per_second": 5.784,
      "epoch": 36.981757877280266
    },
    {
      "eval_loss": 3.3960444927215576,
      "eval_runtime": 44.6844,
      "eval_samples_per_second": 92.404,
      "eval_steps_per_second": 5.796,
      "epoch": 37.14759535655058
    },
    {
      "eval_loss": 3.418092727661133,
      "eval_runtime": 44.844,
      "eval_samples_per_second": 92.075,
      "eval_steps_per_second": 5.776,
      "epoch": 37.3134328358209
    },
    {
      "eval_loss": 3.378563642501831,
      "eval_runtime": 44.7737,
      "eval_samples_per_second": 92.219,
      "eval_steps_per_second": 5.785,
      "epoch": 37.47927031509121
    },
    {
      "eval_loss": 3.3861939907073975,
      "eval_runtime": 44.825,
      "eval_samples_per_second": 92.114,
      "eval_steps_per_second": 5.778,
      "epoch": 37.64510779436153
    },
    {
      "eval_loss": 3.3811521530151367,
      "eval_runtime": 44.885,
      "eval_samples_per_second": 91.991,
      "eval_steps_per_second": 5.77,
      "epoch": 37.81094527363184
    },
    {
      "eval_loss": 3.37243914604187,
      "eval_runtime": 44.8368,
      "eval_samples_per_second": 92.09,
      "eval_steps_per_second": 5.777,
      "epoch": 37.97678275290215
    },
    {
      "eval_loss": 3.3724560737609863,
      "eval_runtime": 44.826,
      "eval_samples_per_second": 92.112,
      "eval_steps_per_second": 5.778,
      "epoch": 38.14262023217247
    },
    {
      "eval_loss": 3.3612220287323,
      "eval_runtime": 44.8255,
      "eval_samples_per_second": 92.113,
      "eval_steps_per_second": 5.778,
      "epoch": 38.308457711442784
    },
    {
      "eval_loss": 3.3640027046203613,
      "eval_runtime": 44.857,
      "eval_samples_per_second": 92.048,
      "eval_steps_per_second": 5.774,
      "epoch": 38.4742951907131
    },
    {
      "eval_loss": 3.3507111072540283,
      "eval_runtime": 44.8095,
      "eval_samples_per_second": 92.146,
      "eval_steps_per_second": 5.78,
      "epoch": 38.640132669983416
    },
    {
      "eval_loss": 3.3498647212982178,
      "eval_runtime": 44.8599,
      "eval_samples_per_second": 92.042,
      "eval_steps_per_second": 5.774,
      "epoch": 38.80597014925373
    },
    {
      "eval_loss": 3.349191427230835,
      "eval_runtime": 44.8183,
      "eval_samples_per_second": 92.128,
      "eval_steps_per_second": 5.779,
      "epoch": 38.97180762852405
    },
    {
      "eval_loss": 3.3258252143859863,
      "eval_runtime": 44.7472,
      "eval_samples_per_second": 92.274,
      "eval_steps_per_second": 5.788,
      "epoch": 39.13764510779436
    },
    {
      "eval_loss": 3.322061777114868,
      "eval_runtime": 44.7994,
      "eval_samples_per_second": 92.166,
      "eval_steps_per_second": 5.781,
      "epoch": 39.30348258706468
    },
    {
      "eval_loss": 3.325988531112671,
      "eval_runtime": 44.8895,
      "eval_samples_per_second": 91.981,
      "eval_steps_per_second": 5.77,
      "epoch": 39.469320066334994
    },
    {
      "eval_loss": 3.3308424949645996,
      "eval_runtime": 44.748,
      "eval_samples_per_second": 92.272,
      "eval_steps_per_second": 5.788,
      "epoch": 39.63515754560531
    },
    {
      "eval_loss": 3.3181838989257812,
      "eval_runtime": 44.8312,
      "eval_samples_per_second": 92.101,
      "eval_steps_per_second": 5.777,
      "epoch": 39.80099502487562
    },
    {
      "eval_loss": 3.3137474060058594,
      "eval_runtime": 44.9601,
      "eval_samples_per_second": 91.837,
      "eval_steps_per_second": 5.761,
      "epoch": 39.966832504145934
    },
    {
      "eval_loss": 3.3190197944641113,
      "eval_runtime": 44.9297,
      "eval_samples_per_second": 91.899,
      "eval_steps_per_second": 5.765,
      "epoch": 40.13266998341625
    },
    {
      "eval_loss": 3.2879443168640137,
      "eval_runtime": 44.7881,
      "eval_samples_per_second": 92.19,
      "eval_steps_per_second": 5.783,
      "epoch": 40.298507462686565
    },
    {
      "eval_loss": 3.295941114425659,
      "eval_runtime": 44.7672,
      "eval_samples_per_second": 92.233,
      "eval_steps_per_second": 5.785,
      "epoch": 40.46434494195688
    },
    {
      "eval_loss": 3.276482105255127,
      "eval_runtime": 44.8525,
      "eval_samples_per_second": 92.057,
      "eval_steps_per_second": 5.774,
      "epoch": 40.6301824212272
    },
    {
      "eval_loss": 3.2859244346618652,
      "eval_runtime": 44.964,
      "eval_samples_per_second": 91.829,
      "eval_steps_per_second": 5.76,
      "epoch": 40.79601990049751
    },
    {
      "eval_loss": 3.274113655090332,
      "eval_runtime": 44.9675,
      "eval_samples_per_second": 91.822,
      "eval_steps_per_second": 5.76,
      "epoch": 40.96185737976783
    },
    {
      "eval_loss": 3.256075620651245,
      "eval_runtime": 44.8648,
      "eval_samples_per_second": 92.032,
      "eval_steps_per_second": 5.773,
      "epoch": 41.127694859038144
    },
    {
      "eval_loss": 3.262702703475952,
      "eval_runtime": 44.8093,
      "eval_samples_per_second": 92.146,
      "eval_steps_per_second": 5.78,
      "epoch": 41.29353233830846
    },
    {
      "eval_loss": 3.2767136096954346,
      "eval_runtime": 44.9585,
      "eval_samples_per_second": 91.84,
      "eval_steps_per_second": 5.761,
      "epoch": 41.459369817578775
    },
    {
      "eval_loss": 3.2513697147369385,
      "eval_runtime": 44.8353,
      "eval_samples_per_second": 92.093,
      "eval_steps_per_second": 5.777,
      "epoch": 41.62520729684909
    },
    {
      "eval_loss": 3.2618753910064697,
      "eval_runtime": 44.8331,
      "eval_samples_per_second": 92.097,
      "eval_steps_per_second": 5.777,
      "epoch": 41.791044776119406
    },
    {
      "eval_loss": 3.2425596714019775,
      "eval_runtime": 44.9097,
      "eval_samples_per_second": 91.94,
      "eval_steps_per_second": 5.767,
      "epoch": 41.956882255389715
    },
    {
      "eval_loss": 3.2356631755828857,
      "eval_runtime": 44.7657,
      "eval_samples_per_second": 92.236,
      "eval_steps_per_second": 5.786,
      "epoch": 42.12271973466003
    },
    {
      "eval_loss": 3.2405736446380615,
      "eval_runtime": 44.8514,
      "eval_samples_per_second": 92.06,
      "eval_steps_per_second": 5.775,
      "epoch": 42.288557213930346
    },
    {
      "eval_loss": 3.2461225986480713,
      "eval_runtime": 44.7349,
      "eval_samples_per_second": 92.299,
      "eval_steps_per_second": 5.79,
      "epoch": 42.45439469320066
    },
    {
      "eval_loss": 3.230443239212036,
      "eval_runtime": 46.2306,
      "eval_samples_per_second": 89.313,
      "eval_steps_per_second": 5.602,
      "epoch": 42.62023217247098
    },
    {
      "eval_loss": 3.219569206237793,
      "eval_runtime": 46.19,
      "eval_samples_per_second": 89.392,
      "eval_steps_per_second": 5.607,
      "epoch": 42.78606965174129
    },
    {
      "eval_loss": 3.2099087238311768,
      "eval_runtime": 45.9857,
      "eval_samples_per_second": 89.789,
      "eval_steps_per_second": 5.632,
      "epoch": 42.95190713101161
    },
    {
      "eval_loss": 3.223168134689331,
      "eval_runtime": 45.3877,
      "eval_samples_per_second": 90.972,
      "eval_steps_per_second": 5.706,
      "epoch": 43.117744610281925
    },
    {
      "eval_loss": 3.2046031951904297,
      "eval_runtime": 46.3416,
      "eval_samples_per_second": 89.099,
      "eval_steps_per_second": 5.589,
      "epoch": 43.28358208955224
    },
    {
      "eval_loss": 3.1903035640716553,
      "eval_runtime": 46.1049,
      "eval_samples_per_second": 89.557,
      "eval_steps_per_second": 5.618,
      "epoch": 43.449419568822556
    },
    {
      "eval_loss": 3.2141690254211426,
      "eval_runtime": 43.559,
      "eval_samples_per_second": 94.791,
      "eval_steps_per_second": 5.946,
      "epoch": 43.61525704809287
    },
    {
      "eval_loss": 3.2099123001098633,
      "eval_runtime": 46.4512,
      "eval_samples_per_second": 88.889,
      "eval_steps_per_second": 5.576,
      "epoch": 43.78109452736319
    },
    {
      "eval_loss": 3.1888222694396973,
      "eval_runtime": 46.6152,
      "eval_samples_per_second": 88.576,
      "eval_steps_per_second": 5.556,
      "epoch": 43.946932006633496
    },
    {
      "eval_loss": 3.1834030151367188,
      "eval_runtime": 46.6691,
      "eval_samples_per_second": 88.474,
      "eval_steps_per_second": 5.55,
      "epoch": 44.11276948590381
    },
    {
      "eval_loss": 3.1851277351379395,
      "eval_runtime": 46.7158,
      "eval_samples_per_second": 88.386,
      "eval_steps_per_second": 5.544,
      "epoch": 44.27860696517413
    },
    {
      "eval_loss": 3.1820638179779053,
      "eval_runtime": 46.4935,
      "eval_samples_per_second": 88.808,
      "eval_steps_per_second": 5.571,
      "epoch": 44.44444444444444
    },
    {
      "eval_loss": 3.1703684329986572,
      "eval_runtime": 46.7174,
      "eval_samples_per_second": 88.383,
      "eval_steps_per_second": 5.544,
      "epoch": 44.61028192371476
    },
    {
      "eval_loss": 3.1656787395477295,
      "eval_runtime": 46.229,
      "eval_samples_per_second": 89.316,
      "eval_steps_per_second": 5.603,
      "epoch": 44.776119402985074
    },
    {
      "eval_loss": 3.1679763793945312,
      "eval_runtime": 46.6628,
      "eval_samples_per_second": 88.486,
      "eval_steps_per_second": 5.55,
      "epoch": 44.94195688225539
    },
    {
      "eval_loss": 3.137481451034546,
      "eval_runtime": 46.5082,
      "eval_samples_per_second": 88.78,
      "eval_steps_per_second": 5.569,
      "epoch": 45.107794361525706
    },
    {
      "eval_loss": 3.166499137878418,
      "eval_runtime": 46.7278,
      "eval_samples_per_second": 88.363,
      "eval_steps_per_second": 5.543,
      "epoch": 45.27363184079602
    },
    {
      "eval_loss": 3.1540400981903076,
      "eval_runtime": 46.7439,
      "eval_samples_per_second": 88.332,
      "eval_steps_per_second": 5.541,
      "epoch": 45.43946932006634
    },
    {
      "eval_loss": 3.145939826965332,
      "eval_runtime": 46.5147,
      "eval_samples_per_second": 88.768,
      "eval_steps_per_second": 5.568,
      "epoch": 45.60530679933665
    },
    {
      "eval_loss": 3.1480653285980225,
      "eval_runtime": 46.8157,
      "eval_samples_per_second": 88.197,
      "eval_steps_per_second": 5.532,
      "epoch": 45.77114427860697
    },
    {
      "eval_loss": 3.141754627227783,
      "eval_runtime": 46.9207,
      "eval_samples_per_second": 88.0,
      "eval_steps_per_second": 5.52,
      "epoch": 45.93698175787728
    },
    {
      "eval_loss": 3.1397643089294434,
      "eval_runtime": 46.9575,
      "eval_samples_per_second": 87.931,
      "eval_steps_per_second": 5.516,
      "epoch": 46.10281923714759
    },
    {
      "eval_loss": 3.1347315311431885,
      "eval_runtime": 46.5704,
      "eval_samples_per_second": 88.661,
      "eval_steps_per_second": 5.561,
      "epoch": 46.26865671641791
    },
    {
      "eval_loss": 3.135179281234741,
      "eval_runtime": 46.951,
      "eval_samples_per_second": 87.943,
      "eval_steps_per_second": 5.516,
      "epoch": 46.434494195688224
    },
    {
      "eval_loss": 3.1284148693084717,
      "eval_runtime": 46.7171,
      "eval_samples_per_second": 88.383,
      "eval_steps_per_second": 5.544,
      "epoch": 46.60033167495854
    },
    {
      "eval_loss": 3.134147882461548,
      "eval_runtime": 46.6262,
      "eval_samples_per_second": 88.555,
      "eval_steps_per_second": 5.555,
      "epoch": 46.766169154228855
    },
    {
      "eval_loss": 3.128800630569458,
      "eval_runtime": 46.6358,
      "eval_samples_per_second": 88.537,
      "eval_steps_per_second": 5.554,
      "epoch": 46.93200663349917
    },
    {
      "eval_loss": 3.129486083984375,
      "eval_runtime": 46.6269,
      "eval_samples_per_second": 88.554,
      "eval_steps_per_second": 5.555,
      "epoch": 47.09784411276949
    },
    {
      "eval_loss": 3.099191188812256,
      "eval_runtime": 46.7672,
      "eval_samples_per_second": 88.288,
      "eval_steps_per_second": 5.538,
      "epoch": 47.2636815920398
    },
    {
      "eval_loss": 3.114976167678833,
      "eval_runtime": 46.5313,
      "eval_samples_per_second": 88.736,
      "eval_steps_per_second": 5.566,
      "epoch": 47.42951907131012
    },
    {
      "eval_loss": 3.1146786212921143,
      "eval_runtime": 46.6763,
      "eval_samples_per_second": 88.46,
      "eval_steps_per_second": 5.549,
      "epoch": 47.59535655058043
    },
    {
      "eval_loss": 3.09824800491333,
      "eval_runtime": 46.5053,
      "eval_samples_per_second": 88.786,
      "eval_steps_per_second": 5.569,
      "epoch": 47.76119402985075
    },
    {
      "eval_loss": 3.0991084575653076,
      "eval_runtime": 46.569,
      "eval_samples_per_second": 88.664,
      "eval_steps_per_second": 5.562,
      "epoch": 47.927031509121065
    },
    {
      "eval_loss": 3.0816140174865723,
      "eval_runtime": 47.2798,
      "eval_samples_per_second": 87.331,
      "eval_steps_per_second": 5.478,
      "epoch": 48.09286898839137
    },
    {
      "eval_loss": 3.091029167175293,
      "eval_runtime": 46.3085,
      "eval_samples_per_second": 89.163,
      "eval_steps_per_second": 5.593,
      "epoch": 48.25870646766169
    },
    {
      "eval_loss": 3.087475299835205,
      "eval_runtime": 46.6005,
      "eval_samples_per_second": 88.604,
      "eval_steps_per_second": 5.558,
      "epoch": 48.424543946932005
    },
    {
      "eval_loss": 3.0860629081726074,
      "eval_runtime": 47.4964,
      "eval_samples_per_second": 86.933,
      "eval_steps_per_second": 5.453,
      "epoch": 48.59038142620232
    },
    {
      "eval_loss": 3.071071147918701,
      "eval_runtime": 46.8086,
      "eval_samples_per_second": 88.21,
      "eval_steps_per_second": 5.533,
      "epoch": 48.756218905472636
    },
    {
      "eval_loss": 3.0664379596710205,
      "eval_runtime": 46.666,
      "eval_samples_per_second": 88.48,
      "eval_steps_per_second": 5.55,
      "epoch": 48.92205638474295
    },
    {
      "eval_loss": 3.0687315464019775,
      "eval_runtime": 46.5749,
      "eval_samples_per_second": 88.653,
      "eval_steps_per_second": 5.561,
      "epoch": 49.08789386401327
    },
    {
      "eval_loss": 3.048915147781372,
      "eval_runtime": 46.5876,
      "eval_samples_per_second": 88.629,
      "eval_steps_per_second": 5.559,
      "epoch": 49.25373134328358
    },
    {
      "eval_loss": 3.0574257373809814,
      "eval_runtime": 46.2767,
      "eval_samples_per_second": 89.224,
      "eval_steps_per_second": 5.597,
      "epoch": 49.4195688225539
    },
    {
      "eval_loss": 3.068362236022949,
      "eval_runtime": 46.6953,
      "eval_samples_per_second": 88.424,
      "eval_steps_per_second": 5.547,
      "epoch": 49.585406301824214
    },
    {
      "eval_loss": 3.0626111030578613,
      "eval_runtime": 46.636,
      "eval_samples_per_second": 88.537,
      "eval_steps_per_second": 5.554,
      "epoch": 49.75124378109453
    },
    {
      "eval_loss": 3.068281888961792,
      "eval_runtime": 46.6559,
      "eval_samples_per_second": 88.499,
      "eval_steps_per_second": 5.551,
      "epoch": 49.917081260364846
    },
    {
      "eval_loss": 3.048734664916992,
      "eval_runtime": 46.5439,
      "eval_samples_per_second": 88.712,
      "eval_steps_per_second": 5.565,
      "epoch": 50.082918739635154
    },
    {
      "eval_loss": 3.0394341945648193,
      "eval_runtime": 46.6502,
      "eval_samples_per_second": 88.51,
      "eval_steps_per_second": 5.552,
      "epoch": 50.24875621890547
    },
    {
      "eval_loss": 3.0622029304504395,
      "eval_runtime": 46.481,
      "eval_samples_per_second": 88.832,
      "eval_steps_per_second": 5.572,
      "epoch": 50.414593698175786
    },
    {
      "eval_loss": 3.0330963134765625,
      "eval_runtime": 46.6105,
      "eval_samples_per_second": 88.585,
      "eval_steps_per_second": 5.557,
      "epoch": 50.5804311774461
    },
    {
      "eval_loss": 3.030937910079956,
      "eval_runtime": 46.6349,
      "eval_samples_per_second": 88.539,
      "eval_steps_per_second": 5.554,
      "epoch": 50.74626865671642
    },
    {
      "eval_loss": 3.044794797897339,
      "eval_runtime": 46.8156,
      "eval_samples_per_second": 88.197,
      "eval_steps_per_second": 5.532,
      "epoch": 50.91210613598673
    },
    {
      "eval_loss": 3.0288426876068115,
      "eval_runtime": 46.722,
      "eval_samples_per_second": 88.374,
      "eval_steps_per_second": 5.543,
      "epoch": 51.07794361525705
    },
    {
      "eval_loss": 3.020921468734741,
      "eval_runtime": 46.6997,
      "eval_samples_per_second": 88.416,
      "eval_steps_per_second": 5.546,
      "epoch": 51.243781094527364
    },
    {
      "eval_loss": 3.0287253856658936,
      "eval_runtime": 46.727,
      "eval_samples_per_second": 88.364,
      "eval_steps_per_second": 5.543,
      "epoch": 51.40961857379768
    },
    {
      "eval_loss": 3.0280721187591553,
      "eval_runtime": 47.8281,
      "eval_samples_per_second": 86.33,
      "eval_steps_per_second": 5.415,
      "epoch": 51.575456053067995
    },
    {
      "eval_loss": 3.0278849601745605,
      "eval_runtime": 46.3436,
      "eval_samples_per_second": 89.095,
      "eval_steps_per_second": 5.589,
      "epoch": 51.74129353233831
    },
    {
      "eval_loss": 3.017292022705078,
      "eval_runtime": 46.4539,
      "eval_samples_per_second": 88.884,
      "eval_steps_per_second": 5.575,
      "epoch": 51.90713101160863
    },
    {
      "eval_loss": 3.0350613594055176,
      "eval_runtime": 47.3577,
      "eval_samples_per_second": 87.187,
      "eval_steps_per_second": 5.469,
      "epoch": 52.072968490878935
    },
    {
      "eval_loss": 3.0060997009277344,
      "eval_runtime": 46.3191,
      "eval_samples_per_second": 89.142,
      "eval_steps_per_second": 5.592,
      "epoch": 52.23880597014925
    },
    {
      "eval_loss": 3.004136800765991,
      "eval_runtime": 46.3526,
      "eval_samples_per_second": 89.078,
      "eval_steps_per_second": 5.588,
      "epoch": 52.40464344941957
    },
    {
      "eval_loss": 3.0087156295776367,
      "eval_runtime": 46.3768,
      "eval_samples_per_second": 89.032,
      "eval_steps_per_second": 5.585,
      "epoch": 52.57048092868988
    },
    {
      "eval_loss": 3.0131402015686035,
      "eval_runtime": 46.5578,
      "eval_samples_per_second": 88.685,
      "eval_steps_per_second": 5.563,
      "epoch": 52.7363184079602
    },
    {
      "eval_loss": 2.9874255657196045,
      "eval_runtime": 46.6247,
      "eval_samples_per_second": 88.558,
      "eval_steps_per_second": 5.555,
      "epoch": 52.90215588723051
    },
    {
      "eval_loss": 2.9942455291748047,
      "eval_runtime": 46.6245,
      "eval_samples_per_second": 88.559,
      "eval_steps_per_second": 5.555,
      "epoch": 53.06799336650083
    },
    {
      "eval_loss": 2.989356279373169,
      "eval_runtime": 47.6952,
      "eval_samples_per_second": 86.57,
      "eval_steps_per_second": 5.43,
      "epoch": 53.233830845771145
    },
    {
      "eval_loss": 2.986487627029419,
      "eval_runtime": 46.5139,
      "eval_samples_per_second": 88.769,
      "eval_steps_per_second": 5.568,
      "epoch": 53.39966832504146
    },
    {
      "eval_loss": 2.9830424785614014,
      "eval_runtime": 46.5391,
      "eval_samples_per_second": 88.721,
      "eval_steps_per_second": 5.565,
      "epoch": 53.565505804311776
    },
    {
      "eval_loss": 2.985645055770874,
      "eval_runtime": 46.5443,
      "eval_samples_per_second": 88.711,
      "eval_steps_per_second": 5.565,
      "epoch": 53.73134328358209
    },
    {
      "eval_loss": 2.988900899887085,
      "eval_runtime": 46.6254,
      "eval_samples_per_second": 88.557,
      "eval_steps_per_second": 5.555,
      "epoch": 53.89718076285241
    },
    {
      "eval_loss": 2.980658531188965,
      "eval_runtime": 46.592,
      "eval_samples_per_second": 88.62,
      "eval_steps_per_second": 5.559,
      "epoch": 54.06301824212272
    },
    {
      "eval_loss": 2.9645206928253174,
      "eval_runtime": 46.4868,
      "eval_samples_per_second": 88.821,
      "eval_steps_per_second": 5.571,
      "epoch": 54.22885572139303
    },
    {
      "eval_loss": 2.9654603004455566,
      "eval_runtime": 46.5517,
      "eval_samples_per_second": 88.697,
      "eval_steps_per_second": 5.564,
      "epoch": 54.39469320066335
    },
    {
      "eval_loss": 2.9718544483184814,
      "eval_runtime": 47.3565,
      "eval_samples_per_second": 87.19,
      "eval_steps_per_second": 5.469,
      "epoch": 54.56053067993366
    },
    {
      "eval_loss": 2.9673385620117188,
      "eval_runtime": 46.6361,
      "eval_samples_per_second": 88.537,
      "eval_steps_per_second": 5.554,
      "epoch": 54.72636815920398
    },
    {
      "eval_loss": 2.9828858375549316,
      "eval_runtime": 46.6381,
      "eval_samples_per_second": 88.533,
      "eval_steps_per_second": 5.553,
      "epoch": 54.892205638474294
    },
    {
      "eval_loss": 2.9606502056121826,
      "eval_runtime": 46.5227,
      "eval_samples_per_second": 88.752,
      "eval_steps_per_second": 5.567,
      "epoch": 55.05804311774461
    },
    {
      "eval_loss": 2.9615843296051025,
      "eval_runtime": 46.588,
      "eval_samples_per_second": 88.628,
      "eval_steps_per_second": 5.559,
      "epoch": 55.223880597014926
    },
    {
      "eval_loss": 2.9554615020751953,
      "eval_runtime": 46.4708,
      "eval_samples_per_second": 88.851,
      "eval_steps_per_second": 5.573,
      "epoch": 55.38971807628524
    },
    {
      "eval_loss": 2.9644298553466797,
      "eval_runtime": 46.3185,
      "eval_samples_per_second": 89.144,
      "eval_steps_per_second": 5.592,
      "epoch": 55.55555555555556
    },
    {
      "eval_loss": 2.9586052894592285,
      "eval_runtime": 46.6148,
      "eval_samples_per_second": 88.577,
      "eval_steps_per_second": 5.556,
      "epoch": 55.72139303482587
    },
    {
      "eval_loss": 2.9357962608337402,
      "eval_runtime": 46.5201,
      "eval_samples_per_second": 88.757,
      "eval_steps_per_second": 5.567,
      "epoch": 55.88723051409619
    },
    {
      "eval_loss": 2.9519405364990234,
      "eval_runtime": 46.6073,
      "eval_samples_per_second": 88.591,
      "eval_steps_per_second": 5.557,
      "epoch": 56.053067993366504
    },
    {
      "eval_loss": 2.948415994644165,
      "eval_runtime": 46.6412,
      "eval_samples_per_second": 88.527,
      "eval_steps_per_second": 5.553,
      "epoch": 56.21890547263681
    },
    {
      "eval_loss": 2.944415330886841,
      "eval_runtime": 46.6759,
      "eval_samples_per_second": 88.461,
      "eval_steps_per_second": 5.549,
      "epoch": 56.38474295190713
    },
    {
      "eval_loss": 2.9392316341400146,
      "eval_runtime": 46.4908,
      "eval_samples_per_second": 88.813,
      "eval_steps_per_second": 5.571,
      "epoch": 56.550580431177444
    },
    {
      "eval_loss": 2.9452617168426514,
      "eval_runtime": 46.6291,
      "eval_samples_per_second": 88.55,
      "eval_steps_per_second": 5.554,
      "epoch": 56.71641791044776
    },
    {
      "eval_loss": 2.9350993633270264,
      "eval_runtime": 46.7675,
      "eval_samples_per_second": 88.288,
      "eval_steps_per_second": 5.538,
      "epoch": 56.882255389718075
    },
    {
      "eval_loss": 2.9408175945281982,
      "eval_runtime": 46.5543,
      "eval_samples_per_second": 88.692,
      "eval_steps_per_second": 5.563,
      "epoch": 57.04809286898839
    },
    {
      "eval_loss": 2.9472968578338623,
      "eval_runtime": 46.5668,
      "eval_samples_per_second": 88.668,
      "eval_steps_per_second": 5.562,
      "epoch": 57.21393034825871
    },
    {
      "eval_loss": 2.928577423095703,
      "eval_runtime": 46.4955,
      "eval_samples_per_second": 88.804,
      "eval_steps_per_second": 5.57,
      "epoch": 57.37976782752902
    },
    {
      "eval_loss": 2.92002010345459,
      "eval_runtime": 46.4138,
      "eval_samples_per_second": 88.961,
      "eval_steps_per_second": 5.58,
      "epoch": 57.54560530679934
    },
    {
      "eval_loss": 2.919623374938965,
      "eval_runtime": 46.3358,
      "eval_samples_per_second": 89.11,
      "eval_steps_per_second": 5.59,
      "epoch": 57.711442786069654
    },
    {
      "eval_loss": 2.926647424697876,
      "eval_runtime": 46.4459,
      "eval_samples_per_second": 88.899,
      "eval_steps_per_second": 5.576,
      "epoch": 57.87728026533997
    },
    {
      "eval_loss": 2.918654203414917,
      "eval_runtime": 46.2209,
      "eval_samples_per_second": 89.332,
      "eval_steps_per_second": 5.604,
      "epoch": 58.043117744610285
    },
    {
      "eval_loss": 2.911079168319702,
      "eval_runtime": 46.0328,
      "eval_samples_per_second": 89.697,
      "eval_steps_per_second": 5.626,
      "epoch": 58.208955223880594
    },
    {
      "eval_loss": 2.9151082038879395,
      "eval_runtime": 45.7232,
      "eval_samples_per_second": 90.304,
      "eval_steps_per_second": 5.665,
      "epoch": 58.37479270315091
    },
    {
      "eval_loss": 2.92311429977417,
      "eval_runtime": 45.5972,
      "eval_samples_per_second": 90.554,
      "eval_steps_per_second": 5.68,
      "epoch": 58.540630182421225
    },
    {
      "eval_loss": 2.8994140625,
      "eval_runtime": 45.173,
      "eval_samples_per_second": 91.404,
      "eval_steps_per_second": 5.734,
      "epoch": 58.70646766169154
    },
    {
      "eval_loss": 2.898406744003296,
      "eval_runtime": 45.6736,
      "eval_samples_per_second": 90.402,
      "eval_steps_per_second": 5.671,
      "epoch": 58.872305140961856
    },
    {
      "eval_loss": 2.909198760986328,
      "eval_runtime": 49.6595,
      "eval_samples_per_second": 83.146,
      "eval_steps_per_second": 5.216,
      "epoch": 59.03814262023217
    },
    {
      "eval_loss": 2.9260153770446777,
      "eval_runtime": 47.5861,
      "eval_samples_per_second": 86.769,
      "eval_steps_per_second": 5.443,
      "epoch": 59.20398009950249
    },
    {
      "eval_loss": 2.914180278778076,
      "eval_runtime": 45.5134,
      "eval_samples_per_second": 90.721,
      "eval_steps_per_second": 5.691,
      "epoch": 59.3698175787728
    },
    {
      "eval_loss": 2.9286255836486816,
      "eval_runtime": 45.1649,
      "eval_samples_per_second": 91.421,
      "eval_steps_per_second": 5.735,
      "epoch": 59.53565505804312
    },
    {
      "eval_loss": 2.9006969928741455,
      "eval_runtime": 45.2674,
      "eval_samples_per_second": 91.214,
      "eval_steps_per_second": 5.722,
      "epoch": 59.701492537313435
    },
    {
      "eval_loss": 2.9103360176086426,
      "eval_runtime": 45.295,
      "eval_samples_per_second": 91.158,
      "eval_steps_per_second": 5.718,
      "epoch": 59.86733001658375
    },
    {
      "eval_loss": 2.90128231048584,
      "eval_runtime": 44.637,
      "eval_samples_per_second": 92.502,
      "eval_steps_per_second": 5.802,
      "epoch": 60.033167495854066
    },
    {
      "eval_loss": 2.90374493598938,
      "eval_runtime": 44.572,
      "eval_samples_per_second": 92.637,
      "eval_steps_per_second": 5.811,
      "epoch": 60.19900497512438
    },
    {
      "eval_loss": 2.87467360496521,
      "eval_runtime": 48.9853,
      "eval_samples_per_second": 84.291,
      "eval_steps_per_second": 5.287,
      "epoch": 60.36484245439469
    },
    {
      "eval_loss": 2.884253740310669,
      "eval_runtime": 48.9243,
      "eval_samples_per_second": 84.396,
      "eval_steps_per_second": 5.294,
      "epoch": 60.530679933665006
    },
    {
      "eval_loss": 2.876120090484619,
      "eval_runtime": 49.7181,
      "eval_samples_per_second": 83.048,
      "eval_steps_per_second": 5.209,
      "epoch": 60.69651741293532
    },
    {
      "eval_loss": 2.890573024749756,
      "eval_runtime": 49.3819,
      "eval_samples_per_second": 83.614,
      "eval_steps_per_second": 5.245,
      "epoch": 60.86235489220564
    },
    {
      "eval_loss": 2.8625218868255615,
      "eval_runtime": 49.7339,
      "eval_samples_per_second": 83.022,
      "eval_steps_per_second": 5.208,
      "epoch": 61.02819237147595
    },
    {
      "eval_loss": 2.8707284927368164,
      "eval_runtime": 51.2141,
      "eval_samples_per_second": 80.622,
      "eval_steps_per_second": 5.057,
      "epoch": 61.19402985074627
    },
    {
      "eval_loss": 2.8870952129364014,
      "eval_runtime": 49.2738,
      "eval_samples_per_second": 83.797,
      "eval_steps_per_second": 5.256,
      "epoch": 61.359867330016584
    },
    {
      "eval_loss": 2.8803999423980713,
      "eval_runtime": 45.8203,
      "eval_samples_per_second": 90.113,
      "eval_steps_per_second": 5.653,
      "epoch": 61.5257048092869
    },
    {
      "eval_loss": 2.8675715923309326,
      "eval_runtime": 45.9128,
      "eval_samples_per_second": 89.931,
      "eval_steps_per_second": 5.641,
      "epoch": 61.691542288557216
    },
    {
      "eval_loss": 2.87373948097229,
      "eval_runtime": 49.496,
      "eval_samples_per_second": 83.421,
      "eval_steps_per_second": 5.233,
      "epoch": 61.85737976782753
    },
    {
      "eval_loss": 2.8703103065490723,
      "eval_runtime": 45.7416,
      "eval_samples_per_second": 90.268,
      "eval_steps_per_second": 5.662,
      "epoch": 62.02321724709785
    },
    {
      "eval_loss": 2.8625564575195312,
      "eval_runtime": 47.3131,
      "eval_samples_per_second": 87.27,
      "eval_steps_per_second": 5.474,
      "epoch": 62.18905472636816
    },
    {
      "eval_loss": 2.8645052909851074,
      "eval_runtime": 47.1442,
      "eval_samples_per_second": 87.582,
      "eval_steps_per_second": 5.494,
      "epoch": 62.35489220563847
    },
    {
      "eval_loss": 2.877948760986328,
      "eval_runtime": 47.0093,
      "eval_samples_per_second": 87.834,
      "eval_steps_per_second": 5.51,
      "epoch": 62.52072968490879
    },
    {
      "eval_loss": 2.8776817321777344,
      "eval_runtime": 46.9644,
      "eval_samples_per_second": 87.918,
      "eval_steps_per_second": 5.515,
      "epoch": 62.6865671641791
    },
    {
      "eval_loss": 2.854315757751465,
      "eval_runtime": 46.844,
      "eval_samples_per_second": 88.144,
      "eval_steps_per_second": 5.529,
      "epoch": 62.85240464344942
    },
    {
      "eval_loss": 2.858328104019165,
      "eval_runtime": 46.9413,
      "eval_samples_per_second": 87.961,
      "eval_steps_per_second": 5.518,
      "epoch": 63.018242122719734
    },
    {
      "eval_loss": 2.857604742050171,
      "eval_runtime": 46.9001,
      "eval_samples_per_second": 88.038,
      "eval_steps_per_second": 5.522,
      "epoch": 63.18407960199005
    },
    {
      "eval_loss": 2.863326072692871,
      "eval_runtime": 46.5677,
      "eval_samples_per_second": 88.667,
      "eval_steps_per_second": 5.562,
      "epoch": 63.349917081260365
    },
    {
      "eval_loss": 2.8597261905670166,
      "eval_runtime": 47.3473,
      "eval_samples_per_second": 87.207,
      "eval_steps_per_second": 5.47,
      "epoch": 63.51575456053068
    },
    {
      "eval_loss": 2.863450527191162,
      "eval_runtime": 48.7489,
      "eval_samples_per_second": 84.699,
      "eval_steps_per_second": 5.313,
      "epoch": 63.681592039801
    },
    {
      "eval_loss": 2.8526246547698975,
      "eval_runtime": 49.5981,
      "eval_samples_per_second": 83.249,
      "eval_steps_per_second": 5.222,
      "epoch": 63.84742951907131
    },
    {
      "eval_loss": 2.847461462020874,
      "eval_runtime": 46.1877,
      "eval_samples_per_second": 89.396,
      "eval_steps_per_second": 5.608,
      "epoch": 64.01326699834162
    },
    {
      "eval_loss": 2.84598708152771,
      "eval_runtime": 47.0468,
      "eval_samples_per_second": 87.764,
      "eval_steps_per_second": 5.505,
      "epoch": 64.17910447761194
    },
    {
      "eval_loss": 2.851780652999878,
      "eval_runtime": 49.1041,
      "eval_samples_per_second": 84.087,
      "eval_steps_per_second": 5.275,
      "epoch": 64.34494195688225
    },
    {
      "eval_loss": 2.857077121734619,
      "eval_runtime": 47.1842,
      "eval_samples_per_second": 87.508,
      "eval_steps_per_second": 5.489,
      "epoch": 64.51077943615257
    },
    {
      "eval_loss": 2.8360917568206787,
      "eval_runtime": 46.9381,
      "eval_samples_per_second": 87.967,
      "eval_steps_per_second": 5.518,
      "epoch": 64.67661691542288
    },
    {
      "eval_loss": 2.8438503742218018,
      "eval_runtime": 46.0655,
      "eval_samples_per_second": 89.633,
      "eval_steps_per_second": 5.622,
      "epoch": 64.8424543946932
    },
    {
      "eval_loss": 2.8502748012542725,
      "eval_runtime": 46.5296,
      "eval_samples_per_second": 88.739,
      "eval_steps_per_second": 5.566,
      "epoch": 65.00829187396351
    },
    {
      "eval_loss": 2.844392776489258,
      "eval_runtime": 49.1545,
      "eval_samples_per_second": 84.0,
      "eval_steps_per_second": 5.269,
      "epoch": 65.17412935323384
    },
    {
      "eval_loss": 2.852677583694458,
      "eval_runtime": 49.2205,
      "eval_samples_per_second": 83.888,
      "eval_steps_per_second": 5.262,
      "epoch": 65.33996683250415
    },
    {
      "eval_loss": 2.8355860710144043,
      "eval_runtime": 49.3992,
      "eval_samples_per_second": 83.584,
      "eval_steps_per_second": 5.243,
      "epoch": 65.50580431177445
    },
    {
      "eval_loss": 2.8268582820892334,
      "eval_runtime": 48.9505,
      "eval_samples_per_second": 84.35,
      "eval_steps_per_second": 5.291,
      "epoch": 65.67164179104478
    },
    {
      "eval_loss": 2.837480068206787,
      "eval_runtime": 49.2201,
      "eval_samples_per_second": 83.888,
      "eval_steps_per_second": 5.262,
      "epoch": 65.83747927031509
    },
    {
      "eval_loss": 2.835357427597046,
      "eval_runtime": 49.0587,
      "eval_samples_per_second": 84.164,
      "eval_steps_per_second": 5.279,
      "epoch": 66.00331674958541
    },
    {
      "eval_loss": 2.836359977722168,
      "eval_runtime": 48.9618,
      "eval_samples_per_second": 84.331,
      "eval_steps_per_second": 5.29,
      "epoch": 66.16915422885572
    },
    {
      "eval_loss": 2.832552909851074,
      "eval_runtime": 49.1562,
      "eval_samples_per_second": 83.998,
      "eval_steps_per_second": 5.269,
      "epoch": 66.33499170812604
    },
    {
      "eval_loss": 2.834019422531128,
      "eval_runtime": 49.6947,
      "eval_samples_per_second": 83.087,
      "eval_steps_per_second": 5.212,
      "epoch": 66.50082918739635
    },
    {
      "eval_loss": 2.8283779621124268,
      "eval_runtime": 49.3062,
      "eval_samples_per_second": 83.742,
      "eval_steps_per_second": 5.253,
      "epoch": 66.66666666666667
    },
    {
      "eval_loss": 2.814612627029419,
      "eval_runtime": 48.9841,
      "eval_samples_per_second": 84.293,
      "eval_steps_per_second": 5.287,
      "epoch": 66.83250414593698
    },
    {
      "eval_loss": 2.8218207359313965,
      "eval_runtime": 49.3158,
      "eval_samples_per_second": 83.726,
      "eval_steps_per_second": 5.252,
      "epoch": 66.9983416252073
    },
    {
      "eval_loss": 2.8187050819396973,
      "eval_runtime": 49.3121,
      "eval_samples_per_second": 83.732,
      "eval_steps_per_second": 5.252,
      "epoch": 67.16417910447761
    },
    {
      "eval_loss": 2.822303533554077,
      "eval_runtime": 49.2373,
      "eval_samples_per_second": 83.859,
      "eval_steps_per_second": 5.26,
      "epoch": 67.33001658374793
    },
    {
      "eval_loss": 2.8169522285461426,
      "eval_runtime": 49.1168,
      "eval_samples_per_second": 84.065,
      "eval_steps_per_second": 5.273,
      "epoch": 67.49585406301824
    },
    {
      "eval_loss": 2.8173396587371826,
      "eval_runtime": 49.3768,
      "eval_samples_per_second": 83.622,
      "eval_steps_per_second": 5.245,
      "epoch": 67.66169154228855
    },
    {
      "eval_loss": 2.798412322998047,
      "eval_runtime": 48.5875,
      "eval_samples_per_second": 84.981,
      "eval_steps_per_second": 5.331,
      "epoch": 67.82752902155887
    },
    {
      "eval_loss": 2.805098295211792,
      "eval_runtime": 49.196,
      "eval_samples_per_second": 83.93,
      "eval_steps_per_second": 5.265,
      "epoch": 67.99336650082918
    },
    {
      "eval_loss": 2.8122010231018066,
      "eval_runtime": 49.472,
      "eval_samples_per_second": 83.461,
      "eval_steps_per_second": 5.235,
      "epoch": 68.1592039800995
    },
    {
      "eval_loss": 2.8020834922790527,
      "eval_runtime": 48.6443,
      "eval_samples_per_second": 84.881,
      "eval_steps_per_second": 5.324,
      "epoch": 68.32504145936981
    },
    {
      "eval_loss": 2.80495023727417,
      "eval_runtime": 48.5033,
      "eval_samples_per_second": 85.128,
      "eval_steps_per_second": 5.34,
      "epoch": 68.49087893864014
    },
    {
      "eval_loss": 2.8130273818969727,
      "eval_runtime": 49.13,
      "eval_samples_per_second": 84.042,
      "eval_steps_per_second": 5.272,
      "epoch": 68.65671641791045
    },
    {
      "eval_loss": 2.8064723014831543,
      "eval_runtime": 49.4819,
      "eval_samples_per_second": 83.445,
      "eval_steps_per_second": 5.234,
      "epoch": 68.82255389718077
    },
    {
      "eval_loss": 2.7963640689849854,
      "eval_runtime": 49.1048,
      "eval_samples_per_second": 84.085,
      "eval_steps_per_second": 5.274,
      "epoch": 68.98839137645108
    },
    {
      "eval_loss": 2.807535171508789,
      "eval_runtime": 46.3254,
      "eval_samples_per_second": 89.13,
      "eval_steps_per_second": 5.591,
      "epoch": 69.1542288557214
    },
    {
      "eval_loss": 2.79221510887146,
      "eval_runtime": 46.2699,
      "eval_samples_per_second": 89.237,
      "eval_steps_per_second": 5.598,
      "epoch": 69.32006633499171
    },
    {
      "eval_loss": 2.795353651046753,
      "eval_runtime": 46.1589,
      "eval_samples_per_second": 89.452,
      "eval_steps_per_second": 5.611,
      "epoch": 69.48590381426202
    },
    {
      "eval_loss": 2.792280435562134,
      "eval_runtime": 46.3347,
      "eval_samples_per_second": 89.113,
      "eval_steps_per_second": 5.59,
      "epoch": 69.65174129353234
    },
    {
      "eval_loss": 2.7935268878936768,
      "eval_runtime": 46.2703,
      "eval_samples_per_second": 89.237,
      "eval_steps_per_second": 5.598,
      "epoch": 69.81757877280265
    },
    {
      "eval_loss": 2.7966244220733643,
      "eval_runtime": 46.1779,
      "eval_samples_per_second": 89.415,
      "eval_steps_per_second": 5.609,
      "epoch": 69.98341625207297
    },
    {
      "eval_loss": 2.805509567260742,
      "eval_runtime": 46.2794,
      "eval_samples_per_second": 89.219,
      "eval_steps_per_second": 5.596,
      "epoch": 70.14925373134328
    },
    {
      "eval_loss": 2.7834789752960205,
      "eval_runtime": 46.2618,
      "eval_samples_per_second": 89.253,
      "eval_steps_per_second": 5.599,
      "epoch": 70.3150912106136
    },
    {
      "eval_loss": 2.7995612621307373,
      "eval_runtime": 46.233,
      "eval_samples_per_second": 89.309,
      "eval_steps_per_second": 5.602,
      "epoch": 70.48092868988391
    },
    {
      "eval_loss": 2.80234432220459,
      "eval_runtime": 46.2178,
      "eval_samples_per_second": 89.338,
      "eval_steps_per_second": 5.604,
      "epoch": 70.64676616915423
    },
    {
      "eval_loss": 2.7835021018981934,
      "eval_runtime": 46.2397,
      "eval_samples_per_second": 89.296,
      "eval_steps_per_second": 5.601,
      "epoch": 70.81260364842454
    },
    {
      "eval_loss": 2.791728973388672,
      "eval_runtime": 46.215,
      "eval_samples_per_second": 89.343,
      "eval_steps_per_second": 5.604,
      "epoch": 70.97844112769486
    },
    {
      "eval_loss": 2.7878260612487793,
      "eval_runtime": 47.0997,
      "eval_samples_per_second": 87.665,
      "eval_steps_per_second": 5.499,
      "epoch": 71.14427860696517
    },
    {
      "eval_loss": 2.792365550994873,
      "eval_runtime": 46.2717,
      "eval_samples_per_second": 89.234,
      "eval_steps_per_second": 5.597,
      "epoch": 71.3101160862355
    },
    {
      "eval_loss": 2.7806942462921143,
      "eval_runtime": 46.2736,
      "eval_samples_per_second": 89.23,
      "eval_steps_per_second": 5.597,
      "epoch": 71.4759535655058
    },
    {
      "eval_loss": 2.777536630630493,
      "eval_runtime": 49.205,
      "eval_samples_per_second": 83.914,
      "eval_steps_per_second": 5.264,
      "epoch": 71.64179104477611
    },
    {
      "eval_loss": 2.7781546115875244,
      "eval_runtime": 47.4873,
      "eval_samples_per_second": 86.95,
      "eval_steps_per_second": 5.454,
      "epoch": 71.80762852404644
    },
    {
      "eval_loss": 2.7832276821136475,
      "eval_runtime": 49.2185,
      "eval_samples_per_second": 83.891,
      "eval_steps_per_second": 5.262,
      "epoch": 71.97346600331674
    },
    {
      "eval_loss": 2.775618314743042,
      "eval_runtime": 48.7108,
      "eval_samples_per_second": 84.766,
      "eval_steps_per_second": 5.317,
      "epoch": 72.13930348258707
    },
    {
      "eval_loss": 2.791391611099243,
      "eval_runtime": 47.8968,
      "eval_samples_per_second": 86.206,
      "eval_steps_per_second": 5.407,
      "epoch": 72.30514096185738
    },
    {
      "eval_loss": 2.770627975463867,
      "eval_runtime": 48.6661,
      "eval_samples_per_second": 84.843,
      "eval_steps_per_second": 5.322,
      "epoch": 72.4709784411277
    },
    {
      "eval_loss": 2.7644472122192383,
      "eval_runtime": 45.9746,
      "eval_samples_per_second": 89.81,
      "eval_steps_per_second": 5.634,
      "epoch": 72.636815920398
    },
    {
      "eval_loss": 2.798290252685547,
      "eval_runtime": 48.3617,
      "eval_samples_per_second": 85.377,
      "eval_steps_per_second": 5.355,
      "epoch": 72.80265339966833
    },
    {
      "eval_loss": 2.769012451171875,
      "eval_runtime": 48.0235,
      "eval_samples_per_second": 85.979,
      "eval_steps_per_second": 5.393,
      "epoch": 72.96849087893864
    },
    {
      "eval_loss": 2.7822704315185547,
      "eval_runtime": 47.5356,
      "eval_samples_per_second": 86.861,
      "eval_steps_per_second": 5.449,
      "epoch": 73.13432835820896
    },
    {
      "eval_loss": 2.7691400051116943,
      "eval_runtime": 49.7905,
      "eval_samples_per_second": 82.927,
      "eval_steps_per_second": 5.202,
      "epoch": 73.30016583747927
    },
    {
      "eval_loss": 2.7629706859588623,
      "eval_runtime": 48.1943,
      "eval_samples_per_second": 85.674,
      "eval_steps_per_second": 5.374,
      "epoch": 73.46600331674958
    },
    {
      "eval_loss": 2.7559871673583984,
      "eval_runtime": 50.7942,
      "eval_samples_per_second": 81.289,
      "eval_steps_per_second": 5.099,
      "epoch": 73.6318407960199
    },
    {
      "eval_loss": 2.7634212970733643,
      "eval_runtime": 49.9629,
      "eval_samples_per_second": 82.641,
      "eval_steps_per_second": 5.184,
      "epoch": 73.79767827529021
    },
    {
      "eval_loss": 2.767535448074341,
      "eval_runtime": 51.0423,
      "eval_samples_per_second": 80.894,
      "eval_steps_per_second": 5.074,
      "epoch": 73.96351575456053
    },
    {
      "eval_loss": 2.7696590423583984,
      "eval_runtime": 51.2373,
      "eval_samples_per_second": 80.586,
      "eval_steps_per_second": 5.055,
      "epoch": 74.12935323383084
    },
    {
      "eval_loss": 2.7650465965270996,
      "eval_runtime": 49.881,
      "eval_samples_per_second": 82.777,
      "eval_steps_per_second": 5.192,
      "epoch": 74.29519071310116
    },
    {
      "eval_loss": 2.7782602310180664,
      "eval_runtime": 47.6768,
      "eval_samples_per_second": 86.604,
      "eval_steps_per_second": 5.432,
      "epoch": 74.46102819237147
    },
    {
      "eval_loss": 2.7736215591430664,
      "eval_runtime": 47.5637,
      "eval_samples_per_second": 86.81,
      "eval_steps_per_second": 5.445,
      "epoch": 74.6268656716418
    },
    {
      "eval_loss": 2.766679525375366,
      "eval_runtime": 47.6256,
      "eval_samples_per_second": 86.697,
      "eval_steps_per_second": 5.438,
      "epoch": 74.7927031509121
    },
    {
      "eval_loss": 2.768174171447754,
      "eval_runtime": 47.7348,
      "eval_samples_per_second": 86.499,
      "eval_steps_per_second": 5.426,
      "epoch": 74.95854063018243
    },
    {
      "eval_loss": 2.765151023864746,
      "eval_runtime": 47.6348,
      "eval_samples_per_second": 86.68,
      "eval_steps_per_second": 5.437,
      "epoch": 75.12437810945273
    },
    {
      "eval_loss": 2.7482223510742188,
      "eval_runtime": 47.4893,
      "eval_samples_per_second": 86.946,
      "eval_steps_per_second": 5.454,
      "epoch": 75.29021558872306
    },
    {
      "eval_loss": 2.750168800354004,
      "eval_runtime": 47.598,
      "eval_samples_per_second": 86.747,
      "eval_steps_per_second": 5.441,
      "epoch": 75.45605306799337
    },
    {
      "eval_loss": 2.7595765590667725,
      "eval_runtime": 47.5137,
      "eval_samples_per_second": 86.901,
      "eval_steps_per_second": 5.451,
      "epoch": 75.62189054726367
    },
    {
      "eval_loss": 2.7550623416900635,
      "eval_runtime": 47.3672,
      "eval_samples_per_second": 87.17,
      "eval_steps_per_second": 5.468,
      "epoch": 75.787728026534
    },
    {
      "eval_loss": 2.756676197052002,
      "eval_runtime": 47.4442,
      "eval_samples_per_second": 87.029,
      "eval_steps_per_second": 5.459,
      "epoch": 75.9535655058043
    },
    {
      "eval_loss": 2.762590169906616,
      "eval_runtime": 47.4976,
      "eval_samples_per_second": 86.931,
      "eval_steps_per_second": 5.453,
      "epoch": 76.11940298507463
    },
    {
      "eval_loss": 2.75960111618042,
      "eval_runtime": 47.5171,
      "eval_samples_per_second": 86.895,
      "eval_steps_per_second": 5.451,
      "epoch": 76.28524046434494
    },
    {
      "eval_loss": 2.7718472480773926,
      "eval_runtime": 47.6575,
      "eval_samples_per_second": 86.639,
      "eval_steps_per_second": 5.435,
      "epoch": 76.45107794361526
    },
    {
      "eval_loss": 2.7385473251342773,
      "eval_runtime": 47.7757,
      "eval_samples_per_second": 86.425,
      "eval_steps_per_second": 5.421,
      "epoch": 76.61691542288557
    },
    {
      "eval_loss": 2.7447195053100586,
      "eval_runtime": 47.589,
      "eval_samples_per_second": 86.764,
      "eval_steps_per_second": 5.442,
      "epoch": 76.78275290215589
    },
    {
      "eval_loss": 2.7447822093963623,
      "eval_runtime": 47.6,
      "eval_samples_per_second": 86.744,
      "eval_steps_per_second": 5.441,
      "epoch": 76.9485903814262
    },
    {
      "eval_loss": 2.759553909301758,
      "eval_runtime": 47.6862,
      "eval_samples_per_second": 86.587,
      "eval_steps_per_second": 5.431,
      "epoch": 77.11442786069652
    },
    {
      "eval_loss": 2.7610840797424316,
      "eval_runtime": 47.8649,
      "eval_samples_per_second": 86.264,
      "eval_steps_per_second": 5.411,
      "epoch": 77.28026533996683
    },
    {
      "eval_loss": 2.747469902038574,
      "eval_runtime": 47.9231,
      "eval_samples_per_second": 86.159,
      "eval_steps_per_second": 5.404,
      "epoch": 77.44610281923715
    },
    {
      "eval_loss": 2.762399435043335,
      "eval_runtime": 47.952,
      "eval_samples_per_second": 86.107,
      "eval_steps_per_second": 5.401,
      "epoch": 77.61194029850746
    },
    {
      "eval_loss": 2.7355268001556396,
      "eval_runtime": 47.7652,
      "eval_samples_per_second": 86.444,
      "eval_steps_per_second": 5.422,
      "epoch": 77.77777777777777
    },
    {
      "eval_loss": 2.749480724334717,
      "eval_runtime": 48.5471,
      "eval_samples_per_second": 85.051,
      "eval_steps_per_second": 5.335,
      "epoch": 77.9436152570481
    },
    {
      "eval_loss": 2.742067337036133,
      "eval_runtime": 48.8459,
      "eval_samples_per_second": 84.531,
      "eval_steps_per_second": 5.302,
      "epoch": 78.1094527363184
    },
    {
      "eval_loss": 2.7442996501922607,
      "eval_runtime": 49.2529,
      "eval_samples_per_second": 83.833,
      "eval_steps_per_second": 5.259,
      "epoch": 78.27529021558873
    },
    {
      "eval_loss": 2.7463793754577637,
      "eval_runtime": 46.8694,
      "eval_samples_per_second": 88.096,
      "eval_steps_per_second": 5.526,
      "epoch": 78.44112769485903
    },
    {
      "eval_loss": 2.757061719894409,
      "eval_runtime": 48.7761,
      "eval_samples_per_second": 84.652,
      "eval_steps_per_second": 5.31,
      "epoch": 78.60696517412936
    },
    {
      "eval_loss": 2.7360684871673584,
      "eval_runtime": 47.6522,
      "eval_samples_per_second": 86.649,
      "eval_steps_per_second": 5.435,
      "epoch": 78.77280265339967
    },
    {
      "eval_loss": 2.7462432384490967,
      "eval_runtime": 46.2494,
      "eval_samples_per_second": 89.277,
      "eval_steps_per_second": 5.6,
      "epoch": 78.93864013266999
    },
    {
      "eval_loss": 2.7301025390625,
      "eval_runtime": 46.1033,
      "eval_samples_per_second": 89.56,
      "eval_steps_per_second": 5.618,
      "epoch": 79.1044776119403
    },
    {
      "eval_loss": 2.7538321018218994,
      "eval_runtime": 46.1944,
      "eval_samples_per_second": 89.383,
      "eval_steps_per_second": 5.607,
      "epoch": 79.27031509121062
    },
    {
      "eval_loss": 2.7451651096343994,
      "eval_runtime": 46.178,
      "eval_samples_per_second": 89.415,
      "eval_steps_per_second": 5.609,
      "epoch": 79.43615257048093
    },
    {
      "eval_loss": 2.739567995071411,
      "eval_runtime": 46.019,
      "eval_samples_per_second": 89.724,
      "eval_steps_per_second": 5.628,
      "epoch": 79.60199004975124
    },
    {
      "eval_loss": 2.732771396636963,
      "eval_runtime": 46.0981,
      "eval_samples_per_second": 89.57,
      "eval_steps_per_second": 5.618,
      "epoch": 79.76782752902156
    },
    {
      "eval_loss": 2.731144666671753,
      "eval_runtime": 46.0731,
      "eval_samples_per_second": 89.618,
      "eval_steps_per_second": 5.621,
      "epoch": 79.93366500829187
    },
    {
      "eval_loss": 2.737589120864868,
      "eval_runtime": 46.1443,
      "eval_samples_per_second": 89.48,
      "eval_steps_per_second": 5.613,
      "epoch": 80.09950248756219
    },
    {
      "eval_loss": 2.73954701423645,
      "eval_runtime": 46.0841,
      "eval_samples_per_second": 89.597,
      "eval_steps_per_second": 5.62,
      "epoch": 80.2653399668325
    },
    {
      "eval_loss": 2.7315926551818848,
      "eval_runtime": 46.0998,
      "eval_samples_per_second": 89.567,
      "eval_steps_per_second": 5.618,
      "epoch": 80.43117744610282
    },
    {
      "eval_loss": 2.743705987930298,
      "eval_runtime": 46.0813,
      "eval_samples_per_second": 89.603,
      "eval_steps_per_second": 5.621,
      "epoch": 80.59701492537313
    },
    {
      "eval_loss": 2.742995023727417,
      "eval_runtime": 46.1096,
      "eval_samples_per_second": 89.547,
      "eval_steps_per_second": 5.617,
      "epoch": 80.76285240464345
    },
    {
      "eval_loss": 2.731686592102051,
      "eval_runtime": 46.1684,
      "eval_samples_per_second": 89.434,
      "eval_steps_per_second": 5.61,
      "epoch": 80.92868988391376
    },
    {
      "eval_loss": 2.7307310104370117,
      "eval_runtime": 46.0782,
      "eval_samples_per_second": 89.609,
      "eval_steps_per_second": 5.621,
      "epoch": 81.09452736318408
    },
    {
      "eval_loss": 2.7427139282226562,
      "eval_runtime": 46.0183,
      "eval_samples_per_second": 89.725,
      "eval_steps_per_second": 5.628,
      "epoch": 81.2603648424544
    },
    {
      "eval_loss": 2.7369229793548584,
      "eval_runtime": 46.1402,
      "eval_samples_per_second": 89.488,
      "eval_steps_per_second": 5.613,
      "epoch": 81.42620232172472
    },
    {
      "eval_loss": 2.7358248233795166,
      "eval_runtime": 46.0421,
      "eval_samples_per_second": 89.679,
      "eval_steps_per_second": 5.625,
      "epoch": 81.59203980099502
    },
    {
      "eval_loss": 2.7365987300872803,
      "eval_runtime": 45.673,
      "eval_samples_per_second": 90.404,
      "eval_steps_per_second": 5.671,
      "epoch": 81.59203980099502
    }
  ],
  "log_history": [
    {
      "loss": 10.9566,
      "grad_norm": 4.742486000061035,
      "learning_rate": 0.0,
      "epoch": 0.001658374792703151
    },
    {
      "loss": 10.9637,
      "grad_norm": 4.902157783508301,
      "learning_rate": 5.417357656163627e-08,
      "epoch": 0.08291873963515754
    },
    {
      "loss": 10.9242,
      "grad_norm": 4.549622535705566,
      "learning_rate": 1.0945273631840797e-07,
      "epoch": 0.16583747927031509
    },
    {
      "eval_loss": 10.879128456115723,
      "eval_runtime": 46.8108,
      "eval_samples_per_second": 88.206,
      "eval_steps_per_second": 5.533,
      "epoch": 0.16583747927031509
    },
    {
      "loss": 10.8563,
      "grad_norm": 4.168105602264404,
      "learning_rate": 1.6473189607517967e-07,
      "epoch": 0.24875621890547264
    },
    {
      "loss": 10.7484,
      "grad_norm": 4.485222816467285,
      "learning_rate": 2.2001105583195135e-07,
      "epoch": 0.33167495854063017
    },
    {
      "eval_loss": 10.635735511779785,
      "eval_runtime": 45.7579,
      "eval_samples_per_second": 90.236,
      "eval_steps_per_second": 5.66,
      "epoch": 0.33167495854063017
    },
    {
      "loss": 10.607,
      "grad_norm": 4.015215873718262,
      "learning_rate": 2.752902155887231e-07,
      "epoch": 0.41459369817578773
    },
    {
      "loss": 10.4584,
      "grad_norm": 3.7590181827545166,
      "learning_rate": 3.3056937534549477e-07,
      "epoch": 0.4975124378109453
    },
    {
      "eval_loss": 10.325429916381836,
      "eval_runtime": 45.9125,
      "eval_samples_per_second": 89.932,
      "eval_steps_per_second": 5.641,
      "epoch": 0.4975124378109453
    },
    {
      "loss": 10.3247,
      "grad_norm": 3.2363831996917725,
      "learning_rate": 3.8584853510226653e-07,
      "epoch": 0.5804311774461028
    },
    {
      "loss": 10.2045,
      "grad_norm": 3.1008269786834717,
      "learning_rate": 4.411276948590382e-07,
      "epoch": 0.6633499170812603
    },
    {
      "eval_loss": 10.083735466003418,
      "eval_runtime": 45.5034,
      "eval_samples_per_second": 90.74,
      "eval_steps_per_second": 5.692,
      "epoch": 0.6633499170812603
    },
    {
      "loss": 10.0877,
      "grad_norm": 3.0451407432556152,
      "learning_rate": 4.964068546158099e-07,
      "epoch": 0.746268656716418
    },
    {
      "loss": 10.0033,
      "grad_norm": 2.772944927215576,
      "learning_rate": 5.516860143725816e-07,
      "epoch": 0.8291873963515755
    },
    {
      "eval_loss": 9.907322883605957,
      "eval_runtime": 45.0895,
      "eval_samples_per_second": 91.574,
      "eval_steps_per_second": 5.744,
      "epoch": 0.8291873963515755
    },
    {
      "loss": 9.9431,
      "grad_norm": 2.675131320953369,
      "learning_rate": 6.069651741293533e-07,
      "epoch": 0.912106135986733
    },
    {
      "loss": 9.8636,
      "grad_norm": 2.739267110824585,
      "learning_rate": 6.62244333886125e-07,
      "epoch": 0.9950248756218906
    },
    {
      "eval_loss": 9.785597801208496,
      "eval_runtime": 45.1287,
      "eval_samples_per_second": 91.494,
      "eval_steps_per_second": 5.739,
      "epoch": 0.9950248756218906
    },
    {
      "loss": 9.8248,
      "grad_norm": 2.746826648712158,
      "learning_rate": 7.175234936428966e-07,
      "epoch": 1.077943615257048
    },
    {
      "loss": 9.7678,
      "grad_norm": 2.615436315536499,
      "learning_rate": 7.728026533996684e-07,
      "epoch": 1.1608623548922057
    },
    {
      "eval_loss": 9.686332702636719,
      "eval_runtime": 45.1166,
      "eval_samples_per_second": 91.519,
      "eval_steps_per_second": 5.741,
      "epoch": 1.1608623548922057
    },
    {
      "loss": 9.7153,
      "grad_norm": 2.7660531997680664,
      "learning_rate": 8.280818131564402e-07,
      "epoch": 1.243781094527363
    },
    {
      "loss": 9.6619,
      "grad_norm": 2.497063398361206,
      "learning_rate": 8.833609729132118e-07,
      "epoch": 1.3266998341625207
    },
    {
      "eval_loss": 9.582322120666504,
      "eval_runtime": 44.9886,
      "eval_samples_per_second": 91.779,
      "eval_steps_per_second": 5.757,
      "epoch": 1.3266998341625207
    },
    {
      "loss": 9.6319,
      "grad_norm": 2.9219963550567627,
      "learning_rate": 9.386401326699835e-07,
      "epoch": 1.4096185737976783
    },
    {
      "loss": 9.585,
      "grad_norm": 2.650625467300415,
      "learning_rate": 9.939192924267551e-07,
      "epoch": 1.4925373134328357
    },
    {
      "eval_loss": 9.494952201843262,
      "eval_runtime": 45.0453,
      "eval_samples_per_second": 91.663,
      "eval_steps_per_second": 5.75,
      "epoch": 1.4925373134328357
    },
    {
      "loss": 9.5446,
      "grad_norm": 2.7181077003479004,
      "learning_rate": 1.049198452183527e-06,
      "epoch": 1.5754560530679935
    },
    {
      "loss": 9.5013,
      "grad_norm": 2.53474497795105,
      "learning_rate": 1.1044776119402987e-06,
      "epoch": 1.658374792703151
    },
    {
      "eval_loss": 9.412392616271973,
      "eval_runtime": 46.4866,
      "eval_samples_per_second": 88.821,
      "eval_steps_per_second": 5.571,
      "epoch": 1.658374792703151
    },
    {
      "loss": 9.44,
      "grad_norm": 2.5653345584869385,
      "learning_rate": 1.1597567716970703e-06,
      "epoch": 1.7412935323383083
    },
    {
      "loss": 9.4015,
      "grad_norm": 2.6627917289733887,
      "learning_rate": 1.215035931453842e-06,
      "epoch": 1.8242122719734661
    },
    {
      "eval_loss": 9.317459106445312,
      "eval_runtime": 46.2039,
      "eval_samples_per_second": 89.365,
      "eval_steps_per_second": 5.606,
      "epoch": 1.8242122719734661
    },
    {
      "loss": 9.357,
      "grad_norm": 2.401170253753662,
      "learning_rate": 1.2703150912106138e-06,
      "epoch": 1.9071310116086235
    },
    {
      "loss": 9.3083,
      "grad_norm": 2.6560723781585693,
      "learning_rate": 1.3255942509673855e-06,
      "epoch": 1.9900497512437811
    },
    {
      "eval_loss": 9.217268943786621,
      "eval_runtime": 45.2414,
      "eval_samples_per_second": 91.266,
      "eval_steps_per_second": 5.725,
      "epoch": 1.9900497512437811
    },
    {
      "loss": 9.2621,
      "grad_norm": 2.293715715408325,
      "learning_rate": 1.380873410724157e-06,
      "epoch": 2.0729684908789388
    },
    {
      "loss": 9.2048,
      "grad_norm": 2.6726884841918945,
      "learning_rate": 1.4361525704809288e-06,
      "epoch": 2.155887230514096
    },
    {
      "eval_loss": 9.122360229492188,
      "eval_runtime": 45.041,
      "eval_samples_per_second": 91.672,
      "eval_steps_per_second": 5.75,
      "epoch": 2.155887230514096
    },
    {
      "loss": 9.1319,
      "grad_norm": 2.4839704036712646,
      "learning_rate": 1.4914317302377005e-06,
      "epoch": 2.2388059701492535
    },
    {
      "loss": 9.0888,
      "grad_norm": 2.3492417335510254,
      "learning_rate": 1.5467108899944721e-06,
      "epoch": 2.3217247097844114
    },
    {
      "eval_loss": 9.00854778289795,
      "eval_runtime": 47.3965,
      "eval_samples_per_second": 87.116,
      "eval_steps_per_second": 5.465,
      "epoch": 2.3217247097844114
    },
    {
      "loss": 9.0645,
      "grad_norm": 2.525355577468872,
      "learning_rate": 1.601990049751244e-06,
      "epoch": 2.4046434494195688
    },
    {
      "loss": 8.988,
      "grad_norm": 2.28594970703125,
      "learning_rate": 1.6572692095080156e-06,
      "epoch": 2.487562189054726
    },
    {
      "eval_loss": 8.909120559692383,
      "eval_runtime": 45.4551,
      "eval_samples_per_second": 90.837,
      "eval_steps_per_second": 5.698,
      "epoch": 2.487562189054726
    },
    {
      "loss": 8.9536,
      "grad_norm": 2.444484233856201,
      "learning_rate": 1.7125483692647873e-06,
      "epoch": 2.570480928689884
    },
    {
      "loss": 8.901,
      "grad_norm": 2.2346997261047363,
      "learning_rate": 1.7678275290215592e-06,
      "epoch": 2.6533996683250414
    },
    {
      "eval_loss": 8.78382682800293,
      "eval_runtime": 46.2376,
      "eval_samples_per_second": 89.3,
      "eval_steps_per_second": 5.602,
      "epoch": 2.6533996683250414
    },
    {
      "loss": 8.8035,
      "grad_norm": 2.2385382652282715,
      "learning_rate": 1.8231066887783308e-06,
      "epoch": 2.7363184079601988
    },
    {
      "loss": 8.7757,
      "grad_norm": 2.2244179248809814,
      "learning_rate": 1.8783858485351025e-06,
      "epoch": 2.8192371475953566
    },
    {
      "eval_loss": 8.682120323181152,
      "eval_runtime": 46.0074,
      "eval_samples_per_second": 89.746,
      "eval_steps_per_second": 5.63,
      "epoch": 2.8192371475953566
    },
    {
      "loss": 8.716,
      "grad_norm": 2.1351704597473145,
      "learning_rate": 1.9336650082918743e-06,
      "epoch": 2.902155887230514
    },
    {
      "loss": 8.6659,
      "grad_norm": 2.2782070636749268,
      "learning_rate": 1.988944168048646e-06,
      "epoch": 2.9850746268656714
    },
    {
      "eval_loss": 8.56091594696045,
      "eval_runtime": 48.3588,
      "eval_samples_per_second": 85.383,
      "eval_steps_per_second": 5.356,
      "epoch": 2.9850746268656714
    },
    {
      "loss": 8.5815,
      "grad_norm": 2.3956830501556396,
      "learning_rate": 2.0442233278054172e-06,
      "epoch": 3.067993366500829
    },
    {
      "loss": 8.5174,
      "grad_norm": 2.2015366554260254,
      "learning_rate": 2.099502487562189e-06,
      "epoch": 3.1509121061359866
    },
    {
      "eval_loss": 8.438775062561035,
      "eval_runtime": 47.1319,
      "eval_samples_per_second": 87.605,
      "eval_steps_per_second": 5.495,
      "epoch": 3.1509121061359866
    },
    {
      "loss": 8.4882,
      "grad_norm": 2.0785107612609863,
      "learning_rate": 2.154781647318961e-06,
      "epoch": 3.2338308457711444
    },
    {
      "loss": 8.4066,
      "grad_norm": 2.0011067390441895,
      "learning_rate": 2.2100608070757324e-06,
      "epoch": 3.316749585406302
    },
    {
      "eval_loss": 8.322005271911621,
      "eval_runtime": 46.9622,
      "eval_samples_per_second": 87.922,
      "eval_steps_per_second": 5.515,
      "epoch": 3.316749585406302
    },
    {
      "loss": 8.3516,
      "grad_norm": 2.0129621028900146,
      "learning_rate": 2.2653399668325043e-06,
      "epoch": 3.399668325041459
    },
    {
      "loss": 8.2808,
      "grad_norm": 2.1556663513183594,
      "learning_rate": 2.320619126589276e-06,
      "epoch": 3.482587064676617
    },
    {
      "eval_loss": 8.196805953979492,
      "eval_runtime": 46.6291,
      "eval_samples_per_second": 88.55,
      "eval_steps_per_second": 5.554,
      "epoch": 3.482587064676617
    },
    {
      "loss": 8.2244,
      "grad_norm": 1.8787713050842285,
      "learning_rate": 2.3758982863460476e-06,
      "epoch": 3.5655058043117744
    },
    {
      "loss": 8.1779,
      "grad_norm": 1.953739047050476,
      "learning_rate": 2.4311774461028195e-06,
      "epoch": 3.6484245439469323
    },
    {
      "eval_loss": 8.092820167541504,
      "eval_runtime": 46.9412,
      "eval_samples_per_second": 87.961,
      "eval_steps_per_second": 5.518,
      "epoch": 3.6484245439469323
    },
    {
      "loss": 8.1066,
      "grad_norm": 1.9441967010498047,
      "learning_rate": 2.4864566058595913e-06,
      "epoch": 3.7313432835820897
    },
    {
      "loss": 8.0674,
      "grad_norm": 1.991795301437378,
      "learning_rate": 2.5417357656163628e-06,
      "epoch": 3.814262023217247
    },
    {
      "eval_loss": 7.970271110534668,
      "eval_runtime": 47.127,
      "eval_samples_per_second": 87.614,
      "eval_steps_per_second": 5.496,
      "epoch": 3.814262023217247
    },
    {
      "loss": 7.9867,
      "grad_norm": 1.8914352655410767,
      "learning_rate": 2.5970149253731346e-06,
      "epoch": 3.897180762852405
    },
    {
      "loss": 7.9411,
      "grad_norm": 1.6657747030258179,
      "learning_rate": 2.652294085129906e-06,
      "epoch": 3.9800995024875623
    },
    {
      "eval_loss": 7.851957321166992,
      "eval_runtime": 45.8491,
      "eval_samples_per_second": 90.056,
      "eval_steps_per_second": 5.649,
      "epoch": 3.9800995024875623
    },
    {
      "loss": 7.8694,
      "grad_norm": 1.8613905906677246,
      "learning_rate": 2.707573244886678e-06,
      "epoch": 4.06301824212272
    },
    {
      "loss": 7.8063,
      "grad_norm": 1.9291983842849731,
      "learning_rate": 2.7628524046434494e-06,
      "epoch": 4.1459369817578775
    },
    {
      "eval_loss": 7.731200218200684,
      "eval_runtime": 44.939,
      "eval_samples_per_second": 91.88,
      "eval_steps_per_second": 5.763,
      "epoch": 4.1459369817578775
    },
    {
      "loss": 7.7454,
      "grad_norm": 1.7083543539047241,
      "learning_rate": 2.8181315644002217e-06,
      "epoch": 4.2288557213930345
    },
    {
      "loss": 7.7271,
      "grad_norm": 1.596941351890564,
      "learning_rate": 2.873410724156993e-06,
      "epoch": 4.311774461028192
    },
    {
      "eval_loss": 7.646726131439209,
      "eval_runtime": 45.1278,
      "eval_samples_per_second": 91.496,
      "eval_steps_per_second": 5.739,
      "epoch": 4.311774461028192
    },
    {
      "loss": 7.6654,
      "grad_norm": 1.6216955184936523,
      "learning_rate": 2.9286898839137646e-06,
      "epoch": 4.39469320066335
    },
    {
      "loss": 7.6028,
      "grad_norm": 1.6963496208190918,
      "learning_rate": 2.9839690436705365e-06,
      "epoch": 4.477611940298507
    },
    {
      "eval_loss": 7.554970741271973,
      "eval_runtime": 45.0108,
      "eval_samples_per_second": 91.734,
      "eval_steps_per_second": 5.754,
      "epoch": 4.477611940298507
    },
    {
      "loss": 7.5719,
      "grad_norm": 1.5831496715545654,
      "learning_rate": 3.039248203427308e-06,
      "epoch": 4.560530679933665
    },
    {
      "loss": 7.5314,
      "grad_norm": 1.6628557443618774,
      "learning_rate": 3.09452736318408e-06,
      "epoch": 4.643449419568823
    },
    {
      "eval_loss": 7.4625563621521,
      "eval_runtime": 44.8757,
      "eval_samples_per_second": 92.01,
      "eval_steps_per_second": 5.771,
      "epoch": 4.643449419568823
    },
    {
      "loss": 7.462,
      "grad_norm": 1.5095751285552979,
      "learning_rate": 3.1498065229408516e-06,
      "epoch": 4.726368159203981
    },
    {
      "loss": 7.4419,
      "grad_norm": 1.7124309539794922,
      "learning_rate": 3.2050856826976235e-06,
      "epoch": 4.8092868988391375
    },
    {
      "eval_loss": 7.393415927886963,
      "eval_runtime": 45.02,
      "eval_samples_per_second": 91.715,
      "eval_steps_per_second": 5.753,
      "epoch": 4.8092868988391375
    },
    {
      "loss": 7.4076,
      "grad_norm": 1.5965691804885864,
      "learning_rate": 3.260364842454395e-06,
      "epoch": 4.892205638474295
    },
    {
      "loss": 7.4028,
      "grad_norm": 1.5632094144821167,
      "learning_rate": 3.315644002211167e-06,
      "epoch": 4.975124378109452
    },
    {
      "eval_loss": 7.3229217529296875,
      "eval_runtime": 44.9562,
      "eval_samples_per_second": 91.845,
      "eval_steps_per_second": 5.761,
      "epoch": 4.975124378109452
    },
    {
      "loss": 7.3582,
      "grad_norm": 1.5546607971191406,
      "learning_rate": 3.3709231619679383e-06,
      "epoch": 5.05804311774461
    },
    {
      "loss": 7.2983,
      "grad_norm": 1.4542357921600342,
      "learning_rate": 3.4262023217247097e-06,
      "epoch": 5.140961857379768
    },
    {
      "eval_loss": 7.268559455871582,
      "eval_runtime": 44.8761,
      "eval_samples_per_second": 92.009,
      "eval_steps_per_second": 5.771,
      "epoch": 5.140961857379768
    },
    {
      "loss": 7.285,
      "grad_norm": 1.4676111936569214,
      "learning_rate": 3.481481481481482e-06,
      "epoch": 5.223880597014926
    },
    {
      "loss": 7.2676,
      "grad_norm": 1.4789509773254395,
      "learning_rate": 3.5367606412382534e-06,
      "epoch": 5.306799336650083
    },
    {
      "eval_loss": 7.210257053375244,
      "eval_runtime": 44.9207,
      "eval_samples_per_second": 91.917,
      "eval_steps_per_second": 5.766,
      "epoch": 5.306799336650083
    },
    {
      "loss": 7.2364,
      "grad_norm": 1.4203070402145386,
      "learning_rate": 3.5920398009950253e-06,
      "epoch": 5.389718076285241
    },
    {
      "loss": 7.2037,
      "grad_norm": 1.6913678646087646,
      "learning_rate": 3.6473189607517968e-06,
      "epoch": 5.472636815920398
    },
    {
      "eval_loss": 7.1743483543396,
      "eval_runtime": 44.8792,
      "eval_samples_per_second": 92.003,
      "eval_steps_per_second": 5.771,
      "epoch": 5.472636815920398
    },
    {
      "loss": 7.18,
      "grad_norm": 1.4575480222702026,
      "learning_rate": 3.7025981205085686e-06,
      "epoch": 5.555555555555555
    },
    {
      "loss": 7.1825,
      "grad_norm": 1.4059863090515137,
      "learning_rate": 3.75787728026534e-06,
      "epoch": 5.638474295190713
    },
    {
      "eval_loss": 7.118061542510986,
      "eval_runtime": 44.9338,
      "eval_samples_per_second": 91.891,
      "eval_steps_per_second": 5.764,
      "epoch": 5.638474295190713
    },
    {
      "loss": 7.1655,
      "grad_norm": 1.3368442058563232,
      "learning_rate": 3.8131564400221124e-06,
      "epoch": 5.721393034825871
    },
    {
      "loss": 7.109,
      "grad_norm": 1.5474547147750854,
      "learning_rate": 3.868435599778883e-06,
      "epoch": 5.804311774461028
    },
    {
      "eval_loss": 7.089191436767578,
      "eval_runtime": 44.8901,
      "eval_samples_per_second": 91.98,
      "eval_steps_per_second": 5.77,
      "epoch": 5.804311774461028
    },
    {
      "loss": 7.1194,
      "grad_norm": 1.548478126525879,
      "learning_rate": 3.923714759535655e-06,
      "epoch": 5.887230514096186
    },
    {
      "loss": 7.0859,
      "grad_norm": 1.4362413883209229,
      "learning_rate": 3.978993919292427e-06,
      "epoch": 5.970149253731344
    },
    {
      "eval_loss": 7.05344820022583,
      "eval_runtime": 44.8692,
      "eval_samples_per_second": 92.023,
      "eval_steps_per_second": 5.772,
      "epoch": 5.970149253731344
    },
    {
      "loss": 7.0664,
      "grad_norm": 1.414383888244629,
      "learning_rate": 4.034273079049198e-06,
      "epoch": 6.053067993366501
    },
    {
      "loss": 7.0763,
      "grad_norm": 1.417783260345459,
      "learning_rate": 4.089552238805971e-06,
      "epoch": 6.135986733001658
    },
    {
      "eval_loss": 7.0124711990356445,
      "eval_runtime": 44.8168,
      "eval_samples_per_second": 92.131,
      "eval_steps_per_second": 5.779,
      "epoch": 6.135986733001658
    },
    {
      "loss": 7.0538,
      "grad_norm": 1.4196810722351074,
      "learning_rate": 4.144831398562742e-06,
      "epoch": 6.218905472636816
    },
    {
      "loss": 7.0181,
      "grad_norm": 1.328866958618164,
      "learning_rate": 4.200110558319514e-06,
      "epoch": 6.301824212271973
    },
    {
      "eval_loss": 6.956892967224121,
      "eval_runtime": 44.8821,
      "eval_samples_per_second": 91.997,
      "eval_steps_per_second": 5.771,
      "epoch": 6.301824212271973
    },
    {
      "loss": 6.999,
      "grad_norm": 1.5315864086151123,
      "learning_rate": 4.255389718076286e-06,
      "epoch": 6.384742951907131
    },
    {
      "loss": 6.9614,
      "grad_norm": 1.4533332586288452,
      "learning_rate": 4.3106688778330575e-06,
      "epoch": 6.467661691542289
    },
    {
      "eval_loss": 6.893344402313232,
      "eval_runtime": 44.8771,
      "eval_samples_per_second": 92.007,
      "eval_steps_per_second": 5.771,
      "epoch": 6.467661691542289
    },
    {
      "loss": 6.9383,
      "grad_norm": 1.517183542251587,
      "learning_rate": 4.3659480375898285e-06,
      "epoch": 6.550580431177446
    },
    {
      "loss": 6.9177,
      "grad_norm": 1.6613593101501465,
      "learning_rate": 4.421227197346601e-06,
      "epoch": 6.633499170812604
    },
    {
      "eval_loss": 6.864377498626709,
      "eval_runtime": 44.8818,
      "eval_samples_per_second": 91.997,
      "eval_steps_per_second": 5.771,
      "epoch": 6.633499170812604
    },
    {
      "loss": 6.893,
      "grad_norm": 1.582265853881836,
      "learning_rate": 4.476506357103372e-06,
      "epoch": 6.7164179104477615
    },
    {
      "loss": 6.8593,
      "grad_norm": 1.5335623025894165,
      "learning_rate": 4.531785516860144e-06,
      "epoch": 6.799336650082918
    },
    {
      "eval_loss": 6.800537586212158,
      "eval_runtime": 44.845,
      "eval_samples_per_second": 92.073,
      "eval_steps_per_second": 5.775,
      "epoch": 6.799336650082918
    },
    {
      "loss": 6.8293,
      "grad_norm": 1.6206082105636597,
      "learning_rate": 4.587064676616916e-06,
      "epoch": 6.882255389718076
    },
    {
      "loss": 6.8214,
      "grad_norm": 1.558048963546753,
      "learning_rate": 4.642343836373687e-06,
      "epoch": 6.965174129353234
    },
    {
      "eval_loss": 6.749076843261719,
      "eval_runtime": 44.9992,
      "eval_samples_per_second": 91.757,
      "eval_steps_per_second": 5.756,
      "epoch": 6.965174129353234
    },
    {
      "loss": 6.7789,
      "grad_norm": 1.649100661277771,
      "learning_rate": 4.697622996130459e-06,
      "epoch": 7.048092868988391
    },
    {
      "loss": 6.7782,
      "grad_norm": 1.6722900867462158,
      "learning_rate": 4.752902155887231e-06,
      "epoch": 7.131011608623549
    },
    {
      "eval_loss": 6.702304840087891,
      "eval_runtime": 44.9104,
      "eval_samples_per_second": 91.939,
      "eval_steps_per_second": 5.767,
      "epoch": 7.131011608623549
    },
    {
      "loss": 6.7434,
      "grad_norm": 1.6320935487747192,
      "learning_rate": 4.808181315644003e-06,
      "epoch": 7.213930348258707
    },
    {
      "loss": 6.7036,
      "grad_norm": 1.8857557773590088,
      "learning_rate": 4.8634604754007745e-06,
      "epoch": 7.296849087893864
    },
    {
      "eval_loss": 6.638909816741943,
      "eval_runtime": 44.8138,
      "eval_samples_per_second": 92.137,
      "eval_steps_per_second": 5.779,
      "epoch": 7.296849087893864
    },
    {
      "loss": 6.6855,
      "grad_norm": 1.7497819662094116,
      "learning_rate": 4.918739635157546e-06,
      "epoch": 7.3797678275290215
    },
    {
      "loss": 6.663,
      "grad_norm": 1.9301944971084595,
      "learning_rate": 4.974018794914317e-06,
      "epoch": 7.462686567164179
    },
    {
      "eval_loss": 6.585209846496582,
      "eval_runtime": 44.9596,
      "eval_samples_per_second": 91.838,
      "eval_steps_per_second": 5.761,
      "epoch": 7.462686567164179
    },
    {
      "loss": 6.6692,
      "grad_norm": 1.759088158607483,
      "learning_rate": 5.029297954671089e-06,
      "epoch": 7.545605306799336
    },
    {
      "loss": 6.6114,
      "grad_norm": 1.9105181694030762,
      "learning_rate": 5.084577114427861e-06,
      "epoch": 7.628524046434494
    },
    {
      "eval_loss": 6.540661334991455,
      "eval_runtime": 44.8823,
      "eval_samples_per_second": 91.996,
      "eval_steps_per_second": 5.771,
      "epoch": 7.628524046434494
    },
    {
      "loss": 6.6032,
      "grad_norm": 1.8860806226730347,
      "learning_rate": 5.139856274184632e-06,
      "epoch": 7.711442786069652
    },
    {
      "loss": 6.6078,
      "grad_norm": 1.851237416267395,
      "learning_rate": 5.195135433941405e-06,
      "epoch": 7.79436152570481
    },
    {
      "eval_loss": 6.469175338745117,
      "eval_runtime": 44.941,
      "eval_samples_per_second": 91.876,
      "eval_steps_per_second": 5.763,
      "epoch": 7.79436152570481
    },
    {
      "loss": 6.5359,
      "grad_norm": 1.9155042171478271,
      "learning_rate": 5.250414593698177e-06,
      "epoch": 7.877280265339967
    },
    {
      "loss": 6.5204,
      "grad_norm": 1.8901816606521606,
      "learning_rate": 5.305693753454948e-06,
      "epoch": 7.960199004975125
    },
    {
      "eval_loss": 6.448359966278076,
      "eval_runtime": 44.9463,
      "eval_samples_per_second": 91.865,
      "eval_steps_per_second": 5.762,
      "epoch": 7.960199004975125
    },
    {
      "loss": 6.5034,
      "grad_norm": 2.0068373680114746,
      "learning_rate": 5.36097291321172e-06,
      "epoch": 8.043117744610282
    },
    {
      "loss": 6.5231,
      "grad_norm": 1.9599051475524902,
      "learning_rate": 5.416252072968491e-06,
      "epoch": 8.12603648424544
    },
    {
      "eval_loss": 6.388474464416504,
      "eval_runtime": 44.7366,
      "eval_samples_per_second": 92.296,
      "eval_steps_per_second": 5.789,
      "epoch": 8.12603648424544
    },
    {
      "loss": 6.4598,
      "grad_norm": 2.0953476428985596,
      "learning_rate": 5.471531232725263e-06,
      "epoch": 8.208955223880597
    },
    {
      "loss": 6.4462,
      "grad_norm": 1.9975690841674805,
      "learning_rate": 5.526810392482035e-06,
      "epoch": 8.291873963515755
    },
    {
      "eval_loss": 6.346341133117676,
      "eval_runtime": 44.9132,
      "eval_samples_per_second": 91.933,
      "eval_steps_per_second": 5.767,
      "epoch": 8.291873963515755
    },
    {
      "loss": 6.4307,
      "grad_norm": 1.860457420349121,
      "learning_rate": 5.582089552238806e-06,
      "epoch": 8.374792703150913
    },
    {
      "loss": 6.4215,
      "grad_norm": 1.9238072633743286,
      "learning_rate": 5.637368711995578e-06,
      "epoch": 8.457711442786069
    },
    {
      "eval_loss": 6.304312705993652,
      "eval_runtime": 44.9788,
      "eval_samples_per_second": 91.799,
      "eval_steps_per_second": 5.758,
      "epoch": 8.457711442786069
    },
    {
      "loss": 6.3845,
      "grad_norm": 1.9647798538208008,
      "learning_rate": 5.69264787175235e-06,
      "epoch": 8.540630182421227
    },
    {
      "loss": 6.373,
      "grad_norm": 2.057391881942749,
      "learning_rate": 5.747927031509122e-06,
      "epoch": 8.623548922056385
    },
    {
      "eval_loss": 6.261873245239258,
      "eval_runtime": 44.8792,
      "eval_samples_per_second": 92.002,
      "eval_steps_per_second": 5.771,
      "epoch": 8.623548922056385
    },
    {
      "loss": 6.3393,
      "grad_norm": 1.929376482963562,
      "learning_rate": 5.803206191265893e-06,
      "epoch": 8.706467661691542
    },
    {
      "loss": 6.3159,
      "grad_norm": 2.205397367477417,
      "learning_rate": 5.858485351022665e-06,
      "epoch": 8.7893864013267
    },
    {
      "eval_loss": 6.2202324867248535,
      "eval_runtime": 44.916,
      "eval_samples_per_second": 91.927,
      "eval_steps_per_second": 5.766,
      "epoch": 8.7893864013267
    },
    {
      "loss": 6.3122,
      "grad_norm": 2.1180810928344727,
      "learning_rate": 5.913764510779436e-06,
      "epoch": 8.872305140961858
    },
    {
      "loss": 6.2616,
      "grad_norm": 2.2057225704193115,
      "learning_rate": 5.9690436705362084e-06,
      "epoch": 8.955223880597014
    },
    {
      "eval_loss": 6.189189434051514,
      "eval_runtime": 44.864,
      "eval_samples_per_second": 92.034,
      "eval_steps_per_second": 5.773,
      "epoch": 8.955223880597014
    },
    {
      "loss": 6.2657,
      "grad_norm": 2.1447715759277344,
      "learning_rate": 6.02432283029298e-06,
      "epoch": 9.038142620232172
    },
    {
      "loss": 6.2483,
      "grad_norm": 2.149600028991699,
      "learning_rate": 6.079601990049751e-06,
      "epoch": 9.12106135986733
    },
    {
      "eval_loss": 6.157534122467041,
      "eval_runtime": 44.8457,
      "eval_samples_per_second": 92.071,
      "eval_steps_per_second": 5.775,
      "epoch": 9.12106135986733
    },
    {
      "loss": 6.2443,
      "grad_norm": 2.1924540996551514,
      "learning_rate": 6.134881149806523e-06,
      "epoch": 9.203980099502488
    },
    {
      "loss": 6.2253,
      "grad_norm": 2.1270806789398193,
      "learning_rate": 6.190160309563296e-06,
      "epoch": 9.286898839137645
    },
    {
      "eval_loss": 6.109443187713623,
      "eval_runtime": 44.9459,
      "eval_samples_per_second": 91.866,
      "eval_steps_per_second": 5.762,
      "epoch": 9.286898839137645
    },
    {
      "loss": 6.2131,
      "grad_norm": 2.1898679733276367,
      "learning_rate": 6.245439469320067e-06,
      "epoch": 9.369817578772803
    },
    {
      "loss": 6.1756,
      "grad_norm": 2.2323975563049316,
      "learning_rate": 6.300718629076839e-06,
      "epoch": 9.45273631840796
    },
    {
      "eval_loss": 6.08407735824585,
      "eval_runtime": 44.8089,
      "eval_samples_per_second": 92.147,
      "eval_steps_per_second": 5.78,
      "epoch": 9.45273631840796
    },
    {
      "loss": 6.1628,
      "grad_norm": 2.2351338863372803,
      "learning_rate": 6.35599778883361e-06,
      "epoch": 9.535655058043117
    },
    {
      "loss": 6.1509,
      "grad_norm": 2.113097906112671,
      "learning_rate": 6.411276948590382e-06,
      "epoch": 9.618573797678275
    },
    {
      "eval_loss": 6.051288604736328,
      "eval_runtime": 44.8004,
      "eval_samples_per_second": 92.164,
      "eval_steps_per_second": 5.781,
      "epoch": 9.618573797678275
    },
    {
      "loss": 6.1052,
      "grad_norm": 2.365041732788086,
      "learning_rate": 6.4665561083471536e-06,
      "epoch": 9.701492537313433
    },
    {
      "loss": 6.0775,
      "grad_norm": 2.40736722946167,
      "learning_rate": 6.5218352681039254e-06,
      "epoch": 9.78441127694859
    },
    {
      "eval_loss": 6.00562047958374,
      "eval_runtime": 44.9711,
      "eval_samples_per_second": 91.815,
      "eval_steps_per_second": 5.759,
      "epoch": 9.78441127694859
    },
    {
      "loss": 6.0906,
      "grad_norm": 2.2615773677825928,
      "learning_rate": 6.5771144278606965e-06,
      "epoch": 9.867330016583749
    },
    {
      "loss": 6.0672,
      "grad_norm": 2.287912607192993,
      "learning_rate": 6.632393587617468e-06,
      "epoch": 9.950248756218905
    },
    {
      "eval_loss": 5.955612659454346,
      "eval_runtime": 44.961,
      "eval_samples_per_second": 91.835,
      "eval_steps_per_second": 5.761,
      "epoch": 9.950248756218905
    },
    {
      "loss": 6.0704,
      "grad_norm": 2.3913731575012207,
      "learning_rate": 6.687672747374241e-06,
      "epoch": 10.033167495854062
    },
    {
      "loss": 6.0438,
      "grad_norm": 2.26442813873291,
      "learning_rate": 6.742951907131012e-06,
      "epoch": 10.11608623548922
    },
    {
      "eval_loss": 5.942311763763428,
      "eval_runtime": 45.0153,
      "eval_samples_per_second": 91.724,
      "eval_steps_per_second": 5.754,
      "epoch": 10.11608623548922
    },
    {
      "loss": 6.0168,
      "grad_norm": 2.401289463043213,
      "learning_rate": 6.798231066887784e-06,
      "epoch": 10.199004975124378
    },
    {
      "loss": 6.0195,
      "grad_norm": 2.3927481174468994,
      "learning_rate": 6.853510226644555e-06,
      "epoch": 10.281923714759536
    },
    {
      "eval_loss": 5.916191101074219,
      "eval_runtime": 44.8846,
      "eval_samples_per_second": 91.992,
      "eval_steps_per_second": 5.77,
      "epoch": 10.281923714759536
    },
    {
      "loss": 5.9736,
      "grad_norm": 2.389756441116333,
      "learning_rate": 6.908789386401328e-06,
      "epoch": 10.364842454394694
    },
    {
      "loss": 5.9814,
      "grad_norm": 2.4473352432250977,
      "learning_rate": 6.9640685461580995e-06,
      "epoch": 10.447761194029852
    },
    {
      "eval_loss": 5.8641157150268555,
      "eval_runtime": 44.9026,
      "eval_samples_per_second": 91.955,
      "eval_steps_per_second": 5.768,
      "epoch": 10.447761194029852
    },
    {
      "loss": 5.971,
      "grad_norm": 2.4850597381591797,
      "learning_rate": 7.0193477059148706e-06,
      "epoch": 10.530679933665008
    },
    {
      "loss": 5.9575,
      "grad_norm": 2.2944717407226562,
      "learning_rate": 7.0746268656716424e-06,
      "epoch": 10.613598673300165
    },
    {
      "eval_loss": 5.8277058601379395,
      "eval_runtime": 44.8313,
      "eval_samples_per_second": 92.101,
      "eval_steps_per_second": 5.777,
      "epoch": 10.613598673300165
    },
    {
      "loss": 5.9475,
      "grad_norm": 2.392334222793579,
      "learning_rate": 7.1299060254284134e-06,
      "epoch": 10.696517412935323
    },
    {
      "loss": 5.9097,
      "grad_norm": 2.1638238430023193,
      "learning_rate": 7.185185185185186e-06,
      "epoch": 10.779436152570481
    },
    {
      "eval_loss": 5.816876411437988,
      "eval_runtime": 44.9551,
      "eval_samples_per_second": 91.847,
      "eval_steps_per_second": 5.761,
      "epoch": 10.779436152570481
    },
    {
      "loss": 5.923,
      "grad_norm": 2.5088016986846924,
      "learning_rate": 7.240464344941957e-06,
      "epoch": 10.862354892205639
    },
    {
      "loss": 5.8914,
      "grad_norm": 2.6537327766418457,
      "learning_rate": 7.295743504698729e-06,
      "epoch": 10.945273631840797
    },
    {
      "eval_loss": 5.7909932136535645,
      "eval_runtime": 44.8658,
      "eval_samples_per_second": 92.03,
      "eval_steps_per_second": 5.773,
      "epoch": 10.945273631840797
    },
    {
      "loss": 5.8703,
      "grad_norm": 2.300137758255005,
      "learning_rate": 7.351022664455501e-06,
      "epoch": 11.028192371475953
    },
    {
      "loss": 5.8798,
      "grad_norm": 2.436129570007324,
      "learning_rate": 7.406301824212273e-06,
      "epoch": 11.11111111111111
    },
    {
      "eval_loss": 5.760439872741699,
      "eval_runtime": 44.8846,
      "eval_samples_per_second": 91.991,
      "eval_steps_per_second": 5.77,
      "epoch": 11.11111111111111
    },
    {
      "loss": 5.8248,
      "grad_norm": 2.4752395153045654,
      "learning_rate": 7.461580983969045e-06,
      "epoch": 11.194029850746269
    },
    {
      "loss": 5.8148,
      "grad_norm": 2.404580593109131,
      "learning_rate": 7.516860143725816e-06,
      "epoch": 11.276948590381426
    },
    {
      "eval_loss": 5.711305141448975,
      "eval_runtime": 44.9097,
      "eval_samples_per_second": 91.94,
      "eval_steps_per_second": 5.767,
      "epoch": 11.276948590381426
    },
    {
      "loss": 5.823,
      "grad_norm": 2.4921717643737793,
      "learning_rate": 7.5721393034825875e-06,
      "epoch": 11.359867330016584
    },
    {
      "loss": 5.8022,
      "grad_norm": 2.4516446590423584,
      "learning_rate": 7.6274184632393586e-06,
      "epoch": 11.442786069651742
    },
    {
      "eval_loss": 5.695173263549805,
      "eval_runtime": 44.9375,
      "eval_samples_per_second": 91.883,
      "eval_steps_per_second": 5.764,
      "epoch": 11.442786069651742
    },
    {
      "loss": 5.7439,
      "grad_norm": 2.549602746963501,
      "learning_rate": 7.682697622996131e-06,
      "epoch": 11.525704809286898
    },
    {
      "loss": 5.8003,
      "grad_norm": 2.8656153678894043,
      "learning_rate": 7.737976782752903e-06,
      "epoch": 11.608623548922056
    },
    {
      "eval_loss": 5.657410144805908,
      "eval_runtime": 44.8705,
      "eval_samples_per_second": 92.02,
      "eval_steps_per_second": 5.772,
      "epoch": 11.608623548922056
    },
    {
      "loss": 5.7476,
      "grad_norm": 2.855980396270752,
      "learning_rate": 7.793255942509675e-06,
      "epoch": 11.691542288557214
    },
    {
      "loss": 5.7274,
      "grad_norm": 2.7095847129821777,
      "learning_rate": 7.848535102266445e-06,
      "epoch": 11.774461028192372
    },
    {
      "eval_loss": 5.629942893981934,
      "eval_runtime": 44.8995,
      "eval_samples_per_second": 91.961,
      "eval_steps_per_second": 5.768,
      "epoch": 11.774461028192372
    },
    {
      "loss": 5.7334,
      "grad_norm": 2.508673906326294,
      "learning_rate": 7.903814262023219e-06,
      "epoch": 11.85737976782753
    },
    {
      "loss": 5.6997,
      "grad_norm": 2.6512959003448486,
      "learning_rate": 7.95909342177999e-06,
      "epoch": 11.940298507462687
    },
    {
      "eval_loss": 5.583545684814453,
      "eval_runtime": 44.8834,
      "eval_samples_per_second": 91.994,
      "eval_steps_per_second": 5.771,
      "epoch": 11.940298507462687
    },
    {
      "loss": 5.6878,
      "grad_norm": 2.42924165725708,
      "learning_rate": 8.01437258153676e-06,
      "epoch": 12.023217247097843
    },
    {
      "loss": 5.6668,
      "grad_norm": 2.498377799987793,
      "learning_rate": 8.069651741293533e-06,
      "epoch": 12.106135986733001
    },
    {
      "eval_loss": 5.568991184234619,
      "eval_runtime": 44.8735,
      "eval_samples_per_second": 92.014,
      "eval_steps_per_second": 5.772,
      "epoch": 12.106135986733001
    },
    {
      "loss": 5.6633,
      "grad_norm": 2.4608662128448486,
      "learning_rate": 8.124930901050305e-06,
      "epoch": 12.189054726368159
    },
    {
      "loss": 5.6324,
      "grad_norm": 2.3502681255340576,
      "learning_rate": 8.180210060807076e-06,
      "epoch": 12.271973466003317
    },
    {
      "eval_loss": 5.542919635772705,
      "eval_runtime": 44.8804,
      "eval_samples_per_second": 92.0,
      "eval_steps_per_second": 5.771,
      "epoch": 12.271973466003317
    },
    {
      "loss": 5.6224,
      "grad_norm": 2.4463815689086914,
      "learning_rate": 8.235489220563848e-06,
      "epoch": 12.354892205638475
    },
    {
      "loss": 5.6523,
      "grad_norm": 2.413620710372925,
      "learning_rate": 8.29076838032062e-06,
      "epoch": 12.437810945273633
    },
    {
      "eval_loss": 5.517384052276611,
      "eval_runtime": 44.6778,
      "eval_samples_per_second": 92.417,
      "eval_steps_per_second": 5.797,
      "epoch": 12.437810945273633
    },
    {
      "loss": 5.6421,
      "grad_norm": 2.4983043670654297,
      "learning_rate": 8.34604754007739e-06,
      "epoch": 12.520729684908789
    },
    {
      "loss": 5.6258,
      "grad_norm": 2.5846569538116455,
      "learning_rate": 8.401326699834164e-06,
      "epoch": 12.603648424543946
    },
    {
      "eval_loss": 5.4776506423950195,
      "eval_runtime": 44.8763,
      "eval_samples_per_second": 92.008,
      "eval_steps_per_second": 5.771,
      "epoch": 12.603648424543946
    },
    {
      "loss": 5.5907,
      "grad_norm": 2.3645215034484863,
      "learning_rate": 8.456605859590936e-06,
      "epoch": 12.686567164179104
    },
    {
      "loss": 5.5878,
      "grad_norm": 2.3220860958099365,
      "learning_rate": 8.511885019347706e-06,
      "epoch": 12.769485903814262
    },
    {
      "eval_loss": 5.460651874542236,
      "eval_runtime": 44.8081,
      "eval_samples_per_second": 92.149,
      "eval_steps_per_second": 5.78,
      "epoch": 12.769485903814262
    },
    {
      "loss": 5.5442,
      "grad_norm": 2.541339159011841,
      "learning_rate": 8.567164179104478e-06,
      "epoch": 12.85240464344942
    },
    {
      "loss": 5.5272,
      "grad_norm": 2.4082655906677246,
      "learning_rate": 8.622443338861251e-06,
      "epoch": 12.935323383084578
    },
    {
      "eval_loss": 5.443472385406494,
      "eval_runtime": 44.8909,
      "eval_samples_per_second": 91.979,
      "eval_steps_per_second": 5.77,
      "epoch": 12.935323383084578
    },
    {
      "loss": 5.5382,
      "grad_norm": 2.4534928798675537,
      "learning_rate": 8.677722498618022e-06,
      "epoch": 13.018242122719734
    },
    {
      "loss": 5.5114,
      "grad_norm": 2.5675547122955322,
      "learning_rate": 8.733001658374793e-06,
      "epoch": 13.101160862354892
    },
    {
      "eval_loss": 5.423609256744385,
      "eval_runtime": 44.9998,
      "eval_samples_per_second": 91.756,
      "eval_steps_per_second": 5.756,
      "epoch": 13.101160862354892
    },
    {
      "loss": 5.4866,
      "grad_norm": 2.4529683589935303,
      "learning_rate": 8.788280818131565e-06,
      "epoch": 13.18407960199005
    },
    {
      "loss": 5.4763,
      "grad_norm": 2.5588016510009766,
      "learning_rate": 8.843559977888337e-06,
      "epoch": 13.266998341625207
    },
    {
      "eval_loss": 5.38193941116333,
      "eval_runtime": 44.901,
      "eval_samples_per_second": 91.958,
      "eval_steps_per_second": 5.768,
      "epoch": 13.266998341625207
    },
    {
      "loss": 5.4455,
      "grad_norm": 2.44097900390625,
      "learning_rate": 8.898839137645109e-06,
      "epoch": 13.349917081260365
    },
    {
      "loss": 5.4555,
      "grad_norm": 2.9660534858703613,
      "learning_rate": 8.954118297401881e-06,
      "epoch": 13.432835820895523
    },
    {
      "eval_loss": 5.357533931732178,
      "eval_runtime": 44.8702,
      "eval_samples_per_second": 92.021,
      "eval_steps_per_second": 5.772,
      "epoch": 13.432835820895523
    },
    {
      "loss": 5.4657,
      "grad_norm": 2.6088316440582275,
      "learning_rate": 9.009397457158651e-06,
      "epoch": 13.51575456053068
    },
    {
      "loss": 5.4424,
      "grad_norm": 2.4225690364837646,
      "learning_rate": 9.064676616915423e-06,
      "epoch": 13.598673300165837
    },
    {
      "eval_loss": 5.334133625030518,
      "eval_runtime": 44.9247,
      "eval_samples_per_second": 91.909,
      "eval_steps_per_second": 5.765,
      "epoch": 13.598673300165837
    },
    {
      "loss": 5.4505,
      "grad_norm": 2.6229488849639893,
      "learning_rate": 9.119955776672196e-06,
      "epoch": 13.681592039800995
    },
    {
      "loss": 5.4305,
      "grad_norm": 2.5143179893493652,
      "learning_rate": 9.175234936428967e-06,
      "epoch": 13.764510779436153
    },
    {
      "eval_loss": 5.293779373168945,
      "eval_runtime": 44.929,
      "eval_samples_per_second": 91.901,
      "eval_steps_per_second": 5.765,
      "epoch": 13.764510779436153
    },
    {
      "loss": 5.3949,
      "grad_norm": 2.5874409675598145,
      "learning_rate": 9.230514096185739e-06,
      "epoch": 13.84742951907131
    },
    {
      "loss": 5.3673,
      "grad_norm": 2.3625547885894775,
      "learning_rate": 9.28579325594251e-06,
      "epoch": 13.930348258706468
    },
    {
      "eval_loss": 5.283754348754883,
      "eval_runtime": 44.893,
      "eval_samples_per_second": 91.974,
      "eval_steps_per_second": 5.769,
      "epoch": 13.930348258706468
    },
    {
      "loss": 5.3688,
      "grad_norm": 2.505998134613037,
      "learning_rate": 9.341072415699282e-06,
      "epoch": 14.013266998341626
    },
    {
      "loss": 5.3647,
      "grad_norm": 2.626058578491211,
      "learning_rate": 9.396351575456054e-06,
      "epoch": 14.096185737976782
    },
    {
      "eval_loss": 5.2625651359558105,
      "eval_runtime": 44.9296,
      "eval_samples_per_second": 91.899,
      "eval_steps_per_second": 5.765,
      "epoch": 14.096185737976782
    },
    {
      "loss": 5.364,
      "grad_norm": 2.4693286418914795,
      "learning_rate": 9.451630735212826e-06,
      "epoch": 14.17910447761194
    },
    {
      "loss": 5.3046,
      "grad_norm": 2.444755792617798,
      "learning_rate": 9.506909894969598e-06,
      "epoch": 14.262023217247098
    },
    {
      "eval_loss": 5.207681655883789,
      "eval_runtime": 44.9592,
      "eval_samples_per_second": 91.839,
      "eval_steps_per_second": 5.761,
      "epoch": 14.262023217247098
    },
    {
      "loss": 5.3435,
      "grad_norm": 2.416337251663208,
      "learning_rate": 9.562189054726368e-06,
      "epoch": 14.344941956882256
    },
    {
      "loss": 5.3101,
      "grad_norm": 2.583552122116089,
      "learning_rate": 9.617468214483142e-06,
      "epoch": 14.427860696517413
    },
    {
      "eval_loss": 5.209364891052246,
      "eval_runtime": 44.8933,
      "eval_samples_per_second": 91.974,
      "eval_steps_per_second": 5.769,
      "epoch": 14.427860696517413
    },
    {
      "loss": 5.258,
      "grad_norm": 2.705463409423828,
      "learning_rate": 9.672747374239912e-06,
      "epoch": 14.510779436152571
    },
    {
      "loss": 5.2932,
      "grad_norm": 2.92818284034729,
      "learning_rate": 9.728026533996684e-06,
      "epoch": 14.593698175787727
    },
    {
      "eval_loss": 5.183217525482178,
      "eval_runtime": 44.8478,
      "eval_samples_per_second": 92.067,
      "eval_steps_per_second": 5.775,
      "epoch": 14.593698175787727
    },
    {
      "loss": 5.2822,
      "grad_norm": 2.5978965759277344,
      "learning_rate": 9.783305693753456e-06,
      "epoch": 14.676616915422885
    },
    {
      "loss": 5.246,
      "grad_norm": 2.404097557067871,
      "learning_rate": 9.838584853510227e-06,
      "epoch": 14.759535655058043
    },
    {
      "eval_loss": 5.160017013549805,
      "eval_runtime": 44.8178,
      "eval_samples_per_second": 92.129,
      "eval_steps_per_second": 5.779,
      "epoch": 14.759535655058043
    },
    {
      "loss": 5.2505,
      "grad_norm": 2.46894907951355,
      "learning_rate": 9.893864013267e-06,
      "epoch": 14.8424543946932
    },
    {
      "loss": 5.2314,
      "grad_norm": 2.4202334880828857,
      "learning_rate": 9.949143173023771e-06,
      "epoch": 14.925373134328359
    },
    {
      "eval_loss": 5.144285678863525,
      "eval_runtime": 44.8485,
      "eval_samples_per_second": 92.066,
      "eval_steps_per_second": 5.775,
      "epoch": 14.925373134328359
    },
    {
      "loss": 5.2521,
      "grad_norm": 2.3481662273406982,
      "learning_rate": 1.0004422332780543e-05,
      "epoch": 15.008291873963516
    },
    {
      "loss": 5.1649,
      "grad_norm": 2.403796434402466,
      "learning_rate": 1.0059701492537315e-05,
      "epoch": 15.091210613598673
    },
    {
      "eval_loss": 5.106461524963379,
      "eval_runtime": 44.8658,
      "eval_samples_per_second": 92.03,
      "eval_steps_per_second": 5.773,
      "epoch": 15.091210613598673
    },
    {
      "loss": 5.2003,
      "grad_norm": 2.358527898788452,
      "learning_rate": 1.0114980652294087e-05,
      "epoch": 15.17412935323383
    },
    {
      "loss": 5.1765,
      "grad_norm": 2.644507646560669,
      "learning_rate": 1.0170259812050859e-05,
      "epoch": 15.257048092868988
    },
    {
      "eval_loss": 5.09509801864624,
      "eval_runtime": 44.831,
      "eval_samples_per_second": 92.101,
      "eval_steps_per_second": 5.777,
      "epoch": 15.257048092868988
    },
    {
      "loss": 5.1793,
      "grad_norm": 2.765592575073242,
      "learning_rate": 1.0225538971807629e-05,
      "epoch": 15.339966832504146
    },
    {
      "loss": 5.1649,
      "grad_norm": 2.4003446102142334,
      "learning_rate": 1.02808181315644e-05,
      "epoch": 15.422885572139304
    },
    {
      "eval_loss": 5.080765247344971,
      "eval_runtime": 44.8553,
      "eval_samples_per_second": 92.051,
      "eval_steps_per_second": 5.774,
      "epoch": 15.422885572139304
    },
    {
      "loss": 5.1523,
      "grad_norm": 2.499962091445923,
      "learning_rate": 1.0336097291321172e-05,
      "epoch": 15.505804311774462
    },
    {
      "loss": 5.174,
      "grad_norm": 2.365406036376953,
      "learning_rate": 1.0391376451077944e-05,
      "epoch": 15.588723051409618
    },
    {
      "eval_loss": 5.038093090057373,
      "eval_runtime": 44.7793,
      "eval_samples_per_second": 92.208,
      "eval_steps_per_second": 5.784,
      "epoch": 15.588723051409618
    },
    {
      "loss": 5.0915,
      "grad_norm": 2.5528688430786133,
      "learning_rate": 1.0446655610834718e-05,
      "epoch": 15.671641791044776
    },
    {
      "loss": 5.1216,
      "grad_norm": 2.496272087097168,
      "learning_rate": 1.0501934770591488e-05,
      "epoch": 15.754560530679933
    },
    {
      "eval_loss": 5.012759685516357,
      "eval_runtime": 44.776,
      "eval_samples_per_second": 92.215,
      "eval_steps_per_second": 5.784,
      "epoch": 15.754560530679933
    },
    {
      "loss": 5.1102,
      "grad_norm": 2.8393497467041016,
      "learning_rate": 1.055721393034826e-05,
      "epoch": 15.837479270315091
    },
    {
      "loss": 5.0831,
      "grad_norm": 2.57500958442688,
      "learning_rate": 1.0612493090105032e-05,
      "epoch": 15.92039800995025
    },
    {
      "eval_loss": 5.004238605499268,
      "eval_runtime": 44.861,
      "eval_samples_per_second": 92.04,
      "eval_steps_per_second": 5.773,
      "epoch": 15.92039800995025
    },
    {
      "loss": 5.1148,
      "grad_norm": 2.554312229156494,
      "learning_rate": 1.0667772249861804e-05,
      "epoch": 16.003316749585405
    },
    {
      "loss": 5.0897,
      "grad_norm": 2.3558995723724365,
      "learning_rate": 1.0723051409618574e-05,
      "epoch": 16.086235489220563
    },
    {
      "eval_loss": 4.994377613067627,
      "eval_runtime": 44.8697,
      "eval_samples_per_second": 92.022,
      "eval_steps_per_second": 5.772,
      "epoch": 16.086235489220563
    },
    {
      "loss": 5.0725,
      "grad_norm": 2.622974157333374,
      "learning_rate": 1.0778330569375346e-05,
      "epoch": 16.16915422885572
    },
    {
      "loss": 5.0691,
      "grad_norm": 2.668390989303589,
      "learning_rate": 1.0833609729132118e-05,
      "epoch": 16.25207296849088
    },
    {
      "eval_loss": 4.970526695251465,
      "eval_runtime": 44.8541,
      "eval_samples_per_second": 92.054,
      "eval_steps_per_second": 5.774,
      "epoch": 16.25207296849088
    },
    {
      "loss": 5.0325,
      "grad_norm": 2.4059512615203857,
      "learning_rate": 1.088888888888889e-05,
      "epoch": 16.334991708126037
    },
    {
      "loss": 5.024,
      "grad_norm": 2.691390037536621,
      "learning_rate": 1.0944168048645663e-05,
      "epoch": 16.417910447761194
    },
    {
      "eval_loss": 4.925838470458984,
      "eval_runtime": 44.9371,
      "eval_samples_per_second": 91.884,
      "eval_steps_per_second": 5.764,
      "epoch": 16.417910447761194
    },
    {
      "loss": 4.9935,
      "grad_norm": 2.388420343399048,
      "learning_rate": 1.0999447208402433e-05,
      "epoch": 16.500829187396352
    },
    {
      "loss": 4.991,
      "grad_norm": 2.4687843322753906,
      "learning_rate": 1.1054726368159205e-05,
      "epoch": 16.58374792703151
    },
    {
      "eval_loss": 4.9372735023498535,
      "eval_runtime": 44.8472,
      "eval_samples_per_second": 92.068,
      "eval_steps_per_second": 5.775,
      "epoch": 16.58374792703151
    },
    {
      "loss": 5.0076,
      "grad_norm": 2.4945247173309326,
      "learning_rate": 1.1110005527915977e-05,
      "epoch": 16.666666666666668
    },
    {
      "loss": 4.9923,
      "grad_norm": 2.435224771499634,
      "learning_rate": 1.1165284687672749e-05,
      "epoch": 16.749585406301826
    },
    {
      "eval_loss": 4.886546611785889,
      "eval_runtime": 44.8675,
      "eval_samples_per_second": 92.027,
      "eval_steps_per_second": 5.773,
      "epoch": 16.749585406301826
    },
    {
      "loss": 4.98,
      "grad_norm": 2.3674659729003906,
      "learning_rate": 1.1220563847429519e-05,
      "epoch": 16.83250414593698
    },
    {
      "loss": 4.9648,
      "grad_norm": 2.590312957763672,
      "learning_rate": 1.1275843007186291e-05,
      "epoch": 16.915422885572138
    },
    {
      "eval_loss": 4.8834357261657715,
      "eval_runtime": 44.8501,
      "eval_samples_per_second": 92.062,
      "eval_steps_per_second": 5.775,
      "epoch": 16.915422885572138
    },
    {
      "loss": 4.9362,
      "grad_norm": 2.6689085960388184,
      "learning_rate": 1.1331122166943063e-05,
      "epoch": 16.998341625207296
    },
    {
      "loss": 4.9245,
      "grad_norm": 2.420269727706909,
      "learning_rate": 1.1386401326699835e-05,
      "epoch": 17.081260364842453
    },
    {
      "eval_loss": 4.848888397216797,
      "eval_runtime": 44.7834,
      "eval_samples_per_second": 92.199,
      "eval_steps_per_second": 5.783,
      "epoch": 17.081260364842453
    },
    {
      "loss": 4.9339,
      "grad_norm": 2.6594974994659424,
      "learning_rate": 1.1441680486456608e-05,
      "epoch": 17.16417910447761
    },
    {
      "loss": 4.9294,
      "grad_norm": 2.548335552215576,
      "learning_rate": 1.149695964621338e-05,
      "epoch": 17.24709784411277
    },
    {
      "eval_loss": 4.848678112030029,
      "eval_runtime": 44.7996,
      "eval_samples_per_second": 92.166,
      "eval_steps_per_second": 5.781,
      "epoch": 17.24709784411277
    },
    {
      "loss": 4.9167,
      "grad_norm": 2.2593352794647217,
      "learning_rate": 1.155223880597015e-05,
      "epoch": 17.330016583747927
    },
    {
      "loss": 4.8912,
      "grad_norm": 2.3336780071258545,
      "learning_rate": 1.1607517965726922e-05,
      "epoch": 17.412935323383085
    },
    {
      "eval_loss": 4.827264308929443,
      "eval_runtime": 44.8413,
      "eval_samples_per_second": 92.08,
      "eval_steps_per_second": 5.776,
      "epoch": 17.412935323383085
    },
    {
      "loss": 4.8924,
      "grad_norm": 2.541323661804199,
      "learning_rate": 1.1662797125483694e-05,
      "epoch": 17.495854063018243
    },
    {
      "loss": 4.8747,
      "grad_norm": 2.4015250205993652,
      "learning_rate": 1.1718076285240466e-05,
      "epoch": 17.5787728026534
    },
    {
      "eval_loss": 4.796995639801025,
      "eval_runtime": 44.8743,
      "eval_samples_per_second": 92.013,
      "eval_steps_per_second": 5.772,
      "epoch": 17.5787728026534
    },
    {
      "loss": 4.888,
      "grad_norm": 2.588642120361328,
      "learning_rate": 1.1773355444997236e-05,
      "epoch": 17.66169154228856
    },
    {
      "loss": 4.8749,
      "grad_norm": 2.5440609455108643,
      "learning_rate": 1.1828634604754008e-05,
      "epoch": 17.744610281923716
    },
    {
      "eval_loss": 4.76686429977417,
      "eval_runtime": 44.765,
      "eval_samples_per_second": 92.237,
      "eval_steps_per_second": 5.786,
      "epoch": 17.744610281923716
    },
    {
      "loss": 4.8883,
      "grad_norm": 2.641991376876831,
      "learning_rate": 1.188391376451078e-05,
      "epoch": 17.827529021558874
    },
    {
      "loss": 4.8599,
      "grad_norm": 2.532076597213745,
      "learning_rate": 1.1939192924267553e-05,
      "epoch": 17.91044776119403
    },
    {
      "eval_loss": 4.754540920257568,
      "eval_runtime": 44.937,
      "eval_samples_per_second": 91.884,
      "eval_steps_per_second": 5.764,
      "epoch": 17.91044776119403
    },
    {
      "loss": 4.8099,
      "grad_norm": 2.6154420375823975,
      "learning_rate": 1.1994472084024325e-05,
      "epoch": 17.993366500829186
    },
    {
      "loss": 4.8229,
      "grad_norm": 2.464808225631714,
      "learning_rate": 1.2049751243781095e-05,
      "epoch": 18.076285240464344
    },
    {
      "eval_loss": 4.734476089477539,
      "eval_runtime": 44.8357,
      "eval_samples_per_second": 92.092,
      "eval_steps_per_second": 5.777,
      "epoch": 18.076285240464344
    },
    {
      "loss": 4.8226,
      "grad_norm": 2.484250545501709,
      "learning_rate": 1.2105030403537867e-05,
      "epoch": 18.1592039800995
    },
    {
      "loss": 4.8103,
      "grad_norm": 2.596205711364746,
      "learning_rate": 1.2160309563294639e-05,
      "epoch": 18.24212271973466
    },
    {
      "eval_loss": 4.730249404907227,
      "eval_runtime": 44.7942,
      "eval_samples_per_second": 92.177,
      "eval_steps_per_second": 5.782,
      "epoch": 18.24212271973466
    },
    {
      "loss": 4.7602,
      "grad_norm": 2.465076208114624,
      "learning_rate": 1.2215588723051411e-05,
      "epoch": 18.325041459369817
    },
    {
      "loss": 4.7722,
      "grad_norm": 2.3708977699279785,
      "learning_rate": 1.2270867882808181e-05,
      "epoch": 18.407960199004975
    },
    {
      "eval_loss": 4.713449954986572,
      "eval_runtime": 44.8743,
      "eval_samples_per_second": 92.013,
      "eval_steps_per_second": 5.772,
      "epoch": 18.407960199004975
    },
    {
      "loss": 4.7811,
      "grad_norm": 2.4525890350341797,
      "learning_rate": 1.2326147042564953e-05,
      "epoch": 18.490878938640133
    },
    {
      "loss": 4.7656,
      "grad_norm": 2.4264073371887207,
      "learning_rate": 1.2381426202321725e-05,
      "epoch": 18.57379767827529
    },
    {
      "eval_loss": 4.6951093673706055,
      "eval_runtime": 44.6531,
      "eval_samples_per_second": 92.468,
      "eval_steps_per_second": 5.8,
      "epoch": 18.57379767827529
    },
    {
      "loss": 4.7306,
      "grad_norm": 2.602304220199585,
      "learning_rate": 1.2436705362078498e-05,
      "epoch": 18.65671641791045
    },
    {
      "loss": 4.7215,
      "grad_norm": 2.381988286972046,
      "learning_rate": 1.249198452183527e-05,
      "epoch": 18.739635157545607
    },
    {
      "eval_loss": 4.664693355560303,
      "eval_runtime": 44.8778,
      "eval_samples_per_second": 92.005,
      "eval_steps_per_second": 5.771,
      "epoch": 18.739635157545607
    },
    {
      "loss": 4.7437,
      "grad_norm": 2.3801825046539307,
      "learning_rate": 1.254726368159204e-05,
      "epoch": 18.822553897180764
    },
    {
      "loss": 4.7055,
      "grad_norm": 2.42865252494812,
      "learning_rate": 1.2602542841348812e-05,
      "epoch": 18.90547263681592
    },
    {
      "eval_loss": 4.652712345123291,
      "eval_runtime": 44.8727,
      "eval_samples_per_second": 92.016,
      "eval_steps_per_second": 5.772,
      "epoch": 18.90547263681592
    },
    {
      "loss": 4.7336,
      "grad_norm": 2.500257730484009,
      "learning_rate": 1.2657822001105584e-05,
      "epoch": 18.988391376451077
    },
    {
      "loss": 4.7017,
      "grad_norm": 2.42043399810791,
      "learning_rate": 1.2713101160862356e-05,
      "epoch": 19.071310116086234
    },
    {
      "eval_loss": 4.634620189666748,
      "eval_runtime": 44.8834,
      "eval_samples_per_second": 91.994,
      "eval_steps_per_second": 5.771,
      "epoch": 19.071310116086234
    },
    {
      "loss": 4.7151,
      "grad_norm": 2.499932289123535,
      "learning_rate": 1.2768380320619126e-05,
      "epoch": 19.154228855721392
    },
    {
      "loss": 4.6865,
      "grad_norm": 2.4089722633361816,
      "learning_rate": 1.2823659480375898e-05,
      "epoch": 19.23714759535655
    },
    {
      "eval_loss": 4.61090612411499,
      "eval_runtime": 44.8072,
      "eval_samples_per_second": 92.15,
      "eval_steps_per_second": 5.78,
      "epoch": 19.23714759535655
    },
    {
      "loss": 4.6668,
      "grad_norm": 2.4539783000946045,
      "learning_rate": 1.2878938640132672e-05,
      "epoch": 19.320066334991708
    },
    {
      "loss": 4.6016,
      "grad_norm": 2.5757546424865723,
      "learning_rate": 1.2934217799889444e-05,
      "epoch": 19.402985074626866
    },
    {
      "eval_loss": 4.596757888793945,
      "eval_runtime": 44.8585,
      "eval_samples_per_second": 92.045,
      "eval_steps_per_second": 5.774,
      "epoch": 19.402985074626866
    },
    {
      "loss": 4.6664,
      "grad_norm": 2.386625289916992,
      "learning_rate": 1.2989496959646215e-05,
      "epoch": 19.485903814262024
    },
    {
      "loss": 4.6324,
      "grad_norm": 2.61678147315979,
      "learning_rate": 1.3044776119402987e-05,
      "epoch": 19.56882255389718
    },
    {
      "eval_loss": 4.58631706237793,
      "eval_runtime": 44.874,
      "eval_samples_per_second": 92.013,
      "eval_steps_per_second": 5.772,
      "epoch": 19.56882255389718
    },
    {
      "loss": 4.637,
      "grad_norm": 2.473463773727417,
      "learning_rate": 1.3100055279159757e-05,
      "epoch": 19.65174129353234
    },
    {
      "loss": 4.6472,
      "grad_norm": 2.4244322776794434,
      "learning_rate": 1.315533443891653e-05,
      "epoch": 19.734660033167497
    },
    {
      "eval_loss": 4.557436943054199,
      "eval_runtime": 44.8597,
      "eval_samples_per_second": 92.043,
      "eval_steps_per_second": 5.774,
      "epoch": 19.734660033167497
    },
    {
      "loss": 4.6046,
      "grad_norm": 2.4534265995025635,
      "learning_rate": 1.3210613598673301e-05,
      "epoch": 19.817578772802655
    },
    {
      "loss": 4.6078,
      "grad_norm": 2.5532517433166504,
      "learning_rate": 1.3265892758430073e-05,
      "epoch": 19.90049751243781
    },
    {
      "eval_loss": 4.532100677490234,
      "eval_runtime": 44.7839,
      "eval_samples_per_second": 92.198,
      "eval_steps_per_second": 5.783,
      "epoch": 19.90049751243781
    },
    {
      "loss": 4.5704,
      "grad_norm": 2.3849074840545654,
      "learning_rate": 1.3321171918186843e-05,
      "epoch": 19.983416252072967
    },
    {
      "loss": 4.5874,
      "grad_norm": 2.422785758972168,
      "learning_rate": 1.3376451077943617e-05,
      "epoch": 20.066334991708125
    },
    {
      "eval_loss": 4.524337291717529,
      "eval_runtime": 44.9724,
      "eval_samples_per_second": 91.812,
      "eval_steps_per_second": 5.759,
      "epoch": 20.066334991708125
    },
    {
      "loss": 4.6008,
      "grad_norm": 2.4619555473327637,
      "learning_rate": 1.3431730237700389e-05,
      "epoch": 20.149253731343283
    },
    {
      "loss": 4.591,
      "grad_norm": 2.5759036540985107,
      "learning_rate": 1.348700939745716e-05,
      "epoch": 20.23217247097844
    },
    {
      "eval_loss": 4.499527931213379,
      "eval_runtime": 44.8701,
      "eval_samples_per_second": 92.021,
      "eval_steps_per_second": 5.772,
      "epoch": 20.23217247097844
    },
    {
      "loss": 4.5503,
      "grad_norm": 2.7162184715270996,
      "learning_rate": 1.3542288557213932e-05,
      "epoch": 20.3150912106136
    },
    {
      "loss": 4.5631,
      "grad_norm": 2.331199884414673,
      "learning_rate": 1.3597567716970703e-05,
      "epoch": 20.398009950248756
    },
    {
      "eval_loss": 4.493046760559082,
      "eval_runtime": 44.8124,
      "eval_samples_per_second": 92.14,
      "eval_steps_per_second": 5.78,
      "epoch": 20.398009950248756
    },
    {
      "loss": 4.5501,
      "grad_norm": 2.4275221824645996,
      "learning_rate": 1.3652846876727474e-05,
      "epoch": 20.480928689883914
    },
    {
      "loss": 4.5443,
      "grad_norm": 2.5075459480285645,
      "learning_rate": 1.3708126036484246e-05,
      "epoch": 20.563847429519072
    },
    {
      "eval_loss": 4.4688825607299805,
      "eval_runtime": 44.8619,
      "eval_samples_per_second": 92.038,
      "eval_steps_per_second": 5.773,
      "epoch": 20.563847429519072
    },
    {
      "loss": 4.5519,
      "grad_norm": 2.3960485458374023,
      "learning_rate": 1.3763405196241018e-05,
      "epoch": 20.64676616915423
    },
    {
      "loss": 4.5067,
      "grad_norm": 2.375393867492676,
      "learning_rate": 1.3818684355997788e-05,
      "epoch": 20.729684908789388
    },
    {
      "eval_loss": 4.452094078063965,
      "eval_runtime": 44.8402,
      "eval_samples_per_second": 92.082,
      "eval_steps_per_second": 5.776,
      "epoch": 20.729684908789388
    },
    {
      "loss": 4.4879,
      "grad_norm": 2.4964687824249268,
      "learning_rate": 1.3873963515754562e-05,
      "epoch": 20.812603648424545
    },
    {
      "loss": 4.4871,
      "grad_norm": 2.486602783203125,
      "learning_rate": 1.3929242675511334e-05,
      "epoch": 20.895522388059703
    },
    {
      "eval_loss": 4.441656112670898,
      "eval_runtime": 44.9335,
      "eval_samples_per_second": 91.891,
      "eval_steps_per_second": 5.764,
      "epoch": 20.895522388059703
    },
    {
      "loss": 4.5046,
      "grad_norm": 2.3462483882904053,
      "learning_rate": 1.3984521835268106e-05,
      "epoch": 20.978441127694857
    },
    {
      "loss": 4.483,
      "grad_norm": 2.2760062217712402,
      "learning_rate": 1.4039800995024878e-05,
      "epoch": 21.061359867330015
    },
    {
      "eval_loss": 4.434301376342773,
      "eval_runtime": 44.855,
      "eval_samples_per_second": 92.052,
      "eval_steps_per_second": 5.774,
      "epoch": 21.061359867330015
    },
    {
      "loss": 4.4821,
      "grad_norm": 2.344048023223877,
      "learning_rate": 1.4095080154781648e-05,
      "epoch": 21.144278606965173
    },
    {
      "loss": 4.4594,
      "grad_norm": 2.6548593044281006,
      "learning_rate": 1.415035931453842e-05,
      "epoch": 21.22719734660033
    },
    {
      "eval_loss": 4.410249710083008,
      "eval_runtime": 44.8399,
      "eval_samples_per_second": 92.083,
      "eval_steps_per_second": 5.776,
      "epoch": 21.22719734660033
    },
    {
      "loss": 4.4595,
      "grad_norm": 2.5257480144500732,
      "learning_rate": 1.4205638474295191e-05,
      "epoch": 21.31011608623549
    },
    {
      "loss": 4.4443,
      "grad_norm": 2.4036812782287598,
      "learning_rate": 1.4260917634051963e-05,
      "epoch": 21.393034825870647
    },
    {
      "eval_loss": 4.389793395996094,
      "eval_runtime": 44.8741,
      "eval_samples_per_second": 92.013,
      "eval_steps_per_second": 5.772,
      "epoch": 21.393034825870647
    },
    {
      "loss": 4.48,
      "grad_norm": 2.643904447555542,
      "learning_rate": 1.4316196793808733e-05,
      "epoch": 21.475953565505804
    },
    {
      "loss": 4.4306,
      "grad_norm": 2.3303933143615723,
      "learning_rate": 1.4371475953565509e-05,
      "epoch": 21.558872305140962
    },
    {
      "eval_loss": 4.386097431182861,
      "eval_runtime": 44.9182,
      "eval_samples_per_second": 91.923,
      "eval_steps_per_second": 5.766,
      "epoch": 21.558872305140962
    },
    {
      "loss": 4.4462,
      "grad_norm": 2.4412693977355957,
      "learning_rate": 1.4426755113322279e-05,
      "epoch": 21.64179104477612
    },
    {
      "loss": 4.4755,
      "grad_norm": 2.5598995685577393,
      "learning_rate": 1.448203427307905e-05,
      "epoch": 21.724709784411278
    },
    {
      "eval_loss": 4.369365215301514,
      "eval_runtime": 44.9298,
      "eval_samples_per_second": 91.899,
      "eval_steps_per_second": 5.765,
      "epoch": 21.724709784411278
    },
    {
      "loss": 4.4005,
      "grad_norm": 2.3902432918548584,
      "learning_rate": 1.4537313432835823e-05,
      "epoch": 21.807628524046436
    },
    {
      "loss": 4.4086,
      "grad_norm": 2.4452872276306152,
      "learning_rate": 1.4592592592592594e-05,
      "epoch": 21.890547263681594
    },
    {
      "eval_loss": 4.351761341094971,
      "eval_runtime": 44.9733,
      "eval_samples_per_second": 91.81,
      "eval_steps_per_second": 5.759,
      "epoch": 21.890547263681594
    },
    {
      "loss": 4.361,
      "grad_norm": 2.489471435546875,
      "learning_rate": 1.4647871752349365e-05,
      "epoch": 21.973466003316748
    },
    {
      "loss": 4.3933,
      "grad_norm": 2.494231939315796,
      "learning_rate": 1.4703150912106137e-05,
      "epoch": 22.056384742951906
    },
    {
      "eval_loss": 4.321926593780518,
      "eval_runtime": 44.8499,
      "eval_samples_per_second": 92.063,
      "eval_steps_per_second": 5.775,
      "epoch": 22.056384742951906
    },
    {
      "loss": 4.3772,
      "grad_norm": 2.6109962463378906,
      "learning_rate": 1.4758430071862908e-05,
      "epoch": 22.139303482587064
    },
    {
      "loss": 4.349,
      "grad_norm": 2.4000630378723145,
      "learning_rate": 1.481370923161968e-05,
      "epoch": 22.22222222222222
    },
    {
      "eval_loss": 4.320221424102783,
      "eval_runtime": 44.7122,
      "eval_samples_per_second": 92.346,
      "eval_steps_per_second": 5.793,
      "epoch": 22.22222222222222
    },
    {
      "loss": 4.3928,
      "grad_norm": 2.358504295349121,
      "learning_rate": 1.4868988391376454e-05,
      "epoch": 22.30514096185738
    },
    {
      "loss": 4.3824,
      "grad_norm": 2.430241346359253,
      "learning_rate": 1.4924267551133224e-05,
      "epoch": 22.388059701492537
    },
    {
      "eval_loss": 4.302127361297607,
      "eval_runtime": 44.8248,
      "eval_samples_per_second": 92.114,
      "eval_steps_per_second": 5.778,
      "epoch": 22.388059701492537
    },
    {
      "loss": 4.3262,
      "grad_norm": 2.447984218597412,
      "learning_rate": 1.4979546710889996e-05,
      "epoch": 22.470978441127695
    },
    {
      "loss": 4.3347,
      "grad_norm": 2.3876588344573975,
      "learning_rate": 1.5034825870646768e-05,
      "epoch": 22.553897180762853
    },
    {
      "eval_loss": 4.27600622177124,
      "eval_runtime": 44.7959,
      "eval_samples_per_second": 92.174,
      "eval_steps_per_second": 5.782,
      "epoch": 22.553897180762853
    },
    {
      "loss": 4.3795,
      "grad_norm": 2.3419296741485596,
      "learning_rate": 1.509010503040354e-05,
      "epoch": 22.63681592039801
    },
    {
      "loss": 4.3074,
      "grad_norm": 2.5417256355285645,
      "learning_rate": 1.514538419016031e-05,
      "epoch": 22.71973466003317
    },
    {
      "eval_loss": 4.26438570022583,
      "eval_runtime": 44.9437,
      "eval_samples_per_second": 91.871,
      "eval_steps_per_second": 5.763,
      "epoch": 22.71973466003317
    },
    {
      "loss": 4.2926,
      "grad_norm": 2.4196600914001465,
      "learning_rate": 1.5200663349917082e-05,
      "epoch": 22.802653399668326
    },
    {
      "loss": 4.2922,
      "grad_norm": 2.614643096923828,
      "learning_rate": 1.5255942509673854e-05,
      "epoch": 22.885572139303484
    },
    {
      "eval_loss": 4.265533924102783,
      "eval_runtime": 44.8926,
      "eval_samples_per_second": 91.975,
      "eval_steps_per_second": 5.769,
      "epoch": 22.885572139303484
    },
    {
      "loss": 4.2843,
      "grad_norm": 2.449038028717041,
      "learning_rate": 1.5311221669430624e-05,
      "epoch": 22.96849087893864
    },
    {
      "loss": 4.2761,
      "grad_norm": 2.5855932235717773,
      "learning_rate": 1.53665008291874e-05,
      "epoch": 23.051409618573796
    },
    {
      "eval_loss": 4.254349231719971,
      "eval_runtime": 44.768,
      "eval_samples_per_second": 92.231,
      "eval_steps_per_second": 5.785,
      "epoch": 23.051409618573796
    },
    {
      "loss": 4.2782,
      "grad_norm": 2.552445888519287,
      "learning_rate": 1.542177998894417e-05,
      "epoch": 23.134328358208954
    },
    {
      "loss": 4.2266,
      "grad_norm": 2.7205705642700195,
      "learning_rate": 1.5477059148700943e-05,
      "epoch": 23.217247097844112
    },
    {
      "eval_loss": 4.220759391784668,
      "eval_runtime": 44.8601,
      "eval_samples_per_second": 92.042,
      "eval_steps_per_second": 5.774,
      "epoch": 23.217247097844112
    },
    {
      "loss": 4.2641,
      "grad_norm": 2.3774492740631104,
      "learning_rate": 1.553233830845771e-05,
      "epoch": 23.30016583747927
    },
    {
      "loss": 4.266,
      "grad_norm": 2.447430372238159,
      "learning_rate": 1.5587617468214483e-05,
      "epoch": 23.383084577114428
    },
    {
      "eval_loss": 4.222668647766113,
      "eval_runtime": 44.7638,
      "eval_samples_per_second": 92.24,
      "eval_steps_per_second": 5.786,
      "epoch": 23.383084577114428
    },
    {
      "loss": 4.2599,
      "grad_norm": 2.397613763809204,
      "learning_rate": 1.5642896627971255e-05,
      "epoch": 23.466003316749585
    },
    {
      "loss": 4.2743,
      "grad_norm": 2.5649847984313965,
      "learning_rate": 1.5698175787728027e-05,
      "epoch": 23.548922056384743
    },
    {
      "eval_loss": 4.185118675231934,
      "eval_runtime": 44.9008,
      "eval_samples_per_second": 91.958,
      "eval_steps_per_second": 5.768,
      "epoch": 23.548922056384743
    },
    {
      "loss": 4.2206,
      "grad_norm": 2.497708559036255,
      "learning_rate": 1.57534549474848e-05,
      "epoch": 23.6318407960199
    },
    {
      "loss": 4.2428,
      "grad_norm": 2.4241695404052734,
      "learning_rate": 1.5808734107241574e-05,
      "epoch": 23.71475953565506
    },
    {
      "eval_loss": 4.180415153503418,
      "eval_runtime": 44.9672,
      "eval_samples_per_second": 91.823,
      "eval_steps_per_second": 5.76,
      "epoch": 23.71475953565506
    },
    {
      "loss": 4.2253,
      "grad_norm": 2.4780423641204834,
      "learning_rate": 1.5864013266998342e-05,
      "epoch": 23.797678275290217
    },
    {
      "loss": 4.2378,
      "grad_norm": 2.5025904178619385,
      "learning_rate": 1.5919292426755114e-05,
      "epoch": 23.880597014925375
    },
    {
      "eval_loss": 4.173740863800049,
      "eval_runtime": 44.8814,
      "eval_samples_per_second": 91.998,
      "eval_steps_per_second": 5.771,
      "epoch": 23.880597014925375
    },
    {
      "loss": 4.213,
      "grad_norm": 2.4291787147521973,
      "learning_rate": 1.5974571586511886e-05,
      "epoch": 23.963515754560532
    },
    {
      "loss": 4.1813,
      "grad_norm": 2.323483467102051,
      "learning_rate": 1.6029850746268658e-05,
      "epoch": 24.046434494195687
    },
    {
      "eval_loss": 4.152820110321045,
      "eval_runtime": 44.7893,
      "eval_samples_per_second": 92.187,
      "eval_steps_per_second": 5.783,
      "epoch": 24.046434494195687
    },
    {
      "loss": 4.2236,
      "grad_norm": 2.334641218185425,
      "learning_rate": 1.608512990602543e-05,
      "epoch": 24.129353233830845
    },
    {
      "loss": 4.206,
      "grad_norm": 2.330864906311035,
      "learning_rate": 1.6140409065782202e-05,
      "epoch": 24.212271973466002
    },
    {
      "eval_loss": 4.138526916503906,
      "eval_runtime": 44.8747,
      "eval_samples_per_second": 92.012,
      "eval_steps_per_second": 5.772,
      "epoch": 24.212271973466002
    },
    {
      "loss": 4.1684,
      "grad_norm": 2.3944478034973145,
      "learning_rate": 1.6195688225538974e-05,
      "epoch": 24.29519071310116
    },
    {
      "loss": 4.151,
      "grad_norm": 2.443751335144043,
      "learning_rate": 1.6250967385295745e-05,
      "epoch": 24.378109452736318
    },
    {
      "eval_loss": 4.115542888641357,
      "eval_runtime": 44.8335,
      "eval_samples_per_second": 92.096,
      "eval_steps_per_second": 5.777,
      "epoch": 24.378109452736318
    },
    {
      "loss": 4.1365,
      "grad_norm": 2.370575428009033,
      "learning_rate": 1.6306246545052517e-05,
      "epoch": 24.461028192371476
    },
    {
      "loss": 4.1679,
      "grad_norm": 2.611891269683838,
      "learning_rate": 1.636152570480929e-05,
      "epoch": 24.543946932006634
    },
    {
      "eval_loss": 4.125391483306885,
      "eval_runtime": 44.8082,
      "eval_samples_per_second": 92.148,
      "eval_steps_per_second": 5.78,
      "epoch": 24.543946932006634
    },
    {
      "loss": 4.1295,
      "grad_norm": 2.297877788543701,
      "learning_rate": 1.641680486456606e-05,
      "epoch": 24.62686567164179
    },
    {
      "loss": 4.1643,
      "grad_norm": 2.6132757663726807,
      "learning_rate": 1.6472084024322833e-05,
      "epoch": 24.70978441127695
    },
    {
      "eval_loss": 4.115341663360596,
      "eval_runtime": 44.7577,
      "eval_samples_per_second": 92.252,
      "eval_steps_per_second": 5.787,
      "epoch": 24.70978441127695
    },
    {
      "loss": 4.1413,
      "grad_norm": 2.537510633468628,
      "learning_rate": 1.6527363184079605e-05,
      "epoch": 24.792703150912107
    },
    {
      "loss": 4.1437,
      "grad_norm": 2.390181064605713,
      "learning_rate": 1.6582642343836373e-05,
      "epoch": 24.875621890547265
    },
    {
      "eval_loss": 4.07948637008667,
      "eval_runtime": 44.893,
      "eval_samples_per_second": 91.974,
      "eval_steps_per_second": 5.769,
      "epoch": 24.875621890547265
    },
    {
      "loss": 4.1395,
      "grad_norm": 2.493807554244995,
      "learning_rate": 1.6637921503593145e-05,
      "epoch": 24.958540630182423
    },
    {
      "loss": 4.1002,
      "grad_norm": 2.3271546363830566,
      "learning_rate": 1.6693200663349917e-05,
      "epoch": 25.041459369817577
    },
    {
      "eval_loss": 4.091250896453857,
      "eval_runtime": 44.8089,
      "eval_samples_per_second": 92.147,
      "eval_steps_per_second": 5.78,
      "epoch": 25.041459369817577
    },
    {
      "loss": 4.1018,
      "grad_norm": 2.309774875640869,
      "learning_rate": 1.674847982310669e-05,
      "epoch": 25.124378109452735
    },
    {
      "loss": 4.1104,
      "grad_norm": 2.3431193828582764,
      "learning_rate": 1.6803758982863464e-05,
      "epoch": 25.207296849087893
    },
    {
      "eval_loss": 4.061549186706543,
      "eval_runtime": 44.8269,
      "eval_samples_per_second": 92.11,
      "eval_steps_per_second": 5.778,
      "epoch": 25.207296849087893
    },
    {
      "loss": 4.0903,
      "grad_norm": 2.4706239700317383,
      "learning_rate": 1.6859038142620233e-05,
      "epoch": 25.29021558872305
    },
    {
      "loss": 4.101,
      "grad_norm": 2.3763668537139893,
      "learning_rate": 1.6914317302377004e-05,
      "epoch": 25.37313432835821
    },
    {
      "eval_loss": 4.060634613037109,
      "eval_runtime": 44.8367,
      "eval_samples_per_second": 92.09,
      "eval_steps_per_second": 5.777,
      "epoch": 25.37313432835821
    },
    {
      "loss": 4.0654,
      "grad_norm": 2.383195161819458,
      "learning_rate": 1.6969596462133776e-05,
      "epoch": 25.456053067993366
    },
    {
      "loss": 4.0643,
      "grad_norm": 2.4399983882904053,
      "learning_rate": 1.7024875621890548e-05,
      "epoch": 25.538971807628524
    },
    {
      "eval_loss": 4.0503926277160645,
      "eval_runtime": 44.8088,
      "eval_samples_per_second": 92.147,
      "eval_steps_per_second": 5.78,
      "epoch": 25.538971807628524
    },
    {
      "loss": 4.0954,
      "grad_norm": 2.368049144744873,
      "learning_rate": 1.708015478164732e-05,
      "epoch": 25.621890547263682
    },
    {
      "loss": 4.0769,
      "grad_norm": 2.428744316101074,
      "learning_rate": 1.7135433941404092e-05,
      "epoch": 25.70480928689884
    },
    {
      "eval_loss": 4.029573440551758,
      "eval_runtime": 45.3194,
      "eval_samples_per_second": 91.109,
      "eval_steps_per_second": 5.715,
      "epoch": 25.70480928689884
    },
    {
      "loss": 4.0907,
      "grad_norm": 2.3882944583892822,
      "learning_rate": 1.7190713101160864e-05,
      "epoch": 25.787728026533998
    },
    {
      "loss": 4.0047,
      "grad_norm": 2.29382061958313,
      "learning_rate": 1.7245992260917636e-05,
      "epoch": 25.870646766169155
    },
    {
      "eval_loss": 4.021984577178955,
      "eval_runtime": 44.9542,
      "eval_samples_per_second": 91.849,
      "eval_steps_per_second": 5.761,
      "epoch": 25.870646766169155
    },
    {
      "loss": 4.0249,
      "grad_norm": 2.3886849880218506,
      "learning_rate": 1.7301271420674408e-05,
      "epoch": 25.953565505804313
    },
    {
      "loss": 4.0378,
      "grad_norm": 2.1973612308502197,
      "learning_rate": 1.735655058043118e-05,
      "epoch": 26.036484245439468
    },
    {
      "eval_loss": 3.9922399520874023,
      "eval_runtime": 45.0276,
      "eval_samples_per_second": 91.699,
      "eval_steps_per_second": 5.752,
      "epoch": 26.036484245439468
    },
    {
      "loss": 4.0591,
      "grad_norm": 2.289424180984497,
      "learning_rate": 1.741182974018795e-05,
      "epoch": 26.119402985074625
    },
    {
      "loss": 4.0087,
      "grad_norm": 2.5589821338653564,
      "learning_rate": 1.7467108899944723e-05,
      "epoch": 26.202321724709783
    },
    {
      "eval_loss": 3.9869275093078613,
      "eval_runtime": 45.0365,
      "eval_samples_per_second": 91.681,
      "eval_steps_per_second": 5.751,
      "epoch": 26.202321724709783
    },
    {
      "loss": 4.0401,
      "grad_norm": 2.3648934364318848,
      "learning_rate": 1.7522388059701495e-05,
      "epoch": 26.28524046434494
    },
    {
      "loss": 3.9866,
      "grad_norm": 2.6145639419555664,
      "learning_rate": 1.7577667219458267e-05,
      "epoch": 26.3681592039801
    },
    {
      "eval_loss": 3.9877068996429443,
      "eval_runtime": 46.4577,
      "eval_samples_per_second": 88.877,
      "eval_steps_per_second": 5.575,
      "epoch": 26.3681592039801
    },
    {
      "loss": 4.0074,
      "grad_norm": 2.3682613372802734,
      "learning_rate": 1.7632946379215035e-05,
      "epoch": 26.451077943615257
    },
    {
      "loss": 3.9988,
      "grad_norm": 2.376734495162964,
      "learning_rate": 1.7688225538971807e-05,
      "epoch": 26.533996683250415
    },
    {
      "eval_loss": 3.9678125381469727,
      "eval_runtime": 45.0091,
      "eval_samples_per_second": 91.737,
      "eval_steps_per_second": 5.754,
      "epoch": 26.533996683250415
    },
    {
      "loss": 3.9721,
      "grad_norm": 2.483973264694214,
      "learning_rate": 1.774350469872858e-05,
      "epoch": 26.616915422885572
    },
    {
      "loss": 4.0282,
      "grad_norm": 2.428921937942505,
      "learning_rate": 1.7798783858485354e-05,
      "epoch": 26.69983416252073
    },
    {
      "eval_loss": 3.9659388065338135,
      "eval_runtime": 44.8424,
      "eval_samples_per_second": 92.078,
      "eval_steps_per_second": 5.776,
      "epoch": 26.69983416252073
    },
    {
      "loss": 3.9959,
      "grad_norm": 2.352424144744873,
      "learning_rate": 1.7854063018242126e-05,
      "epoch": 26.782752902155888
    },
    {
      "loss": 3.9968,
      "grad_norm": 2.385869026184082,
      "learning_rate": 1.7909342177998895e-05,
      "epoch": 26.865671641791046
    },
    {
      "eval_loss": 3.936940908432007,
      "eval_runtime": 44.7397,
      "eval_samples_per_second": 92.289,
      "eval_steps_per_second": 5.789,
      "epoch": 26.865671641791046
    },
    {
      "loss": 3.9536,
      "grad_norm": 2.3331429958343506,
      "learning_rate": 1.7964621337755667e-05,
      "epoch": 26.948590381426204
    },
    {
      "loss": 3.942,
      "grad_norm": 2.5928118228912354,
      "learning_rate": 1.801990049751244e-05,
      "epoch": 27.03150912106136
    },
    {
      "eval_loss": 3.933619737625122,
      "eval_runtime": 44.9501,
      "eval_samples_per_second": 91.857,
      "eval_steps_per_second": 5.762,
      "epoch": 27.03150912106136
    },
    {
      "loss": 3.946,
      "grad_norm": 2.4168989658355713,
      "learning_rate": 1.807517965726921e-05,
      "epoch": 27.114427860696516
    },
    {
      "loss": 3.9567,
      "grad_norm": 2.643573760986328,
      "learning_rate": 1.8130458817025982e-05,
      "epoch": 27.197346600331674
    },
    {
      "eval_loss": 3.935817003250122,
      "eval_runtime": 44.8749,
      "eval_samples_per_second": 92.011,
      "eval_steps_per_second": 5.772,
      "epoch": 27.197346600331674
    },
    {
      "loss": 3.9451,
      "grad_norm": 2.417168617248535,
      "learning_rate": 1.8185737976782754e-05,
      "epoch": 27.28026533996683
    },
    {
      "loss": 3.9509,
      "grad_norm": 2.493354558944702,
      "learning_rate": 1.8241017136539526e-05,
      "epoch": 27.36318407960199
    },
    {
      "eval_loss": 3.910212993621826,
      "eval_runtime": 44.875,
      "eval_samples_per_second": 92.011,
      "eval_steps_per_second": 5.772,
      "epoch": 27.36318407960199
    },
    {
      "loss": 3.9256,
      "grad_norm": 2.425187587738037,
      "learning_rate": 1.8296296296296298e-05,
      "epoch": 27.446102819237147
    },
    {
      "loss": 3.9313,
      "grad_norm": 2.413353443145752,
      "learning_rate": 1.835157545605307e-05,
      "epoch": 27.529021558872305
    },
    {
      "eval_loss": 3.905930280685425,
      "eval_runtime": 44.7404,
      "eval_samples_per_second": 92.288,
      "eval_steps_per_second": 5.789,
      "epoch": 27.529021558872305
    },
    {
      "loss": 3.9203,
      "grad_norm": 2.3711278438568115,
      "learning_rate": 1.840685461580984e-05,
      "epoch": 27.611940298507463
    },
    {
      "loss": 3.8963,
      "grad_norm": 2.425295352935791,
      "learning_rate": 1.8462133775566613e-05,
      "epoch": 27.69485903814262
    },
    {
      "eval_loss": 3.883333683013916,
      "eval_runtime": 44.9112,
      "eval_samples_per_second": 91.937,
      "eval_steps_per_second": 5.767,
      "epoch": 27.69485903814262
    },
    {
      "loss": 3.9136,
      "grad_norm": 2.3475637435913086,
      "learning_rate": 1.8517412935323385e-05,
      "epoch": 27.77777777777778
    },
    {
      "loss": 3.8807,
      "grad_norm": 2.340080499649048,
      "learning_rate": 1.8572692095080157e-05,
      "epoch": 27.860696517412936
    },
    {
      "eval_loss": 3.872774839401245,
      "eval_runtime": 44.8641,
      "eval_samples_per_second": 92.033,
      "eval_steps_per_second": 5.773,
      "epoch": 27.860696517412936
    },
    {
      "loss": 3.9076,
      "grad_norm": 2.5394327640533447,
      "learning_rate": 1.862797125483693e-05,
      "epoch": 27.943615257048094
    },
    {
      "loss": 3.8835,
      "grad_norm": 2.401271104812622,
      "learning_rate": 1.8683250414593697e-05,
      "epoch": 28.026533996683252
    },
    {
      "eval_loss": 3.868746519088745,
      "eval_runtime": 44.7935,
      "eval_samples_per_second": 92.179,
      "eval_steps_per_second": 5.782,
      "epoch": 28.026533996683252
    },
    {
      "loss": 3.8797,
      "grad_norm": 2.400451898574829,
      "learning_rate": 1.8738529574350473e-05,
      "epoch": 28.109452736318406
    },
    {
      "loss": 3.8614,
      "grad_norm": 2.5798561573028564,
      "learning_rate": 1.8793808734107245e-05,
      "epoch": 28.192371475953564
    },
    {
      "eval_loss": 3.8724300861358643,
      "eval_runtime": 44.9315,
      "eval_samples_per_second": 91.895,
      "eval_steps_per_second": 5.764,
      "epoch": 28.192371475953564
    },
    {
      "loss": 3.903,
      "grad_norm": 2.348581552505493,
      "learning_rate": 1.8849087893864016e-05,
      "epoch": 28.275290215588722
    },
    {
      "loss": 3.8487,
      "grad_norm": 2.3866543769836426,
      "learning_rate": 1.890436705362079e-05,
      "epoch": 28.35820895522388
    },
    {
      "eval_loss": 3.8509654998779297,
      "eval_runtime": 44.7701,
      "eval_samples_per_second": 92.227,
      "eval_steps_per_second": 5.785,
      "epoch": 28.35820895522388
    },
    {
      "loss": 3.8679,
      "grad_norm": 2.449525833129883,
      "learning_rate": 1.8959646213377557e-05,
      "epoch": 28.441127694859038
    },
    {
      "loss": 3.8592,
      "grad_norm": 2.2881598472595215,
      "learning_rate": 1.901492537313433e-05,
      "epoch": 28.524046434494196
    },
    {
      "eval_loss": 3.8216724395751953,
      "eval_runtime": 44.8014,
      "eval_samples_per_second": 92.162,
      "eval_steps_per_second": 5.781,
      "epoch": 28.524046434494196
    },
    {
      "loss": 3.8392,
      "grad_norm": 2.4416239261627197,
      "learning_rate": 1.90702045328911e-05,
      "epoch": 28.606965174129353
    },
    {
      "loss": 3.8417,
      "grad_norm": 2.5665783882141113,
      "learning_rate": 1.9125483692647872e-05,
      "epoch": 28.68988391376451
    },
    {
      "eval_loss": 3.8185088634490967,
      "eval_runtime": 44.8692,
      "eval_samples_per_second": 92.023,
      "eval_steps_per_second": 5.772,
      "epoch": 28.68988391376451
    },
    {
      "loss": 3.8357,
      "grad_norm": 2.523000955581665,
      "learning_rate": 1.9180762852404644e-05,
      "epoch": 28.77280265339967
    },
    {
      "loss": 3.8196,
      "grad_norm": 2.4705467224121094,
      "learning_rate": 1.9236042012161416e-05,
      "epoch": 28.855721393034827
    },
    {
      "eval_loss": 3.822977066040039,
      "eval_runtime": 44.8996,
      "eval_samples_per_second": 91.961,
      "eval_steps_per_second": 5.768,
      "epoch": 28.855721393034827
    },
    {
      "loss": 3.825,
      "grad_norm": 2.459768056869507,
      "learning_rate": 1.9291321171918188e-05,
      "epoch": 28.938640132669985
    },
    {
      "loss": 3.8176,
      "grad_norm": 2.3627724647521973,
      "learning_rate": 1.934660033167496e-05,
      "epoch": 29.021558872305143
    },
    {
      "eval_loss": 3.7919721603393555,
      "eval_runtime": 44.7781,
      "eval_samples_per_second": 92.21,
      "eval_steps_per_second": 5.784,
      "epoch": 29.021558872305143
    },
    {
      "loss": 3.8239,
      "grad_norm": 2.442950487136841,
      "learning_rate": 1.9401879491431732e-05,
      "epoch": 29.104477611940297
    },
    {
      "loss": 3.7923,
      "grad_norm": 2.3955976963043213,
      "learning_rate": 1.9457158651188504e-05,
      "epoch": 29.187396351575455
    },
    {
      "eval_loss": 3.794698715209961,
      "eval_runtime": 44.9381,
      "eval_samples_per_second": 91.882,
      "eval_steps_per_second": 5.763,
      "epoch": 29.187396351575455
    },
    {
      "loss": 3.8137,
      "grad_norm": 2.4912803173065186,
      "learning_rate": 1.9512437810945276e-05,
      "epoch": 29.270315091210612
    },
    {
      "loss": 3.8083,
      "grad_norm": 2.420145034790039,
      "learning_rate": 1.9567716970702047e-05,
      "epoch": 29.35323383084577
    },
    {
      "eval_loss": 3.7908804416656494,
      "eval_runtime": 44.7215,
      "eval_samples_per_second": 92.327,
      "eval_steps_per_second": 5.791,
      "epoch": 29.35323383084577
    },
    {
      "loss": 3.7905,
      "grad_norm": 2.361961841583252,
      "learning_rate": 1.962299613045882e-05,
      "epoch": 29.436152570480928
    },
    {
      "loss": 3.792,
      "grad_norm": 2.3383665084838867,
      "learning_rate": 1.9678275290215588e-05,
      "epoch": 29.519071310116086
    },
    {
      "eval_loss": 3.7672083377838135,
      "eval_runtime": 45.0197,
      "eval_samples_per_second": 91.715,
      "eval_steps_per_second": 5.753,
      "epoch": 29.519071310116086
    },
    {
      "loss": 3.7625,
      "grad_norm": 2.3615784645080566,
      "learning_rate": 1.9733554449972363e-05,
      "epoch": 29.601990049751244
    },
    {
      "loss": 3.7685,
      "grad_norm": 2.3778154850006104,
      "learning_rate": 1.9788833609729135e-05,
      "epoch": 29.6849087893864
    },
    {
      "eval_loss": 3.7560667991638184,
      "eval_runtime": 44.8089,
      "eval_samples_per_second": 92.147,
      "eval_steps_per_second": 5.78,
      "epoch": 29.6849087893864
    },
    {
      "loss": 3.7701,
      "grad_norm": 2.352156639099121,
      "learning_rate": 1.9844112769485907e-05,
      "epoch": 29.76782752902156
    },
    {
      "loss": 3.7475,
      "grad_norm": 2.3692665100097656,
      "learning_rate": 1.989939192924268e-05,
      "epoch": 29.850746268656717
    },
    {
      "eval_loss": 3.7604968547821045,
      "eval_runtime": 44.9886,
      "eval_samples_per_second": 91.779,
      "eval_steps_per_second": 5.757,
      "epoch": 29.850746268656717
    },
    {
      "loss": 3.7379,
      "grad_norm": 2.658752918243408,
      "learning_rate": 1.995467108899945e-05,
      "epoch": 29.933665008291875
    },
    {
      "loss": 3.7271,
      "grad_norm": 2.3654587268829346,
      "learning_rate": 1.9999997756510465e-05,
      "epoch": 30.016583747927033
    },
    {
      "eval_loss": 3.7441766262054443,
      "eval_runtime": 44.8897,
      "eval_samples_per_second": 91.981,
      "eval_steps_per_second": 5.77,
      "epoch": 30.016583747927033
    },
    {
      "loss": 3.7502,
      "grad_norm": 2.332385301589966,
      "learning_rate": 1.9999903585496125e-05,
      "epoch": 30.09950248756219
    },
    {
      "loss": 3.6948,
      "grad_norm": 2.304621696472168,
      "learning_rate": 1.999967092896331e-05,
      "epoch": 30.182421227197345
    },
    {
      "eval_loss": 3.7156827449798584,
      "eval_runtime": 44.9204,
      "eval_samples_per_second": 91.918,
      "eval_steps_per_second": 5.766,
      "epoch": 30.182421227197345
    },
    {
      "loss": 3.7248,
      "grad_norm": 2.3681912422180176,
      "learning_rate": 1.999929979013401e-05,
      "epoch": 30.265339966832503
    },
    {
      "loss": 3.7375,
      "grad_norm": 2.38773775100708,
      "learning_rate": 1.9998790174148007e-05,
      "epoch": 30.34825870646766
    },
    {
      "eval_loss": 3.7316970825195312,
      "eval_runtime": 44.8281,
      "eval_samples_per_second": 92.107,
      "eval_steps_per_second": 5.778,
      "epoch": 30.34825870646766
    },
    {
      "loss": 3.7327,
      "grad_norm": 2.455322265625,
      "learning_rate": 1.999814208806281e-05,
      "epoch": 30.43117744610282
    },
    {
      "loss": 3.701,
      "grad_norm": 2.331031560897827,
      "learning_rate": 1.9997355540853566e-05,
      "epoch": 30.514096185737976
    },
    {
      "eval_loss": 3.6949868202209473,
      "eval_runtime": 44.7943,
      "eval_samples_per_second": 92.177,
      "eval_steps_per_second": 5.782,
      "epoch": 30.514096185737976
    },
    {
      "loss": 3.71,
      "grad_norm": 2.385133743286133,
      "learning_rate": 1.999643054341292e-05,
      "epoch": 30.597014925373134
    },
    {
      "loss": 3.7269,
      "grad_norm": 2.3733341693878174,
      "learning_rate": 1.9995367108550865e-05,
      "epoch": 30.679933665008292
    },
    {
      "eval_loss": 3.6969614028930664,
      "eval_runtime": 44.8333,
      "eval_samples_per_second": 92.097,
      "eval_steps_per_second": 5.777,
      "epoch": 30.679933665008292
    },
    {
      "loss": 3.7414,
      "grad_norm": 2.3276448249816895,
      "learning_rate": 1.9994165250994575e-05,
      "epoch": 30.76285240464345
    },
    {
      "loss": 3.703,
      "grad_norm": 2.451648235321045,
      "learning_rate": 1.9992824987388202e-05,
      "epoch": 30.845771144278608
    },
    {
      "eval_loss": 3.694748878479004,
      "eval_runtime": 44.8174,
      "eval_samples_per_second": 92.129,
      "eval_steps_per_second": 5.779,
      "epoch": 30.845771144278608
    },
    {
      "loss": 3.7067,
      "grad_norm": 2.293205499649048,
      "learning_rate": 1.9991346336292638e-05,
      "epoch": 30.928689883913766
    },
    {
      "loss": 3.7285,
      "grad_norm": 2.2971363067626953,
      "learning_rate": 1.9989729318185245e-05,
      "epoch": 31.011608623548923
    },
    {
      "eval_loss": 3.6815686225891113,
      "eval_runtime": 44.7944,
      "eval_samples_per_second": 92.177,
      "eval_steps_per_second": 5.782,
      "epoch": 31.011608623548923
    },
    {
      "loss": 3.621,
      "grad_norm": 2.3267343044281006,
      "learning_rate": 1.9987973955459607e-05,
      "epoch": 31.09452736318408
    },
    {
      "loss": 3.6575,
      "grad_norm": 2.490583896636963,
      "learning_rate": 1.998608027242519e-05,
      "epoch": 31.177446102819236
    },
    {
      "eval_loss": 3.667508840560913,
      "eval_runtime": 44.9226,
      "eval_samples_per_second": 91.914,
      "eval_steps_per_second": 5.765,
      "epoch": 31.177446102819236
    },
    {
      "loss": 3.6531,
      "grad_norm": 2.360797882080078,
      "learning_rate": 1.998404829530701e-05,
      "epoch": 31.260364842454393
    },
    {
      "loss": 3.6752,
      "grad_norm": 2.4584665298461914,
      "learning_rate": 1.9981878052245283e-05,
      "epoch": 31.34328358208955
    },
    {
      "eval_loss": 3.657566785812378,
      "eval_runtime": 45.0012,
      "eval_samples_per_second": 91.753,
      "eval_steps_per_second": 5.755,
      "epoch": 31.34328358208955
    },
    {
      "loss": 3.6596,
      "grad_norm": 2.5448648929595947,
      "learning_rate": 1.9979569573295022e-05,
      "epoch": 31.42620232172471
    },
    {
      "loss": 3.6508,
      "grad_norm": 2.366032361984253,
      "learning_rate": 1.997712289042562e-05,
      "epoch": 31.509121061359867
    },
    {
      "eval_loss": 3.6693782806396484,
      "eval_runtime": 44.9537,
      "eval_samples_per_second": 91.85,
      "eval_steps_per_second": 5.761,
      "epoch": 31.509121061359867
    },
    {
      "loss": 3.6315,
      "grad_norm": 2.3530337810516357,
      "learning_rate": 1.997453803752043e-05,
      "epoch": 31.592039800995025
    },
    {
      "loss": 3.6461,
      "grad_norm": 2.2648468017578125,
      "learning_rate": 1.9971815050376253e-05,
      "epoch": 31.674958540630183
    },
    {
      "eval_loss": 3.6473681926727295,
      "eval_runtime": 44.7167,
      "eval_samples_per_second": 92.337,
      "eval_steps_per_second": 5.792,
      "epoch": 31.674958540630183
    },
    {
      "loss": 3.6779,
      "grad_norm": 2.371666669845581,
      "learning_rate": 1.996895396670289e-05,
      "epoch": 31.75787728026534
    },
    {
      "loss": 3.6393,
      "grad_norm": 2.3013854026794434,
      "learning_rate": 1.9965954826122583e-05,
      "epoch": 31.8407960199005
    },
    {
      "eval_loss": 3.639246702194214,
      "eval_runtime": 44.8243,
      "eval_samples_per_second": 92.115,
      "eval_steps_per_second": 5.778,
      "epoch": 31.8407960199005
    },
    {
      "loss": 3.6548,
      "grad_norm": 2.328594446182251,
      "learning_rate": 1.9962817670169494e-05,
      "epoch": 31.923714759535656
    },
    {
      "loss": 3.6234,
      "grad_norm": 2.43747878074646,
      "learning_rate": 1.9959542542289103e-05,
      "epoch": 32.00663349917081
    },
    {
      "eval_loss": 3.6180613040924072,
      "eval_runtime": 44.8782,
      "eval_samples_per_second": 92.005,
      "eval_steps_per_second": 5.771,
      "epoch": 32.00663349917081
    },
    {
      "loss": 3.5941,
      "grad_norm": 2.35422420501709,
      "learning_rate": 1.9956129487837624e-05,
      "epoch": 32.08955223880597
    },
    {
      "loss": 3.6423,
      "grad_norm": 2.404144048690796,
      "learning_rate": 1.995257855408138e-05,
      "epoch": 32.172470978441126
    },
    {
      "eval_loss": 3.6100614070892334,
      "eval_runtime": 44.9134,
      "eval_samples_per_second": 91.932,
      "eval_steps_per_second": 5.767,
      "epoch": 32.172470978441126
    },
    {
      "loss": 3.5772,
      "grad_norm": 2.3173201084136963,
      "learning_rate": 1.994888979019613e-05,
      "epoch": 32.25538971807629
    },
    {
      "loss": 3.6178,
      "grad_norm": 2.431769371032715,
      "learning_rate": 1.994506324726641e-05,
      "epoch": 32.33830845771144
    },
    {
      "eval_loss": 3.6150455474853516,
      "eval_runtime": 44.8883,
      "eval_samples_per_second": 91.984,
      "eval_steps_per_second": 5.77,
      "epoch": 32.33830845771144
    },
    {
      "loss": 3.59,
      "grad_norm": 2.3257641792297363,
      "learning_rate": 1.9941098978284805e-05,
      "epoch": 32.4212271973466
    },
    {
      "loss": 3.5963,
      "grad_norm": 2.3310163021087646,
      "learning_rate": 1.9936997038151225e-05,
      "epoch": 32.50414593698176
    },
    {
      "eval_loss": 3.6033637523651123,
      "eval_runtime": 44.7316,
      "eval_samples_per_second": 92.306,
      "eval_steps_per_second": 5.79,
      "epoch": 32.50414593698176
    },
    {
      "loss": 3.5872,
      "grad_norm": 2.3442628383636475,
      "learning_rate": 1.993275748367216e-05,
      "epoch": 32.58706467661692
    },
    {
      "loss": 3.5827,
      "grad_norm": 2.3202691078186035,
      "learning_rate": 1.9928380373559855e-05,
      "epoch": 32.66998341625207
    },
    {
      "eval_loss": 3.5966522693634033,
      "eval_runtime": 44.8766,
      "eval_samples_per_second": 92.008,
      "eval_steps_per_second": 5.771,
      "epoch": 32.66998341625207
    },
    {
      "loss": 3.5769,
      "grad_norm": 2.4601800441741943,
      "learning_rate": 1.9923865768431534e-05,
      "epoch": 32.75290215588723
    },
    {
      "loss": 3.5958,
      "grad_norm": 2.417060375213623,
      "learning_rate": 1.9919213730808544e-05,
      "epoch": 32.83582089552239
    },
    {
      "eval_loss": 3.5793328285217285,
      "eval_runtime": 44.7344,
      "eval_samples_per_second": 92.3,
      "eval_steps_per_second": 5.79,
      "epoch": 32.83582089552239
    },
    {
      "loss": 3.5752,
      "grad_norm": 2.387460231781006,
      "learning_rate": 1.9914424325115493e-05,
      "epoch": 32.91873963515754
    },
    {
      "loss": 3.6035,
      "grad_norm": 2.468843460083008,
      "learning_rate": 1.990949761767935e-05,
      "epoch": 33.001658374792704
    },
    {
      "eval_loss": 3.571507453918457,
      "eval_runtime": 44.7829,
      "eval_samples_per_second": 92.2,
      "eval_steps_per_second": 5.783,
      "epoch": 33.001658374792704
    },
    {
      "loss": 3.5436,
      "grad_norm": 2.4428789615631104,
      "learning_rate": 1.9904433676728536e-05,
      "epoch": 33.08457711442786
    },
    {
      "loss": 3.5683,
      "grad_norm": 2.3211660385131836,
      "learning_rate": 1.989923257239198e-05,
      "epoch": 33.16749585406302
    },
    {
      "eval_loss": 3.5554604530334473,
      "eval_runtime": 44.7894,
      "eval_samples_per_second": 92.187,
      "eval_steps_per_second": 5.783,
      "epoch": 33.16749585406302
    },
    {
      "loss": 3.5442,
      "grad_norm": 2.325471878051758,
      "learning_rate": 1.989389437669813e-05,
      "epoch": 33.250414593698174
    },
    {
      "loss": 3.5863,
      "grad_norm": 2.433650493621826,
      "learning_rate": 1.9888419163573988e-05,
      "epoch": 33.333333333333336
    },
    {
      "eval_loss": 3.562905788421631,
      "eval_runtime": 44.8208,
      "eval_samples_per_second": 92.122,
      "eval_steps_per_second": 5.779,
      "epoch": 33.333333333333336
    },
    {
      "loss": 3.5459,
      "grad_norm": 2.3963406085968018,
      "learning_rate": 1.9882807008844055e-05,
      "epoch": 33.41625207296849
    },
    {
      "loss": 3.519,
      "grad_norm": 2.372513771057129,
      "learning_rate": 1.9877057990229297e-05,
      "epoch": 33.49917081260365
    },
    {
      "eval_loss": 3.5444228649139404,
      "eval_runtime": 44.7625,
      "eval_samples_per_second": 92.242,
      "eval_steps_per_second": 5.786,
      "epoch": 33.49917081260365
    },
    {
      "loss": 3.5545,
      "grad_norm": 2.3225343227386475,
      "learning_rate": 1.9871172187346063e-05,
      "epoch": 33.582089552238806
    },
    {
      "loss": 3.5505,
      "grad_norm": 2.4132578372955322,
      "learning_rate": 1.9865149681704982e-05,
      "epoch": 33.66500829187396
    },
    {
      "eval_loss": 3.553972005844116,
      "eval_runtime": 44.795,
      "eval_samples_per_second": 92.175,
      "eval_steps_per_second": 5.782,
      "epoch": 33.66500829187396
    },
    {
      "loss": 3.5551,
      "grad_norm": 2.3435428142547607,
      "learning_rate": 1.9858990556709847e-05,
      "epoch": 33.74792703150912
    },
    {
      "loss": 3.5307,
      "grad_norm": 2.42558217048645,
      "learning_rate": 1.9852694897656435e-05,
      "epoch": 33.830845771144276
    },
    {
      "eval_loss": 3.5422511100769043,
      "eval_runtime": 44.9084,
      "eval_samples_per_second": 91.943,
      "eval_steps_per_second": 5.767,
      "epoch": 33.830845771144276
    },
    {
      "loss": 3.5328,
      "grad_norm": 2.401134967803955,
      "learning_rate": 1.9846262791731355e-05,
      "epoch": 33.91376451077944
    },
    {
      "loss": 3.4986,
      "grad_norm": 2.352684736251831,
      "learning_rate": 1.983969432801081e-05,
      "epoch": 33.99668325041459
    },
    {
      "eval_loss": 3.5425634384155273,
      "eval_runtime": 44.8206,
      "eval_samples_per_second": 92.123,
      "eval_steps_per_second": 5.779,
      "epoch": 33.99668325041459
    },
    {
      "loss": 3.4889,
      "grad_norm": 2.436408519744873,
      "learning_rate": 1.9832989597459393e-05,
      "epoch": 34.07960199004975
    },
    {
      "loss": 3.5346,
      "grad_norm": 2.3243398666381836,
      "learning_rate": 1.9826148692928805e-05,
      "epoch": 34.16252072968491
    },
    {
      "eval_loss": 3.5281667709350586,
      "eval_runtime": 44.7735,
      "eval_samples_per_second": 92.22,
      "eval_steps_per_second": 5.785,
      "epoch": 34.16252072968491
    },
    {
      "loss": 3.5707,
      "grad_norm": 2.4258933067321777,
      "learning_rate": 1.9819171709156582e-05,
      "epoch": 34.24543946932007
    },
    {
      "loss": 3.4831,
      "grad_norm": 2.581127405166626,
      "learning_rate": 1.981205874276478e-05,
      "epoch": 34.32835820895522
    },
    {
      "eval_loss": 3.5268945693969727,
      "eval_runtime": 44.8166,
      "eval_samples_per_second": 92.131,
      "eval_steps_per_second": 5.779,
      "epoch": 34.32835820895522
    },
    {
      "loss": 3.5075,
      "grad_norm": 2.373438835144043,
      "learning_rate": 1.9804809892258623e-05,
      "epoch": 34.411276948590384
    },
    {
      "loss": 3.4858,
      "grad_norm": 2.4306390285491943,
      "learning_rate": 1.979742525802517e-05,
      "epoch": 34.49419568822554
    },
    {
      "eval_loss": 3.5163333415985107,
      "eval_runtime": 44.7544,
      "eval_samples_per_second": 92.259,
      "eval_steps_per_second": 5.787,
      "epoch": 34.49419568822554
    },
    {
      "loss": 3.4922,
      "grad_norm": 2.3858866691589355,
      "learning_rate": 1.97899049423319e-05,
      "epoch": 34.5771144278607
    },
    {
      "loss": 3.4934,
      "grad_norm": 2.4272451400756836,
      "learning_rate": 1.9782249049325282e-05,
      "epoch": 34.660033167495854
    },
    {
      "eval_loss": 3.4883246421813965,
      "eval_runtime": 44.7733,
      "eval_samples_per_second": 92.22,
      "eval_steps_per_second": 5.785,
      "epoch": 34.660033167495854
    },
    {
      "loss": 3.5015,
      "grad_norm": 2.4914662837982178,
      "learning_rate": 1.9774457685029386e-05,
      "epoch": 34.74295190713101
    },
    {
      "loss": 3.4793,
      "grad_norm": 2.377329111099243,
      "learning_rate": 1.9766530957344362e-05,
      "epoch": 34.82587064676617
    },
    {
      "eval_loss": 3.4946136474609375,
      "eval_runtime": 44.9144,
      "eval_samples_per_second": 91.93,
      "eval_steps_per_second": 5.767,
      "epoch": 34.82587064676617
    },
    {
      "loss": 3.4815,
      "grad_norm": 2.392423152923584,
      "learning_rate": 1.9758468976044966e-05,
      "epoch": 34.908789386401324
    },
    {
      "loss": 3.4601,
      "grad_norm": 2.3998560905456543,
      "learning_rate": 1.9750271852779033e-05,
      "epoch": 34.991708126036485
    },
    {
      "eval_loss": 3.502460241317749,
      "eval_runtime": 44.8067,
      "eval_samples_per_second": 92.151,
      "eval_steps_per_second": 5.78,
      "epoch": 34.991708126036485
    },
    {
      "loss": 3.4573,
      "grad_norm": 2.3143117427825928,
      "learning_rate": 1.9741939701065956e-05,
      "epoch": 35.07462686567164
    },
    {
      "loss": 3.4822,
      "grad_norm": 2.385258436203003,
      "learning_rate": 1.9733472636295077e-05,
      "epoch": 35.1575456053068
    },
    {
      "eval_loss": 3.486471652984619,
      "eval_runtime": 44.791,
      "eval_samples_per_second": 92.184,
      "eval_steps_per_second": 5.782,
      "epoch": 35.1575456053068
    },
    {
      "loss": 3.4779,
      "grad_norm": 2.535111665725708,
      "learning_rate": 1.972487077572411e-05,
      "epoch": 35.240464344941955
    },
    {
      "loss": 3.4461,
      "grad_norm": 2.2457199096679688,
      "learning_rate": 1.9716134238477517e-05,
      "epoch": 35.32338308457712
    },
    {
      "eval_loss": 3.469921350479126,
      "eval_runtime": 44.9486,
      "eval_samples_per_second": 91.861,
      "eval_steps_per_second": 5.762,
      "epoch": 35.32338308457712
    },
    {
      "loss": 3.4464,
      "grad_norm": 2.4782230854034424,
      "learning_rate": 1.9707263145544857e-05,
      "epoch": 35.40630182421227
    },
    {
      "loss": 3.4346,
      "grad_norm": 2.301630735397339,
      "learning_rate": 1.96982576197791e-05,
      "epoch": 35.48922056384743
    },
    {
      "eval_loss": 3.47245192527771,
      "eval_runtime": 44.9224,
      "eval_samples_per_second": 91.914,
      "eval_steps_per_second": 5.765,
      "epoch": 35.48922056384743
    },
    {
      "loss": 3.4337,
      "grad_norm": 2.411799669265747,
      "learning_rate": 1.9689117785894943e-05,
      "epoch": 35.57213930348259
    },
    {
      "loss": 3.4462,
      "grad_norm": 2.550781488418579,
      "learning_rate": 1.967984377046707e-05,
      "epoch": 35.65505804311775
    },
    {
      "eval_loss": 3.466874361038208,
      "eval_runtime": 44.7671,
      "eval_samples_per_second": 92.233,
      "eval_steps_per_second": 5.785,
      "epoch": 35.65505804311775
    },
    {
      "loss": 3.4644,
      "grad_norm": 2.443511486053467,
      "learning_rate": 1.9670435701928397e-05,
      "epoch": 35.7379767827529
    },
    {
      "loss": 3.4088,
      "grad_norm": 2.4824233055114746,
      "learning_rate": 1.966089371056831e-05,
      "epoch": 35.82089552238806
    },
    {
      "eval_loss": 3.452944040298462,
      "eval_runtime": 44.9284,
      "eval_samples_per_second": 91.902,
      "eval_steps_per_second": 5.765,
      "epoch": 35.82089552238806
    },
    {
      "loss": 3.4368,
      "grad_norm": 2.4285504817962646,
      "learning_rate": 1.965121792853084e-05,
      "epoch": 35.90381426202322
    },
    {
      "loss": 3.4141,
      "grad_norm": 2.3574371337890625,
      "learning_rate": 1.9641408489812858e-05,
      "epoch": 35.98673300165837
    },
    {
      "eval_loss": 3.4550275802612305,
      "eval_runtime": 44.7528,
      "eval_samples_per_second": 92.262,
      "eval_steps_per_second": 5.787,
      "epoch": 35.98673300165837
    },
    {
      "loss": 3.4231,
      "grad_norm": 2.4479215145111084,
      "learning_rate": 1.9631465530262186e-05,
      "epoch": 36.069651741293534
    },
    {
      "loss": 3.4073,
      "grad_norm": 2.4113893508911133,
      "learning_rate": 1.9621389187575746e-05,
      "epoch": 36.15257048092869
    },
    {
      "eval_loss": 3.4616622924804688,
      "eval_runtime": 44.7599,
      "eval_samples_per_second": 92.248,
      "eval_steps_per_second": 5.786,
      "epoch": 36.15257048092869
    },
    {
      "loss": 3.3959,
      "grad_norm": 2.4827635288238525,
      "learning_rate": 1.9611179601297636e-05,
      "epoch": 36.23548922056385
    },
    {
      "loss": 3.4117,
      "grad_norm": 2.280956983566284,
      "learning_rate": 1.9600836912817204e-05,
      "epoch": 36.318407960199
    },
    {
      "eval_loss": 3.435023069381714,
      "eval_runtime": 44.6469,
      "eval_samples_per_second": 92.481,
      "eval_steps_per_second": 5.801,
      "epoch": 36.318407960199
    },
    {
      "loss": 3.3969,
      "grad_norm": 2.4196996688842773,
      "learning_rate": 1.959036126536709e-05,
      "epoch": 36.401326699834165
    },
    {
      "loss": 3.4325,
      "grad_norm": 2.3852553367614746,
      "learning_rate": 1.9579752804021238e-05,
      "epoch": 36.48424543946932
    },
    {
      "eval_loss": 3.4266960620880127,
      "eval_runtime": 44.7484,
      "eval_samples_per_second": 92.271,
      "eval_steps_per_second": 5.788,
      "epoch": 36.48424543946932
    },
    {
      "loss": 3.4361,
      "grad_norm": 2.3675568103790283,
      "learning_rate": 1.9569011675692895e-05,
      "epoch": 36.56716417910448
    },
    {
      "loss": 3.3894,
      "grad_norm": 2.436154365539551,
      "learning_rate": 1.9558138029132562e-05,
      "epoch": 36.650082918739635
    },
    {
      "eval_loss": 3.431136131286621,
      "eval_runtime": 44.9171,
      "eval_samples_per_second": 91.925,
      "eval_steps_per_second": 5.766,
      "epoch": 36.650082918739635
    },
    {
      "loss": 3.3752,
      "grad_norm": 2.4855878353118896,
      "learning_rate": 1.9547132014925953e-05,
      "epoch": 36.73300165837479
    },
    {
      "loss": 3.3783,
      "grad_norm": 2.3800125122070312,
      "learning_rate": 1.9535993785491895e-05,
      "epoch": 36.81592039800995
    },
    {
      "eval_loss": 3.4136292934417725,
      "eval_runtime": 44.921,
      "eval_samples_per_second": 91.917,
      "eval_steps_per_second": 5.766,
      "epoch": 36.81592039800995
    },
    {
      "loss": 3.3644,
      "grad_norm": 2.506363868713379,
      "learning_rate": 1.9524723495080223e-05,
      "epoch": 36.898839137645105
    },
    {
      "loss": 3.3712,
      "grad_norm": 2.460200548171997,
      "learning_rate": 1.951332129976964e-05,
      "epoch": 36.981757877280266
    },
    {
      "eval_loss": 3.414602756500244,
      "eval_runtime": 44.7801,
      "eval_samples_per_second": 92.206,
      "eval_steps_per_second": 5.784,
      "epoch": 36.981757877280266
    },
    {
      "loss": 3.3592,
      "grad_norm": 2.4118852615356445,
      "learning_rate": 1.950178735746557e-05,
      "epoch": 37.06467661691542
    },
    {
      "loss": 3.3833,
      "grad_norm": 2.529684066772461,
      "learning_rate": 1.9490121827897943e-05,
      "epoch": 37.14759535655058
    },
    {
      "eval_loss": 3.3960444927215576,
      "eval_runtime": 44.6844,
      "eval_samples_per_second": 92.404,
      "eval_steps_per_second": 5.796,
      "epoch": 37.14759535655058
    },
    {
      "loss": 3.3384,
      "grad_norm": 2.545466423034668,
      "learning_rate": 1.947832487261901e-05,
      "epoch": 37.230514096185736
    },
    {
      "loss": 3.3431,
      "grad_norm": 2.3754167556762695,
      "learning_rate": 1.946639665500109e-05,
      "epoch": 37.3134328358209
    },
    {
      "eval_loss": 3.418092727661133,
      "eval_runtime": 44.844,
      "eval_samples_per_second": 92.075,
      "eval_steps_per_second": 5.776,
      "epoch": 37.3134328358209
    },
    {
      "loss": 3.3364,
      "grad_norm": 2.3933229446411133,
      "learning_rate": 1.945433734023432e-05,
      "epoch": 37.39635157545605
    },
    {
      "loss": 3.3657,
      "grad_norm": 2.3742759227752686,
      "learning_rate": 1.9442147095324356e-05,
      "epoch": 37.47927031509121
    },
    {
      "eval_loss": 3.378563642501831,
      "eval_runtime": 44.7737,
      "eval_samples_per_second": 92.219,
      "eval_steps_per_second": 5.785,
      "epoch": 37.47927031509121
    },
    {
      "loss": 3.3439,
      "grad_norm": 2.37912917137146,
      "learning_rate": 1.9429826089090064e-05,
      "epoch": 37.56218905472637
    },
    {
      "loss": 3.3777,
      "grad_norm": 2.335615873336792,
      "learning_rate": 1.941737449216118e-05,
      "epoch": 37.64510779436153
    },
    {
      "eval_loss": 3.3861939907073975,
      "eval_runtime": 44.825,
      "eval_samples_per_second": 92.114,
      "eval_steps_per_second": 5.778,
      "epoch": 37.64510779436153
    },
    {
      "loss": 3.3585,
      "grad_norm": 2.5720608234405518,
      "learning_rate": 1.9404792476975954e-05,
      "epoch": 37.72802653399668
    },
    {
      "loss": 3.3502,
      "grad_norm": 2.435619831085205,
      "learning_rate": 1.9392080217778756e-05,
      "epoch": 37.81094527363184
    },
    {
      "eval_loss": 3.3811521530151367,
      "eval_runtime": 44.885,
      "eval_samples_per_second": 91.991,
      "eval_steps_per_second": 5.77,
      "epoch": 37.81094527363184
    },
    {
      "loss": 3.3232,
      "grad_norm": 2.4470396041870117,
      "learning_rate": 1.9379237890617658e-05,
      "epoch": 37.893864013267
    },
    {
      "loss": 3.3352,
      "grad_norm": 2.4501030445098877,
      "learning_rate": 1.936626567334202e-05,
      "epoch": 37.97678275290215
    },
    {
      "eval_loss": 3.37243914604187,
      "eval_runtime": 44.8368,
      "eval_samples_per_second": 92.09,
      "eval_steps_per_second": 5.777,
      "epoch": 37.97678275290215
    },
    {
      "loss": 3.3275,
      "grad_norm": 2.325359344482422,
      "learning_rate": 1.935316374559999e-05,
      "epoch": 38.059701492537314
    },
    {
      "loss": 3.3407,
      "grad_norm": 2.507514476776123,
      "learning_rate": 1.9339932288836045e-05,
      "epoch": 38.14262023217247
    },
    {
      "eval_loss": 3.3724560737609863,
      "eval_runtime": 44.826,
      "eval_samples_per_second": 92.112,
      "eval_steps_per_second": 5.778,
      "epoch": 38.14262023217247
    },
    {
      "loss": 3.3054,
      "grad_norm": 2.5598387718200684,
      "learning_rate": 1.9326571486288465e-05,
      "epoch": 38.22553897180763
    },
    {
      "loss": 3.323,
      "grad_norm": 2.4058144092559814,
      "learning_rate": 1.9313081522986806e-05,
      "epoch": 38.308457711442784
    },
    {
      "eval_loss": 3.3612220287323,
      "eval_runtime": 44.8255,
      "eval_samples_per_second": 92.113,
      "eval_steps_per_second": 5.778,
      "epoch": 38.308457711442784
    },
    {
      "loss": 3.3067,
      "grad_norm": 2.4213852882385254,
      "learning_rate": 1.9299462585749317e-05,
      "epoch": 38.391376451077946
    },
    {
      "loss": 3.302,
      "grad_norm": 2.3657138347625732,
      "learning_rate": 1.928571486318038e-05,
      "epoch": 38.4742951907131
    },
    {
      "eval_loss": 3.3640027046203613,
      "eval_runtime": 44.857,
      "eval_samples_per_second": 92.048,
      "eval_steps_per_second": 5.774,
      "epoch": 38.4742951907131
    },
    {
      "loss": 3.33,
      "grad_norm": 2.38112735748291,
      "learning_rate": 1.9271838545667876e-05,
      "epoch": 38.55721393034826
    },
    {
      "loss": 3.3251,
      "grad_norm": 2.495853900909424,
      "learning_rate": 1.9257833825380567e-05,
      "epoch": 38.640132669983416
    },
    {
      "eval_loss": 3.3507111072540283,
      "eval_runtime": 44.8095,
      "eval_samples_per_second": 92.146,
      "eval_steps_per_second": 5.78,
      "epoch": 38.640132669983416
    },
    {
      "loss": 3.3062,
      "grad_norm": 2.468045711517334,
      "learning_rate": 1.924370089626541e-05,
      "epoch": 38.72305140961858
    },
    {
      "loss": 3.3245,
      "grad_norm": 2.402966022491455,
      "learning_rate": 1.9229439954044897e-05,
      "epoch": 38.80597014925373
    },
    {
      "eval_loss": 3.3498647212982178,
      "eval_runtime": 44.8599,
      "eval_samples_per_second": 92.042,
      "eval_steps_per_second": 5.774,
      "epoch": 38.80597014925373
    },
    {
      "loss": 3.3125,
      "grad_norm": 2.39715576171875,
      "learning_rate": 1.9215051196214333e-05,
      "epoch": 38.888888888888886
    },
    {
      "loss": 3.2885,
      "grad_norm": 2.3446426391601562,
      "learning_rate": 1.9200534822039095e-05,
      "epoch": 38.97180762852405
    },
    {
      "eval_loss": 3.349191427230835,
      "eval_runtime": 44.8183,
      "eval_samples_per_second": 92.128,
      "eval_steps_per_second": 5.779,
      "epoch": 38.97180762852405
    },
    {
      "loss": 3.2823,
      "grad_norm": 2.66845703125,
      "learning_rate": 1.918589103255188e-05,
      "epoch": 39.0547263681592
    },
    {
      "loss": 3.279,
      "grad_norm": 2.5368242263793945,
      "learning_rate": 1.9171120030549925e-05,
      "epoch": 39.13764510779436
    },
    {
      "eval_loss": 3.3258252143859863,
      "eval_runtime": 44.7472,
      "eval_samples_per_second": 92.274,
      "eval_steps_per_second": 5.788,
      "epoch": 39.13764510779436
    },
    {
      "loss": 3.2571,
      "grad_norm": 2.4397189617156982,
      "learning_rate": 1.915622202059219e-05,
      "epoch": 39.22056384742952
    },
    {
      "loss": 3.2656,
      "grad_norm": 2.4082958698272705,
      "learning_rate": 1.914119720899652e-05,
      "epoch": 39.30348258706468
    },
    {
      "eval_loss": 3.322061777114868,
      "eval_runtime": 44.7994,
      "eval_samples_per_second": 92.166,
      "eval_steps_per_second": 5.781,
      "epoch": 39.30348258706468
    },
    {
      "loss": 3.3084,
      "grad_norm": 2.4544620513916016,
      "learning_rate": 1.912604580383681e-05,
      "epoch": 39.38640132669983
    },
    {
      "loss": 3.297,
      "grad_norm": 2.3524560928344727,
      "learning_rate": 1.91107680149401e-05,
      "epoch": 39.469320066334994
    },
    {
      "eval_loss": 3.325988531112671,
      "eval_runtime": 44.8895,
      "eval_samples_per_second": 91.981,
      "eval_steps_per_second": 5.77,
      "epoch": 39.469320066334994
    },
    {
      "loss": 3.2803,
      "grad_norm": 2.423609495162964,
      "learning_rate": 1.9095364053883688e-05,
      "epoch": 39.55223880597015
    },
    {
      "loss": 3.2694,
      "grad_norm": 2.4105472564697266,
      "learning_rate": 1.9079834133992175e-05,
      "epoch": 39.63515754560531
    },
    {
      "eval_loss": 3.3308424949645996,
      "eval_runtime": 44.748,
      "eval_samples_per_second": 92.272,
      "eval_steps_per_second": 5.788,
      "epoch": 39.63515754560531
    },
    {
      "loss": 3.2572,
      "grad_norm": 2.511579990386963,
      "learning_rate": 1.906417847033454e-05,
      "epoch": 39.718076285240464
    },
    {
      "loss": 3.2308,
      "grad_norm": 2.386014699935913,
      "learning_rate": 1.9048397279721142e-05,
      "epoch": 39.80099502487562
    },
    {
      "eval_loss": 3.3181838989257812,
      "eval_runtime": 44.8312,
      "eval_samples_per_second": 92.101,
      "eval_steps_per_second": 5.777,
      "epoch": 39.80099502487562
    },
    {
      "loss": 3.2967,
      "grad_norm": 2.380253553390503,
      "learning_rate": 1.9032490780700727e-05,
      "epoch": 39.88391376451078
    },
    {
      "loss": 3.2299,
      "grad_norm": 2.421903133392334,
      "learning_rate": 1.9016459193557394e-05,
      "epoch": 39.966832504145934
    },
    {
      "eval_loss": 3.3137474060058594,
      "eval_runtime": 44.9601,
      "eval_samples_per_second": 91.837,
      "eval_steps_per_second": 5.761,
      "epoch": 39.966832504145934
    },
    {
      "loss": 3.2809,
      "grad_norm": 2.413421869277954,
      "learning_rate": 1.900030274030755e-05,
      "epoch": 40.049751243781095
    },
    {
      "loss": 3.2196,
      "grad_norm": 2.391941785812378,
      "learning_rate": 1.8984021644696828e-05,
      "epoch": 40.13266998341625
    },
    {
      "eval_loss": 3.3190197944641113,
      "eval_runtime": 44.9297,
      "eval_samples_per_second": 91.899,
      "eval_steps_per_second": 5.765,
      "epoch": 40.13266998341625
    },
    {
      "loss": 3.2321,
      "grad_norm": 2.472442865371704,
      "learning_rate": 1.8967616132197003e-05,
      "epoch": 40.21558872305141
    },
    {
      "loss": 3.2553,
      "grad_norm": 2.467775821685791,
      "learning_rate": 1.8951086430002856e-05,
      "epoch": 40.298507462686565
    },
    {
      "eval_loss": 3.2879443168640137,
      "eval_runtime": 44.7881,
      "eval_samples_per_second": 92.19,
      "eval_steps_per_second": 5.783,
      "epoch": 40.298507462686565
    },
    {
      "loss": 3.2868,
      "grad_norm": 2.426863431930542,
      "learning_rate": 1.893443276702903e-05,
      "epoch": 40.38142620232173
    },
    {
      "loss": 3.2417,
      "grad_norm": 2.4445955753326416,
      "learning_rate": 1.8917655373906866e-05,
      "epoch": 40.46434494195688
    },
    {
      "eval_loss": 3.295941114425659,
      "eval_runtime": 44.7672,
      "eval_samples_per_second": 92.233,
      "eval_steps_per_second": 5.785,
      "epoch": 40.46434494195688
    },
    {
      "loss": 3.2264,
      "grad_norm": 2.446000337600708,
      "learning_rate": 1.89007544829812e-05,
      "epoch": 40.54726368159204
    },
    {
      "loss": 3.2344,
      "grad_norm": 2.4171671867370605,
      "learning_rate": 1.8883730328307154e-05,
      "epoch": 40.6301824212272
    },
    {
      "eval_loss": 3.276482105255127,
      "eval_runtime": 44.8525,
      "eval_samples_per_second": 92.057,
      "eval_steps_per_second": 5.774,
      "epoch": 40.6301824212272
    },
    {
      "loss": 3.208,
      "grad_norm": 2.4660379886627197,
      "learning_rate": 1.8866583145646892e-05,
      "epoch": 40.71310116086236
    },
    {
      "loss": 3.2393,
      "grad_norm": 2.3445682525634766,
      "learning_rate": 1.884931317246635e-05,
      "epoch": 40.79601990049751
    },
    {
      "eval_loss": 3.2859244346618652,
      "eval_runtime": 44.964,
      "eval_samples_per_second": 91.829,
      "eval_steps_per_second": 5.76,
      "epoch": 40.79601990049751
    },
    {
      "loss": 3.2341,
      "grad_norm": 2.4120490550994873,
      "learning_rate": 1.883192064793195e-05,
      "epoch": 40.87893864013267
    },
    {
      "loss": 3.2153,
      "grad_norm": 2.377427339553833,
      "learning_rate": 1.8814405812907295e-05,
      "epoch": 40.96185737976783
    },
    {
      "eval_loss": 3.274113655090332,
      "eval_runtime": 44.9675,
      "eval_samples_per_second": 91.822,
      "eval_steps_per_second": 5.76,
      "epoch": 40.96185737976783
    },
    {
      "loss": 3.2445,
      "grad_norm": 2.40509033203125,
      "learning_rate": 1.8796768909949828e-05,
      "epoch": 41.04477611940298
    },
    {
      "loss": 3.1928,
      "grad_norm": 2.4241034984588623,
      "learning_rate": 1.877901018330746e-05,
      "epoch": 41.127694859038144
    },
    {
      "eval_loss": 3.256075620651245,
      "eval_runtime": 44.8648,
      "eval_samples_per_second": 92.032,
      "eval_steps_per_second": 5.773,
      "epoch": 41.127694859038144
    },
    {
      "loss": 3.2157,
      "grad_norm": 2.524165391921997,
      "learning_rate": 1.8761129878915215e-05,
      "epoch": 41.2106135986733
    },
    {
      "loss": 3.1989,
      "grad_norm": 2.4564461708068848,
      "learning_rate": 1.8743128244391804e-05,
      "epoch": 41.29353233830846
    },
    {
      "eval_loss": 3.262702703475952,
      "eval_runtime": 44.8093,
      "eval_samples_per_second": 92.146,
      "eval_steps_per_second": 5.78,
      "epoch": 41.29353233830846
    },
    {
      "loss": 3.2282,
      "grad_norm": 2.315159797668457,
      "learning_rate": 1.8725005529036197e-05,
      "epoch": 41.376451077943614
    },
    {
      "loss": 3.1967,
      "grad_norm": 2.3985023498535156,
      "learning_rate": 1.870676198382418e-05,
      "epoch": 41.459369817578775
    },
    {
      "eval_loss": 3.2767136096954346,
      "eval_runtime": 44.9585,
      "eval_samples_per_second": 91.84,
      "eval_steps_per_second": 5.761,
      "epoch": 41.459369817578775
    },
    {
      "loss": 3.1825,
      "grad_norm": 2.297605276107788,
      "learning_rate": 1.868839786140486e-05,
      "epoch": 41.54228855721393
    },
    {
      "loss": 3.1692,
      "grad_norm": 2.4106104373931885,
      "learning_rate": 1.8669913416097205e-05,
      "epoch": 41.62520729684909
    },
    {
      "eval_loss": 3.2513697147369385,
      "eval_runtime": 44.8353,
      "eval_samples_per_second": 92.093,
      "eval_steps_per_second": 5.777,
      "epoch": 41.62520729684909
    },
    {
      "loss": 3.1827,
      "grad_norm": 2.4527127742767334,
      "learning_rate": 1.8651308903886474e-05,
      "epoch": 41.708126036484245
    },
    {
      "loss": 3.1824,
      "grad_norm": 2.435291051864624,
      "learning_rate": 1.8632584582420706e-05,
      "epoch": 41.791044776119406
    },
    {
      "eval_loss": 3.2618753910064697,
      "eval_runtime": 44.8331,
      "eval_samples_per_second": 92.097,
      "eval_steps_per_second": 5.777,
      "epoch": 41.791044776119406
    },
    {
      "loss": 3.1964,
      "grad_norm": 2.455458164215088,
      "learning_rate": 1.8613740711007134e-05,
      "epoch": 41.87396351575456
    },
    {
      "loss": 3.2003,
      "grad_norm": 2.3663389682769775,
      "learning_rate": 1.8594777550608608e-05,
      "epoch": 41.956882255389715
    },
    {
      "eval_loss": 3.2425596714019775,
      "eval_runtime": 44.9097,
      "eval_samples_per_second": 91.94,
      "eval_steps_per_second": 5.767,
      "epoch": 41.956882255389715
    },
    {
      "loss": 3.2054,
      "grad_norm": 2.3903872966766357,
      "learning_rate": 1.857569536383997e-05,
      "epoch": 42.039800995024876
    },
    {
      "loss": 3.1776,
      "grad_norm": 2.4678943157196045,
      "learning_rate": 1.855649441496442e-05,
      "epoch": 42.12271973466003
    },
    {
      "eval_loss": 3.2356631755828857,
      "eval_runtime": 44.7657,
      "eval_samples_per_second": 92.236,
      "eval_steps_per_second": 5.786,
      "epoch": 42.12271973466003
    },
    {
      "loss": 3.1892,
      "grad_norm": 2.496342182159424,
      "learning_rate": 1.8537174969889853e-05,
      "epoch": 42.20563847429519
    },
    {
      "loss": 3.1835,
      "grad_norm": 2.3626387119293213,
      "learning_rate": 1.8517737296165194e-05,
      "epoch": 42.288557213930346
    },
    {
      "eval_loss": 3.2405736446380615,
      "eval_runtime": 44.8514,
      "eval_samples_per_second": 92.06,
      "eval_steps_per_second": 5.775,
      "epoch": 42.288557213930346
    },
    {
      "loss": 3.1524,
      "grad_norm": 2.4664013385772705,
      "learning_rate": 1.8498181662976665e-05,
      "epoch": 42.37147595356551
    },
    {
      "loss": 3.2106,
      "grad_norm": 2.392324686050415,
      "learning_rate": 1.8478508341144076e-05,
      "epoch": 42.45439469320066
    },
    {
      "eval_loss": 3.2461225986480713,
      "eval_runtime": 44.7349,
      "eval_samples_per_second": 92.299,
      "eval_steps_per_second": 5.79,
      "epoch": 42.45439469320066
    },
    {
      "loss": 3.1843,
      "grad_norm": 2.406912088394165,
      "learning_rate": 1.8458717603117077e-05,
      "epoch": 42.53731343283582
    },
    {
      "loss": 3.1477,
      "grad_norm": 2.4091169834136963,
      "learning_rate": 1.843880972297137e-05,
      "epoch": 42.62023217247098
    },
    {
      "eval_loss": 3.230443239212036,
      "eval_runtime": 46.2306,
      "eval_samples_per_second": 89.313,
      "eval_steps_per_second": 5.602,
      "epoch": 42.62023217247098
    },
    {
      "loss": 3.1471,
      "grad_norm": 2.435919761657715,
      "learning_rate": 1.841878497640492e-05,
      "epoch": 42.70315091210614
    },
    {
      "loss": 3.1571,
      "grad_norm": 2.510671615600586,
      "learning_rate": 1.8398643640734147e-05,
      "epoch": 42.78606965174129
    },
    {
      "eval_loss": 3.219569206237793,
      "eval_runtime": 46.19,
      "eval_samples_per_second": 89.392,
      "eval_steps_per_second": 5.607,
      "epoch": 42.78606965174129
    },
    {
      "loss": 3.1543,
      "grad_norm": 2.4633376598358154,
      "learning_rate": 1.8378385994890065e-05,
      "epoch": 42.86898839137645
    },
    {
      "loss": 3.137,
      "grad_norm": 2.473207712173462,
      "learning_rate": 1.8358012319414443e-05,
      "epoch": 42.95190713101161
    },
    {
      "eval_loss": 3.2099087238311768,
      "eval_runtime": 45.9857,
      "eval_samples_per_second": 89.789,
      "eval_steps_per_second": 5.632,
      "epoch": 42.95190713101161
    },
    {
      "loss": 3.1514,
      "grad_norm": 2.3790817260742188,
      "learning_rate": 1.83375228964559e-05,
      "epoch": 43.03482587064676
    },
    {
      "loss": 3.1311,
      "grad_norm": 2.397679328918457,
      "learning_rate": 1.8316918009766012e-05,
      "epoch": 43.117744610281925
    },
    {
      "eval_loss": 3.223168134689331,
      "eval_runtime": 45.3877,
      "eval_samples_per_second": 90.972,
      "eval_steps_per_second": 5.706,
      "epoch": 43.117744610281925
    },
    {
      "loss": 3.149,
      "grad_norm": 2.4453516006469727,
      "learning_rate": 1.8296197944695366e-05,
      "epoch": 43.20066334991708
    },
    {
      "loss": 3.137,
      "grad_norm": 2.4021637439727783,
      "learning_rate": 1.8275362988189627e-05,
      "epoch": 43.28358208955224
    },
    {
      "eval_loss": 3.2046031951904297,
      "eval_runtime": 46.3416,
      "eval_samples_per_second": 89.099,
      "eval_steps_per_second": 5.589,
      "epoch": 43.28358208955224
    },
    {
      "loss": 3.1488,
      "grad_norm": 2.437716245651245,
      "learning_rate": 1.8254413428785552e-05,
      "epoch": 43.366500829187395
    },
    {
      "loss": 3.1164,
      "grad_norm": 2.4281985759735107,
      "learning_rate": 1.8233349556606998e-05,
      "epoch": 43.449419568822556
    },
    {
      "eval_loss": 3.1903035640716553,
      "eval_runtime": 46.1049,
      "eval_samples_per_second": 89.557,
      "eval_steps_per_second": 5.618,
      "epoch": 43.449419568822556
    },
    {
      "loss": 3.1264,
      "grad_norm": 2.3554961681365967,
      "learning_rate": 1.8212171663360902e-05,
      "epoch": 43.53233830845771
    },
    {
      "loss": 3.1427,
      "grad_norm": 2.422006368637085,
      "learning_rate": 1.8190880042333247e-05,
      "epoch": 43.61525704809287
    },
    {
      "eval_loss": 3.2141690254211426,
      "eval_runtime": 43.559,
      "eval_samples_per_second": 94.791,
      "eval_steps_per_second": 5.946,
      "epoch": 43.61525704809287
    },
    {
      "loss": 3.1234,
      "grad_norm": 2.4000394344329834,
      "learning_rate": 1.8169474988384994e-05,
      "epoch": 43.698175787728026
    },
    {
      "loss": 3.1065,
      "grad_norm": 2.4145500659942627,
      "learning_rate": 1.8147956797947994e-05,
      "epoch": 43.78109452736319
    },
    {
      "eval_loss": 3.2099123001098633,
      "eval_runtime": 46.4512,
      "eval_samples_per_second": 88.889,
      "eval_steps_per_second": 5.576,
      "epoch": 43.78109452736319
    },
    {
      "loss": 3.1319,
      "grad_norm": 2.395054340362549,
      "learning_rate": 1.8126325769020906e-05,
      "epoch": 43.86401326699834
    },
    {
      "loss": 3.1167,
      "grad_norm": 2.3701558113098145,
      "learning_rate": 1.8104582201165036e-05,
      "epoch": 43.946932006633496
    },
    {
      "eval_loss": 3.1888222694396973,
      "eval_runtime": 46.6152,
      "eval_samples_per_second": 88.576,
      "eval_steps_per_second": 5.556,
      "epoch": 43.946932006633496
    },
    {
      "loss": 3.125,
      "grad_norm": 2.472158193588257,
      "learning_rate": 1.8082726395500215e-05,
      "epoch": 44.02985074626866
    },
    {
      "loss": 3.072,
      "grad_norm": 2.4652793407440186,
      "learning_rate": 1.8060758654700622e-05,
      "epoch": 44.11276948590381
    },
    {
      "eval_loss": 3.1834030151367188,
      "eval_runtime": 46.6691,
      "eval_samples_per_second": 88.474,
      "eval_steps_per_second": 5.55,
      "epoch": 44.11276948590381
    },
    {
      "loss": 3.0959,
      "grad_norm": 2.4526524543762207,
      "learning_rate": 1.8038679282990587e-05,
      "epoch": 44.19568822553897
    },
    {
      "loss": 3.1229,
      "grad_norm": 2.432250499725342,
      "learning_rate": 1.801648858614038e-05,
      "epoch": 44.27860696517413
    },
    {
      "eval_loss": 3.1851277351379395,
      "eval_runtime": 46.7158,
      "eval_samples_per_second": 88.386,
      "eval_steps_per_second": 5.544,
      "epoch": 44.27860696517413
    },
    {
      "loss": 3.0843,
      "grad_norm": 2.4447271823883057,
      "learning_rate": 1.799418687146198e-05,
      "epoch": 44.36152570480929
    },
    {
      "loss": 3.0978,
      "grad_norm": 2.445629358291626,
      "learning_rate": 1.797177444780482e-05,
      "epoch": 44.44444444444444
    },
    {
      "eval_loss": 3.1820638179779053,
      "eval_runtime": 46.4935,
      "eval_samples_per_second": 88.808,
      "eval_steps_per_second": 5.571,
      "epoch": 44.44444444444444
    },
    {
      "loss": 3.1013,
      "grad_norm": 2.4817051887512207,
      "learning_rate": 1.79492516255515e-05,
      "epoch": 44.527363184079604
    },
    {
      "loss": 3.1131,
      "grad_norm": 2.4662842750549316,
      "learning_rate": 1.7926618716613504e-05,
      "epoch": 44.61028192371476
    },
    {
      "eval_loss": 3.1703684329986572,
      "eval_runtime": 46.7174,
      "eval_samples_per_second": 88.383,
      "eval_steps_per_second": 5.544,
      "epoch": 44.61028192371476
    },
    {
      "loss": 3.1144,
      "grad_norm": 2.5037825107574463,
      "learning_rate": 1.790387603442686e-05,
      "epoch": 44.69320066334992
    },
    {
      "loss": 3.0812,
      "grad_norm": 2.528698444366455,
      "learning_rate": 1.788102389394782e-05,
      "epoch": 44.776119402985074
    },
    {
      "eval_loss": 3.1656787395477295,
      "eval_runtime": 46.229,
      "eval_samples_per_second": 89.316,
      "eval_steps_per_second": 5.603,
      "epoch": 44.776119402985074
    },
    {
      "loss": 3.1019,
      "grad_norm": 2.386319637298584,
      "learning_rate": 1.785806261164849e-05,
      "epoch": 44.859038142620236
    },
    {
      "loss": 3.1112,
      "grad_norm": 2.5193352699279785,
      "learning_rate": 1.7834992505512444e-05,
      "epoch": 44.94195688225539
    },
    {
      "eval_loss": 3.1679763793945312,
      "eval_runtime": 46.6628,
      "eval_samples_per_second": 88.486,
      "eval_steps_per_second": 5.55,
      "epoch": 44.94195688225539
    },
    {
      "loss": 3.0666,
      "grad_norm": 2.482898235321045,
      "learning_rate": 1.781181389503032e-05,
      "epoch": 45.024875621890544
    },
    {
      "loss": 3.0545,
      "grad_norm": 2.5540575981140137,
      "learning_rate": 1.7788527101195407e-05,
      "epoch": 45.107794361525706
    },
    {
      "eval_loss": 3.137481451034546,
      "eval_runtime": 46.5082,
      "eval_samples_per_second": 88.78,
      "eval_steps_per_second": 5.569,
      "epoch": 45.107794361525706
    },
    {
      "loss": 3.101,
      "grad_norm": 2.4201231002807617,
      "learning_rate": 1.776513244649918e-05,
      "epoch": 45.19071310116086
    },
    {
      "loss": 3.0856,
      "grad_norm": 2.520721673965454,
      "learning_rate": 1.7741630254926856e-05,
      "epoch": 45.27363184079602
    },
    {
      "eval_loss": 3.166499137878418,
      "eval_runtime": 46.7278,
      "eval_samples_per_second": 88.363,
      "eval_steps_per_second": 5.543,
      "epoch": 45.27363184079602
    },
    {
      "loss": 3.0661,
      "grad_norm": 2.604250907897949,
      "learning_rate": 1.7718020851952894e-05,
      "epoch": 45.356550580431175
    },
    {
      "loss": 3.0771,
      "grad_norm": 2.413618564605713,
      "learning_rate": 1.7694304564536477e-05,
      "epoch": 45.43946932006634
    },
    {
      "eval_loss": 3.1540400981903076,
      "eval_runtime": 46.7439,
      "eval_samples_per_second": 88.332,
      "eval_steps_per_second": 5.541,
      "epoch": 45.43946932006634
    },
    {
      "loss": 3.0534,
      "grad_norm": 2.516815662384033,
      "learning_rate": 1.7670481721117018e-05,
      "epoch": 45.52238805970149
    },
    {
      "loss": 3.0449,
      "grad_norm": 2.4418821334838867,
      "learning_rate": 1.7646552651609575e-05,
      "epoch": 45.60530679933665
    },
    {
      "eval_loss": 3.145939826965332,
      "eval_runtime": 46.5147,
      "eval_samples_per_second": 88.768,
      "eval_steps_per_second": 5.568,
      "epoch": 45.60530679933665
    },
    {
      "loss": 3.098,
      "grad_norm": 2.5883944034576416,
      "learning_rate": 1.76225176874003e-05,
      "epoch": 45.68822553897181
    },
    {
      "loss": 3.0636,
      "grad_norm": 2.433311939239502,
      "learning_rate": 1.759837716134186e-05,
      "epoch": 45.77114427860697
    },
    {
      "eval_loss": 3.1480653285980225,
      "eval_runtime": 46.8157,
      "eval_samples_per_second": 88.197,
      "eval_steps_per_second": 5.532,
      "epoch": 45.77114427860697
    },
    {
      "loss": 3.0824,
      "grad_norm": 2.3760573863983154,
      "learning_rate": 1.7574131407748796e-05,
      "epoch": 45.85406301824212
    },
    {
      "loss": 3.0852,
      "grad_norm": 2.440382242202759,
      "learning_rate": 1.7549780762392927e-05,
      "epoch": 45.93698175787728
    },
    {
      "eval_loss": 3.141754627227783,
      "eval_runtime": 46.9207,
      "eval_samples_per_second": 88.0,
      "eval_steps_per_second": 5.52,
      "epoch": 45.93698175787728
    },
    {
      "loss": 3.0497,
      "grad_norm": 2.4376394748687744,
      "learning_rate": 1.752532556249867e-05,
      "epoch": 46.01990049751244
    },
    {
      "loss": 3.018,
      "grad_norm": 2.3384571075439453,
      "learning_rate": 1.750076614673841e-05,
      "epoch": 46.10281923714759
    },
    {
      "eval_loss": 3.1397643089294434,
      "eval_runtime": 46.9575,
      "eval_samples_per_second": 87.931,
      "eval_steps_per_second": 5.516,
      "epoch": 46.10281923714759
    },
    {
      "loss": 3.0578,
      "grad_norm": 2.5811679363250732,
      "learning_rate": 1.7476102855227753e-05,
      "epoch": 46.185737976782754
    },
    {
      "loss": 3.028,
      "grad_norm": 2.4876203536987305,
      "learning_rate": 1.745133602952087e-05,
      "epoch": 46.26865671641791
    },
    {
      "eval_loss": 3.1347315311431885,
      "eval_runtime": 46.5704,
      "eval_samples_per_second": 88.661,
      "eval_steps_per_second": 5.561,
      "epoch": 46.26865671641791
    },
    {
      "loss": 3.0403,
      "grad_norm": 2.4626145362854004,
      "learning_rate": 1.7426466012605738e-05,
      "epoch": 46.35157545605307
    },
    {
      "loss": 3.04,
      "grad_norm": 2.494115114212036,
      "learning_rate": 1.740149314889939e-05,
      "epoch": 46.434494195688224
    },
    {
      "eval_loss": 3.135179281234741,
      "eval_runtime": 46.951,
      "eval_samples_per_second": 87.943,
      "eval_steps_per_second": 5.516,
      "epoch": 46.434494195688224
    },
    {
      "loss": 3.0379,
      "grad_norm": 2.451528787612915,
      "learning_rate": 1.737641778424317e-05,
      "epoch": 46.517412935323385
    },
    {
      "loss": 3.0639,
      "grad_norm": 2.432790517807007,
      "learning_rate": 1.7351240265897903e-05,
      "epoch": 46.60033167495854
    },
    {
      "eval_loss": 3.1284148693084717,
      "eval_runtime": 46.7171,
      "eval_samples_per_second": 88.383,
      "eval_steps_per_second": 5.544,
      "epoch": 46.60033167495854
    },
    {
      "loss": 3.0707,
      "grad_norm": 2.467133045196533,
      "learning_rate": 1.7325960942539124e-05,
      "epoch": 46.6832504145937
    },
    {
      "loss": 3.0475,
      "grad_norm": 2.4451563358306885,
      "learning_rate": 1.730058016425223e-05,
      "epoch": 46.766169154228855
    },
    {
      "eval_loss": 3.134147882461548,
      "eval_runtime": 46.6262,
      "eval_samples_per_second": 88.555,
      "eval_steps_per_second": 5.555,
      "epoch": 46.766169154228855
    },
    {
      "loss": 3.025,
      "grad_norm": 2.450732946395874,
      "learning_rate": 1.727509828252763e-05,
      "epoch": 46.84908789386402
    },
    {
      "loss": 3.0305,
      "grad_norm": 2.548595905303955,
      "learning_rate": 1.724951565025589e-05,
      "epoch": 46.93200663349917
    },
    {
      "eval_loss": 3.128800630569458,
      "eval_runtime": 46.6358,
      "eval_samples_per_second": 88.537,
      "eval_steps_per_second": 5.554,
      "epoch": 46.93200663349917
    },
    {
      "loss": 3.0352,
      "grad_norm": 2.465212821960449,
      "learning_rate": 1.722383262172283e-05,
      "epoch": 47.014925373134325
    },
    {
      "loss": 2.9999,
      "grad_norm": 2.44566011428833,
      "learning_rate": 1.7198049552604638e-05,
      "epoch": 47.09784411276949
    },
    {
      "eval_loss": 3.129486083984375,
      "eval_runtime": 46.6269,
      "eval_samples_per_second": 88.554,
      "eval_steps_per_second": 5.555,
      "epoch": 47.09784411276949
    },
    {
      "loss": 3.0492,
      "grad_norm": 2.4835731983184814,
      "learning_rate": 1.717216679996292e-05,
      "epoch": 47.18076285240464
    },
    {
      "loss": 2.9868,
      "grad_norm": 2.48227596282959,
      "learning_rate": 1.714618472223978e-05,
      "epoch": 47.2636815920398
    },
    {
      "eval_loss": 3.099191188812256,
      "eval_runtime": 46.7672,
      "eval_samples_per_second": 88.288,
      "eval_steps_per_second": 5.538,
      "epoch": 47.2636815920398
    },
    {
      "loss": 3.0217,
      "grad_norm": 2.3772072792053223,
      "learning_rate": 1.7120103679252834e-05,
      "epoch": 47.346600331674956
    },
    {
      "loss": 3.0159,
      "grad_norm": 2.4975876808166504,
      "learning_rate": 1.709392403219024e-05,
      "epoch": 47.42951907131012
    },
    {
      "eval_loss": 3.114976167678833,
      "eval_runtime": 46.5313,
      "eval_samples_per_second": 88.736,
      "eval_steps_per_second": 5.566,
      "epoch": 47.42951907131012
    },
    {
      "loss": 3.0332,
      "grad_norm": 2.465428113937378,
      "learning_rate": 1.7067646143605693e-05,
      "epoch": 47.51243781094527
    },
    {
      "loss": 3.0296,
      "grad_norm": 2.5278615951538086,
      "learning_rate": 1.704127037741341e-05,
      "epoch": 47.59535655058043
    },
    {
      "eval_loss": 3.1146786212921143,
      "eval_runtime": 46.6763,
      "eval_samples_per_second": 88.46,
      "eval_steps_per_second": 5.549,
      "epoch": 47.59535655058043
    },
    {
      "loss": 2.9877,
      "grad_norm": 2.5118701457977295,
      "learning_rate": 1.701479709888307e-05,
      "epoch": 47.67827529021559
    },
    {
      "loss": 3.015,
      "grad_norm": 2.4546666145324707,
      "learning_rate": 1.698822667463478e-05,
      "epoch": 47.76119402985075
    },
    {
      "eval_loss": 3.09824800491333,
      "eval_runtime": 46.5053,
      "eval_samples_per_second": 88.786,
      "eval_steps_per_second": 5.569,
      "epoch": 47.76119402985075
    },
    {
      "loss": 3.0351,
      "grad_norm": 2.539360284805298,
      "learning_rate": 1.6961559472633985e-05,
      "epoch": 47.8441127694859
    },
    {
      "loss": 3.0419,
      "grad_norm": 2.5462965965270996,
      "learning_rate": 1.6934795862186382e-05,
      "epoch": 47.927031509121065
    },
    {
      "eval_loss": 3.0991084575653076,
      "eval_runtime": 46.569,
      "eval_samples_per_second": 88.664,
      "eval_steps_per_second": 5.562,
      "epoch": 47.927031509121065
    },
    {
      "loss": 2.9759,
      "grad_norm": 2.556544303894043,
      "learning_rate": 1.6907936213932787e-05,
      "epoch": 48.00995024875622
    },
    {
      "loss": 2.9831,
      "grad_norm": 2.528665065765381,
      "learning_rate": 1.6880980899844013e-05,
      "epoch": 48.09286898839137
    },
    {
      "eval_loss": 3.0816140174865723,
      "eval_runtime": 47.2798,
      "eval_samples_per_second": 87.331,
      "eval_steps_per_second": 5.478,
      "epoch": 48.09286898839137
    },
    {
      "loss": 3.0037,
      "grad_norm": 2.507071018218994,
      "learning_rate": 1.6853930293215735e-05,
      "epoch": 48.175787728026535
    },
    {
      "loss": 2.9989,
      "grad_norm": 2.356647253036499,
      "learning_rate": 1.6826784768663283e-05,
      "epoch": 48.25870646766169
    },
    {
      "eval_loss": 3.091029167175293,
      "eval_runtime": 46.3085,
      "eval_samples_per_second": 89.163,
      "eval_steps_per_second": 5.593,
      "epoch": 48.25870646766169
    },
    {
      "loss": 2.9946,
      "grad_norm": 2.5230414867401123,
      "learning_rate": 1.6799544702116487e-05,
      "epoch": 48.34162520729685
    },
    {
      "loss": 3.0072,
      "grad_norm": 2.5416152477264404,
      "learning_rate": 1.6772210470814464e-05,
      "epoch": 48.424543946932005
    },
    {
      "eval_loss": 3.087475299835205,
      "eval_runtime": 46.6005,
      "eval_samples_per_second": 88.604,
      "eval_steps_per_second": 5.558,
      "epoch": 48.424543946932005
    },
    {
      "loss": 3.0112,
      "grad_norm": 2.5149576663970947,
      "learning_rate": 1.6744782453300378e-05,
      "epoch": 48.507462686567166
    },
    {
      "loss": 3.0084,
      "grad_norm": 2.525012254714966,
      "learning_rate": 1.6717261029416218e-05,
      "epoch": 48.59038142620232
    },
    {
      "eval_loss": 3.0860629081726074,
      "eval_runtime": 47.4964,
      "eval_samples_per_second": 86.933,
      "eval_steps_per_second": 5.453,
      "epoch": 48.59038142620232
    },
    {
      "loss": 3.0229,
      "grad_norm": 2.4823036193847656,
      "learning_rate": 1.6689646580297518e-05,
      "epoch": 48.67330016583748
    },
    {
      "loss": 2.9931,
      "grad_norm": 2.6646981239318848,
      "learning_rate": 1.6661939488368098e-05,
      "epoch": 48.756218905472636
    },
    {
      "eval_loss": 3.071071147918701,
      "eval_runtime": 46.8086,
      "eval_samples_per_second": 88.21,
      "eval_steps_per_second": 5.533,
      "epoch": 48.756218905472636
    },
    {
      "loss": 2.9668,
      "grad_norm": 2.534306049346924,
      "learning_rate": 1.663414013733476e-05,
      "epoch": 48.8391376451078
    },
    {
      "loss": 2.9706,
      "grad_norm": 2.572962522506714,
      "learning_rate": 1.660624891218197e-05,
      "epoch": 48.92205638474295
    },
    {
      "eval_loss": 3.0664379596710205,
      "eval_runtime": 46.666,
      "eval_samples_per_second": 88.48,
      "eval_steps_per_second": 5.55,
      "epoch": 48.92205638474295
    },
    {
      "loss": 2.9612,
      "grad_norm": 2.543949604034424,
      "learning_rate": 1.6578266199166523e-05,
      "epoch": 49.004975124378106
    },
    {
      "loss": 2.9885,
      "grad_norm": 2.394331932067871,
      "learning_rate": 1.6550192385812212e-05,
      "epoch": 49.08789386401327
    },
    {
      "eval_loss": 3.0687315464019775,
      "eval_runtime": 46.5749,
      "eval_samples_per_second": 88.653,
      "eval_steps_per_second": 5.561,
      "epoch": 49.08789386401327
    },
    {
      "loss": 2.9756,
      "grad_norm": 2.518510580062866,
      "learning_rate": 1.652202786090445e-05,
      "epoch": 49.17081260364842
    },
    {
      "loss": 2.9577,
      "grad_norm": 2.5125629901885986,
      "learning_rate": 1.6493773014484867e-05,
      "epoch": 49.25373134328358
    },
    {
      "eval_loss": 3.048915147781372,
      "eval_runtime": 46.5876,
      "eval_samples_per_second": 88.629,
      "eval_steps_per_second": 5.559,
      "epoch": 49.25373134328358
    },
    {
      "loss": 2.9612,
      "grad_norm": 2.613164186477661,
      "learning_rate": 1.6465428237845954e-05,
      "epoch": 49.33665008291874
    },
    {
      "loss": 2.9911,
      "grad_norm": 2.4945499897003174,
      "learning_rate": 1.6436993923525606e-05,
      "epoch": 49.4195688225539
    },
    {
      "eval_loss": 3.0574257373809814,
      "eval_runtime": 46.2767,
      "eval_samples_per_second": 89.224,
      "eval_steps_per_second": 5.597,
      "epoch": 49.4195688225539
    },
    {
      "loss": 2.983,
      "grad_norm": 2.4711897373199463,
      "learning_rate": 1.640847046530168e-05,
      "epoch": 49.50248756218905
    },
    {
      "loss": 2.9607,
      "grad_norm": 2.4786531925201416,
      "learning_rate": 1.6379858258186593e-05,
      "epoch": 49.585406301824214
    },
    {
      "eval_loss": 3.068362236022949,
      "eval_runtime": 46.6953,
      "eval_samples_per_second": 88.424,
      "eval_steps_per_second": 5.547,
      "epoch": 49.585406301824214
    },
    {
      "loss": 2.9677,
      "grad_norm": 2.5878283977508545,
      "learning_rate": 1.635115769842179e-05,
      "epoch": 49.66832504145937
    },
    {
      "loss": 2.9452,
      "grad_norm": 2.543532371520996,
      "learning_rate": 1.6322369183472293e-05,
      "epoch": 49.75124378109453
    },
    {
      "eval_loss": 3.0626111030578613,
      "eval_runtime": 46.636,
      "eval_samples_per_second": 88.537,
      "eval_steps_per_second": 5.554,
      "epoch": 49.75124378109453
    },
    {
      "loss": 2.9639,
      "grad_norm": 2.5302019119262695,
      "learning_rate": 1.6293493112021188e-05,
      "epoch": 49.834162520729684
    },
    {
      "loss": 2.9311,
      "grad_norm": 2.383892059326172,
      "learning_rate": 1.62645298839641e-05,
      "epoch": 49.917081260364846
    },
    {
      "eval_loss": 3.068281888961792,
      "eval_runtime": 46.6559,
      "eval_samples_per_second": 88.499,
      "eval_steps_per_second": 5.551,
      "epoch": 49.917081260364846
    },
    {
      "loss": 2.9562,
      "grad_norm": 2.6648802757263184,
      "learning_rate": 1.623547990040367e-05,
      "epoch": 50.0
    },
    {
      "loss": 2.9429,
      "grad_norm": 2.5349347591400146,
      "learning_rate": 1.6206343563643975e-05,
      "epoch": 50.082918739635154
    },
    {
      "eval_loss": 3.048734664916992,
      "eval_runtime": 46.5439,
      "eval_samples_per_second": 88.712,
      "eval_steps_per_second": 5.565,
      "epoch": 50.082918739635154
    },
    {
      "loss": 2.9494,
      "grad_norm": 2.568498373031616,
      "learning_rate": 1.6177121277184978e-05,
      "epoch": 50.165837479270316
    },
    {
      "loss": 2.9022,
      "grad_norm": 2.634613275527954,
      "learning_rate": 1.6147813445716926e-05,
      "epoch": 50.24875621890547
    },
    {
      "eval_loss": 3.0394341945648193,
      "eval_runtime": 46.6502,
      "eval_samples_per_second": 88.51,
      "eval_steps_per_second": 5.552,
      "epoch": 50.24875621890547
    },
    {
      "loss": 2.9474,
      "grad_norm": 2.3747339248657227,
      "learning_rate": 1.611842047511476e-05,
      "epoch": 50.33167495854063
    },
    {
      "loss": 2.9657,
      "grad_norm": 2.555813789367676,
      "learning_rate": 1.6088942772432478e-05,
      "epoch": 50.414593698175786
    },
    {
      "eval_loss": 3.0622029304504395,
      "eval_runtime": 46.481,
      "eval_samples_per_second": 88.832,
      "eval_steps_per_second": 5.572,
      "epoch": 50.414593698175786
    },
    {
      "loss": 2.9246,
      "grad_norm": 2.5512185096740723,
      "learning_rate": 1.6059380745897514e-05,
      "epoch": 50.49751243781095
    },
    {
      "loss": 2.9424,
      "grad_norm": 2.5129876136779785,
      "learning_rate": 1.6029734804905072e-05,
      "epoch": 50.5804311774461
    },
    {
      "eval_loss": 3.0330963134765625,
      "eval_runtime": 46.6105,
      "eval_samples_per_second": 88.585,
      "eval_steps_per_second": 5.557,
      "epoch": 50.5804311774461
    },
    {
      "loss": 2.9518,
      "grad_norm": 2.7016046047210693,
      "learning_rate": 1.6000005360012455e-05,
      "epoch": 50.66334991708126
    },
    {
      "loss": 2.9389,
      "grad_norm": 2.5099101066589355,
      "learning_rate": 1.59701928229334e-05,
      "epoch": 50.74626865671642
    },
    {
      "eval_loss": 3.030937910079956,
      "eval_runtime": 46.6349,
      "eval_samples_per_second": 88.539,
      "eval_steps_per_second": 5.554,
      "epoch": 50.74626865671642
    },
    {
      "loss": 2.927,
      "grad_norm": 2.4500513076782227,
      "learning_rate": 1.5940297606532347e-05,
      "epoch": 50.82918739635158
    },
    {
      "loss": 2.9042,
      "grad_norm": 2.5788252353668213,
      "learning_rate": 1.5910320124818745e-05,
      "epoch": 50.91210613598673
    },
    {
      "eval_loss": 3.044794797897339,
      "eval_runtime": 46.8156,
      "eval_samples_per_second": 88.197,
      "eval_steps_per_second": 5.532,
      "epoch": 50.91210613598673
    },
    {
      "loss": 2.9466,
      "grad_norm": 2.5610272884368896,
      "learning_rate": 1.5880260792941303e-05,
      "epoch": 50.995024875621894
    },
    {
      "loss": 2.9128,
      "grad_norm": 2.6072025299072266,
      "learning_rate": 1.5850120027182257e-05,
      "epoch": 51.07794361525705
    },
    {
      "eval_loss": 3.0288426876068115,
      "eval_runtime": 46.722,
      "eval_samples_per_second": 88.374,
      "eval_steps_per_second": 5.543,
      "epoch": 51.07794361525705
    },
    {
      "loss": 2.9199,
      "grad_norm": 2.343956470489502,
      "learning_rate": 1.5819898244951583e-05,
      "epoch": 51.1608623548922
    },
    {
      "loss": 2.9169,
      "grad_norm": 2.5094082355499268,
      "learning_rate": 1.5789595864781237e-05,
      "epoch": 51.243781094527364
    },
    {
      "eval_loss": 3.020921468734741,
      "eval_runtime": 46.6997,
      "eval_samples_per_second": 88.416,
      "eval_steps_per_second": 5.546,
      "epoch": 51.243781094527364
    },
    {
      "loss": 2.9109,
      "grad_norm": 2.539949893951416,
      "learning_rate": 1.5759213306319344e-05,
      "epoch": 51.32669983416252
    },
    {
      "loss": 2.9377,
      "grad_norm": 2.563650131225586,
      "learning_rate": 1.5728750990324404e-05,
      "epoch": 51.40961857379768
    },
    {
      "eval_loss": 3.0287253856658936,
      "eval_runtime": 46.727,
      "eval_samples_per_second": 88.364,
      "eval_steps_per_second": 5.543,
      "epoch": 51.40961857379768
    },
    {
      "loss": 2.9109,
      "grad_norm": 2.5788350105285645,
      "learning_rate": 1.5698209338659442e-05,
      "epoch": 51.492537313432834
    },
    {
      "loss": 2.9463,
      "grad_norm": 2.521111011505127,
      "learning_rate": 1.5667588774286183e-05,
      "epoch": 51.575456053067995
    },
    {
      "eval_loss": 3.0280721187591553,
      "eval_runtime": 47.8281,
      "eval_samples_per_second": 86.33,
      "eval_steps_per_second": 5.415,
      "epoch": 51.575456053067995
    },
    {
      "loss": 2.8894,
      "grad_norm": 2.4885871410369873,
      "learning_rate": 1.563688972125919e-05,
      "epoch": 51.65837479270315
    },
    {
      "loss": 2.9117,
      "grad_norm": 2.5623326301574707,
      "learning_rate": 1.5606112604719985e-05,
      "epoch": 51.74129353233831
    },
    {
      "eval_loss": 3.0278849601745605,
      "eval_runtime": 46.3436,
      "eval_samples_per_second": 89.095,
      "eval_steps_per_second": 5.589,
      "epoch": 51.74129353233831
    },
    {
      "loss": 2.9209,
      "grad_norm": 2.5480642318725586,
      "learning_rate": 1.5575257850891178e-05,
      "epoch": 51.824212271973465
    },
    {
      "loss": 2.9052,
      "grad_norm": 2.4937663078308105,
      "learning_rate": 1.5544325887070546e-05,
      "epoch": 51.90713101160863
    },
    {
      "eval_loss": 3.017292022705078,
      "eval_runtime": 46.4539,
      "eval_samples_per_second": 88.884,
      "eval_steps_per_second": 5.575,
      "epoch": 51.90713101160863
    },
    {
      "loss": 2.913,
      "grad_norm": 2.5054116249084473,
      "learning_rate": 1.551331714162512e-05,
      "epoch": 51.99004975124378
    },
    {
      "loss": 2.8999,
      "grad_norm": 2.5907294750213623,
      "learning_rate": 1.548223204398526e-05,
      "epoch": 52.072968490878935
    },
    {
      "eval_loss": 3.0350613594055176,
      "eval_runtime": 47.3577,
      "eval_samples_per_second": 87.187,
      "eval_steps_per_second": 5.469,
      "epoch": 52.072968490878935
    },
    {
      "loss": 2.9056,
      "grad_norm": 2.5789270401000977,
      "learning_rate": 1.5451071024638707e-05,
      "epoch": 52.1558872305141
    },
    {
      "loss": 2.9129,
      "grad_norm": 2.5710175037384033,
      "learning_rate": 1.541983451512461e-05,
      "epoch": 52.23880597014925
    },
    {
      "eval_loss": 3.0060997009277344,
      "eval_runtime": 46.3191,
      "eval_samples_per_second": 89.142,
      "eval_steps_per_second": 5.592,
      "epoch": 52.23880597014925
    },
    {
      "loss": 2.901,
      "grad_norm": 2.520054817199707,
      "learning_rate": 1.538852294802756e-05,
      "epoch": 52.32172470978441
    },
    {
      "loss": 2.8739,
      "grad_norm": 2.5241520404815674,
      "learning_rate": 1.53571367569716e-05,
      "epoch": 52.40464344941957
    },
    {
      "eval_loss": 3.004136800765991,
      "eval_runtime": 46.3526,
      "eval_samples_per_second": 89.078,
      "eval_steps_per_second": 5.588,
      "epoch": 52.40464344941957
    },
    {
      "loss": 2.8857,
      "grad_norm": 2.4072535037994385,
      "learning_rate": 1.5325676376614215e-05,
      "epoch": 52.48756218905473
    },
    {
      "loss": 2.9151,
      "grad_norm": 2.557680130004883,
      "learning_rate": 1.5294142242640317e-05,
      "epoch": 52.57048092868988
    },
    {
      "eval_loss": 3.0087156295776367,
      "eval_runtime": 46.3768,
      "eval_samples_per_second": 89.032,
      "eval_steps_per_second": 5.585,
      "epoch": 52.57048092868988
    },
    {
      "loss": 2.8962,
      "grad_norm": 2.5504798889160156,
      "learning_rate": 1.5262534791756202e-05,
      "epoch": 52.653399668325044
    },
    {
      "loss": 2.8675,
      "grad_norm": 2.614145278930664,
      "learning_rate": 1.5230854461683513e-05,
      "epoch": 52.7363184079602
    },
    {
      "eval_loss": 3.0131402015686035,
      "eval_runtime": 46.5578,
      "eval_samples_per_second": 88.685,
      "eval_steps_per_second": 5.563,
      "epoch": 52.7363184079602
    },
    {
      "loss": 2.8887,
      "grad_norm": 2.56950306892395,
      "learning_rate": 1.5199101691153175e-05,
      "epoch": 52.81923714759536
    },
    {
      "loss": 2.9056,
      "grad_norm": 2.5375986099243164,
      "learning_rate": 1.5167276919899316e-05,
      "epoch": 52.90215588723051
    },
    {
      "eval_loss": 2.9874255657196045,
      "eval_runtime": 46.6247,
      "eval_samples_per_second": 88.558,
      "eval_steps_per_second": 5.555,
      "epoch": 52.90215588723051
    },
    {
      "loss": 2.8693,
      "grad_norm": 2.484983444213867,
      "learning_rate": 1.5135380588653176e-05,
      "epoch": 52.985074626865675
    },
    {
      "loss": 2.8791,
      "grad_norm": 2.5699238777160645,
      "learning_rate": 1.5103413139137019e-05,
      "epoch": 53.06799336650083
    },
    {
      "eval_loss": 2.9942455291748047,
      "eval_runtime": 46.6245,
      "eval_samples_per_second": 88.559,
      "eval_steps_per_second": 5.555,
      "epoch": 53.06799336650083
    },
    {
      "loss": 2.902,
      "grad_norm": 2.48618483543396,
      "learning_rate": 1.507137501405799e-05,
      "epoch": 53.15091210613598
    },
    {
      "loss": 2.8969,
      "grad_norm": 2.517817258834839,
      "learning_rate": 1.5039266657102005e-05,
      "epoch": 53.233830845771145
    },
    {
      "eval_loss": 2.989356279373169,
      "eval_runtime": 47.6952,
      "eval_samples_per_second": 86.57,
      "eval_steps_per_second": 5.43,
      "epoch": 53.233830845771145
    },
    {
      "loss": 2.8857,
      "grad_norm": 2.560030698776245,
      "learning_rate": 1.5007088512927594e-05,
      "epoch": 53.3167495854063
    },
    {
      "loss": 2.8532,
      "grad_norm": 2.500488758087158,
      "learning_rate": 1.4974841027159757e-05,
      "epoch": 53.39966832504146
    },
    {
      "eval_loss": 2.986487627029419,
      "eval_runtime": 46.5139,
      "eval_samples_per_second": 88.769,
      "eval_steps_per_second": 5.568,
      "epoch": 53.39966832504146
    },
    {
      "loss": 2.8677,
      "grad_norm": 2.6393163204193115,
      "learning_rate": 1.4942524646383773e-05,
      "epoch": 53.482587064676615
    },
    {
      "loss": 2.8637,
      "grad_norm": 2.684361457824707,
      "learning_rate": 1.4910139818139035e-05,
      "epoch": 53.565505804311776
    },
    {
      "eval_loss": 2.9830424785614014,
      "eval_runtime": 46.5391,
      "eval_samples_per_second": 88.721,
      "eval_steps_per_second": 5.565,
      "epoch": 53.565505804311776
    },
    {
      "loss": 2.8688,
      "grad_norm": 2.492476224899292,
      "learning_rate": 1.4877686990912837e-05,
      "epoch": 53.64842454394693
    },
    {
      "loss": 2.8497,
      "grad_norm": 2.5351498126983643,
      "learning_rate": 1.4845166614134174e-05,
      "epoch": 53.73134328358209
    },
    {
      "eval_loss": 2.985645055770874,
      "eval_runtime": 46.5443,
      "eval_samples_per_second": 88.711,
      "eval_steps_per_second": 5.565,
      "epoch": 53.73134328358209
    },
    {
      "loss": 2.8582,
      "grad_norm": 2.5832200050354004,
      "learning_rate": 1.4812579138167515e-05,
      "epoch": 53.814262023217246
    },
    {
      "loss": 2.8607,
      "grad_norm": 2.5878021717071533,
      "learning_rate": 1.477992501430656e-05,
      "epoch": 53.89718076285241
    },
    {
      "eval_loss": 2.988900899887085,
      "eval_runtime": 46.6254,
      "eval_samples_per_second": 88.557,
      "eval_steps_per_second": 5.555,
      "epoch": 53.89718076285241
    },
    {
      "loss": 2.8806,
      "grad_norm": 2.5666322708129883,
      "learning_rate": 1.4747204694767993e-05,
      "epoch": 53.98009950248756
    },
    {
      "loss": 2.8542,
      "grad_norm": 2.5242598056793213,
      "learning_rate": 1.4714418632685226e-05,
      "epoch": 54.06301824212272
    },
    {
      "eval_loss": 2.980658531188965,
      "eval_runtime": 46.592,
      "eval_samples_per_second": 88.62,
      "eval_steps_per_second": 5.559,
      "epoch": 54.06301824212272
    },
    {
      "loss": 2.8398,
      "grad_norm": 2.5231773853302,
      "learning_rate": 1.4681567282102119e-05,
      "epoch": 54.14593698175788
    },
    {
      "loss": 2.838,
      "grad_norm": 2.6738457679748535,
      "learning_rate": 1.4648651097966689e-05,
      "epoch": 54.22885572139303
    },
    {
      "eval_loss": 2.9645206928253174,
      "eval_runtime": 46.4868,
      "eval_samples_per_second": 88.821,
      "eval_steps_per_second": 5.571,
      "epoch": 54.22885572139303
    },
    {
      "loss": 2.888,
      "grad_norm": 2.5851171016693115,
      "learning_rate": 1.4615670536124816e-05,
      "epoch": 54.31177446102819
    },
    {
      "loss": 2.8591,
      "grad_norm": 2.5065882205963135,
      "learning_rate": 1.4582626053313917e-05,
      "epoch": 54.39469320066335
    },
    {
      "eval_loss": 2.9654603004455566,
      "eval_runtime": 46.5517,
      "eval_samples_per_second": 88.697,
      "eval_steps_per_second": 5.564,
      "epoch": 54.39469320066335
    },
    {
      "loss": 2.829,
      "grad_norm": 2.5043113231658936,
      "learning_rate": 1.454951810715665e-05,
      "epoch": 54.47761194029851
    },
    {
      "loss": 2.8286,
      "grad_norm": 2.5150234699249268,
      "learning_rate": 1.4516347156154536e-05,
      "epoch": 54.56053067993366
    },
    {
      "eval_loss": 2.9718544483184814,
      "eval_runtime": 47.3565,
      "eval_samples_per_second": 87.19,
      "eval_steps_per_second": 5.469,
      "epoch": 54.56053067993366
    },
    {
      "loss": 2.8595,
      "grad_norm": 2.550483465194702,
      "learning_rate": 1.4483113659681639e-05,
      "epoch": 54.643449419568825
    },
    {
      "loss": 2.8573,
      "grad_norm": 2.5297393798828125,
      "learning_rate": 1.4449818077978193e-05,
      "epoch": 54.72636815920398
    },
    {
      "eval_loss": 2.9673385620117188,
      "eval_runtime": 46.6361,
      "eval_samples_per_second": 88.537,
      "eval_steps_per_second": 5.554,
      "epoch": 54.72636815920398
    },
    {
      "loss": 2.8553,
      "grad_norm": 2.60417103767395,
      "learning_rate": 1.4416460872144237e-05,
      "epoch": 54.80928689883914
    },
    {
      "loss": 2.8685,
      "grad_norm": 2.4599697589874268,
      "learning_rate": 1.4383042504133222e-05,
      "epoch": 54.892205638474294
    },
    {
      "eval_loss": 2.9828858375549316,
      "eval_runtime": 46.6381,
      "eval_samples_per_second": 88.533,
      "eval_steps_per_second": 5.553,
      "epoch": 54.892205638474294
    },
    {
      "loss": 2.855,
      "grad_norm": 2.670830726623535,
      "learning_rate": 1.434956343674561e-05,
      "epoch": 54.975124378109456
    },
    {
      "loss": 2.815,
      "grad_norm": 2.4789793491363525,
      "learning_rate": 1.431602413362247e-05,
      "epoch": 55.05804311774461
    },
    {
      "eval_loss": 2.9606502056121826,
      "eval_runtime": 46.5227,
      "eval_samples_per_second": 88.752,
      "eval_steps_per_second": 5.567,
      "epoch": 55.05804311774461
    },
    {
      "loss": 2.8352,
      "grad_norm": 2.453433036804199,
      "learning_rate": 1.4282425059239057e-05,
      "epoch": 55.140961857379764
    },
    {
      "loss": 2.8291,
      "grad_norm": 2.574197769165039,
      "learning_rate": 1.4248766678898386e-05,
      "epoch": 55.223880597014926
    },
    {
      "eval_loss": 2.9615843296051025,
      "eval_runtime": 46.588,
      "eval_samples_per_second": 88.628,
      "eval_steps_per_second": 5.559,
      "epoch": 55.223880597014926
    },
    {
      "loss": 2.8014,
      "grad_norm": 2.54305362701416,
      "learning_rate": 1.4215049458724773e-05,
      "epoch": 55.30679933665008
    },
    {
      "loss": 2.8328,
      "grad_norm": 2.4847981929779053,
      "learning_rate": 1.418127386565739e-05,
      "epoch": 55.38971807628524
    },
    {
      "eval_loss": 2.9554615020751953,
      "eval_runtime": 46.4708,
      "eval_samples_per_second": 88.851,
      "eval_steps_per_second": 5.573,
      "epoch": 55.38971807628524
    },
    {
      "loss": 2.8268,
      "grad_norm": 2.6550791263580322,
      "learning_rate": 1.4147440367443803e-05,
      "epoch": 55.472636815920396
    },
    {
      "loss": 2.8104,
      "grad_norm": 2.546527862548828,
      "learning_rate": 1.411354943263348e-05,
      "epoch": 55.55555555555556
    },
    {
      "eval_loss": 2.9644298553466797,
      "eval_runtime": 46.3185,
      "eval_samples_per_second": 89.144,
      "eval_steps_per_second": 5.592,
      "epoch": 55.55555555555556
    },
    {
      "loss": 2.8472,
      "grad_norm": 2.589482545852661,
      "learning_rate": 1.4079601530571317e-05,
      "epoch": 55.63847429519071
    },
    {
      "loss": 2.8436,
      "grad_norm": 2.6712701320648193,
      "learning_rate": 1.4045597131391127e-05,
      "epoch": 55.72139303482587
    },
    {
      "eval_loss": 2.9586052894592285,
      "eval_runtime": 46.6148,
      "eval_samples_per_second": 88.577,
      "eval_steps_per_second": 5.556,
      "epoch": 55.72139303482587
    },
    {
      "loss": 2.8534,
      "grad_norm": 2.4942383766174316,
      "learning_rate": 1.4011536706009134e-05,
      "epoch": 55.80431177446103
    },
    {
      "loss": 2.8446,
      "grad_norm": 2.618297815322876,
      "learning_rate": 1.3977420726117457e-05,
      "epoch": 55.88723051409619
    },
    {
      "eval_loss": 2.9357962608337402,
      "eval_runtime": 46.5201,
      "eval_samples_per_second": 88.757,
      "eval_steps_per_second": 5.567,
      "epoch": 55.88723051409619
    },
    {
      "loss": 2.8304,
      "grad_norm": 2.5162734985351562,
      "learning_rate": 1.3943249664177563e-05,
      "epoch": 55.97014925373134
    },
    {
      "loss": 2.831,
      "grad_norm": 2.5390002727508545,
      "learning_rate": 1.3909023993413737e-05,
      "epoch": 56.053067993366504
    },
    {
      "eval_loss": 2.9519405364990234,
      "eval_runtime": 46.6073,
      "eval_samples_per_second": 88.591,
      "eval_steps_per_second": 5.557,
      "epoch": 56.053067993366504
    },
    {
      "loss": 2.7823,
      "grad_norm": 2.655472755432129,
      "learning_rate": 1.3874744187806528e-05,
      "epoch": 56.13598673300166
    },
    {
      "loss": 2.8286,
      "grad_norm": 2.5716376304626465,
      "learning_rate": 1.3840410722086182e-05,
      "epoch": 56.21890547263681
    },
    {
      "eval_loss": 2.948415994644165,
      "eval_runtime": 46.6412,
      "eval_samples_per_second": 88.527,
      "eval_steps_per_second": 5.553,
      "epoch": 56.21890547263681
    },
    {
      "loss": 2.8347,
      "grad_norm": 2.556330919265747,
      "learning_rate": 1.3806024071726054e-05,
      "epoch": 56.301824212271974
    },
    {
      "loss": 2.8089,
      "grad_norm": 2.545499801635742,
      "learning_rate": 1.3771584712936053e-05,
      "epoch": 56.38474295190713
    },
    {
      "eval_loss": 2.944415330886841,
      "eval_runtime": 46.6759,
      "eval_samples_per_second": 88.461,
      "eval_steps_per_second": 5.549,
      "epoch": 56.38474295190713
    },
    {
      "loss": 2.7908,
      "grad_norm": 2.4983608722686768,
      "learning_rate": 1.3737093122656017e-05,
      "epoch": 56.46766169154229
    },
    {
      "loss": 2.8219,
      "grad_norm": 2.48659610748291,
      "learning_rate": 1.3702549778549134e-05,
      "epoch": 56.550580431177444
    },
    {
      "eval_loss": 2.9392316341400146,
      "eval_runtime": 46.4908,
      "eval_samples_per_second": 88.813,
      "eval_steps_per_second": 5.571,
      "epoch": 56.550580431177444
    },
    {
      "loss": 2.7944,
      "grad_norm": 2.496427536010742,
      "learning_rate": 1.3667955158995305e-05,
      "epoch": 56.633499170812605
    },
    {
      "loss": 2.791,
      "grad_norm": 2.638976573944092,
      "learning_rate": 1.3633309743084535e-05,
      "epoch": 56.71641791044776
    },
    {
      "eval_loss": 2.9452617168426514,
      "eval_runtime": 46.6291,
      "eval_samples_per_second": 88.55,
      "eval_steps_per_second": 5.554,
      "epoch": 56.71641791044776
    },
    {
      "loss": 2.8304,
      "grad_norm": 2.555149793624878,
      "learning_rate": 1.3598614010610282e-05,
      "epoch": 56.79933665008292
    },
    {
      "loss": 2.8382,
      "grad_norm": 2.649564266204834,
      "learning_rate": 1.3563868442062836e-05,
      "epoch": 56.882255389718075
    },
    {
      "eval_loss": 2.9350993633270264,
      "eval_runtime": 46.7675,
      "eval_samples_per_second": 88.288,
      "eval_steps_per_second": 5.538,
      "epoch": 56.882255389718075
    },
    {
      "loss": 2.8108,
      "grad_norm": 2.714327096939087,
      "learning_rate": 1.3529073518622637e-05,
      "epoch": 56.96517412935324
    },
    {
      "loss": 2.8111,
      "grad_norm": 2.571256399154663,
      "learning_rate": 1.3494229722153638e-05,
      "epoch": 57.04809286898839
    },
    {
      "eval_loss": 2.9408175945281982,
      "eval_runtime": 46.5543,
      "eval_samples_per_second": 88.692,
      "eval_steps_per_second": 5.563,
      "epoch": 57.04809286898839
    },
    {
      "loss": 2.758,
      "grad_norm": 2.506542444229126,
      "learning_rate": 1.3459337535196607e-05,
      "epoch": 57.13101160862355
    },
    {
      "loss": 2.8052,
      "grad_norm": 2.619424343109131,
      "learning_rate": 1.3424397440962467e-05,
      "epoch": 57.21393034825871
    },
    {
      "eval_loss": 2.9472968578338623,
      "eval_runtime": 46.5668,
      "eval_samples_per_second": 88.668,
      "eval_steps_per_second": 5.562,
      "epoch": 57.21393034825871
    },
    {
      "loss": 2.7832,
      "grad_norm": 2.651806354522705,
      "learning_rate": 1.3389409923325592e-05,
      "epoch": 57.29684908789386
    },
    {
      "loss": 2.7928,
      "grad_norm": 2.597675323486328,
      "learning_rate": 1.3354375466817101e-05,
      "epoch": 57.37976782752902
    },
    {
      "eval_loss": 2.928577423095703,
      "eval_runtime": 46.4955,
      "eval_samples_per_second": 88.804,
      "eval_steps_per_second": 5.57,
      "epoch": 57.37976782752902
    },
    {
      "loss": 2.8055,
      "grad_norm": 2.563458204269409,
      "learning_rate": 1.3319294556618164e-05,
      "epoch": 57.46268656716418
    },
    {
      "loss": 2.784,
      "grad_norm": 2.6206090450286865,
      "learning_rate": 1.3284167678553263e-05,
      "epoch": 57.54560530679934
    },
    {
      "eval_loss": 2.92002010345459,
      "eval_runtime": 46.4138,
      "eval_samples_per_second": 88.961,
      "eval_steps_per_second": 5.58,
      "epoch": 57.54560530679934
    },
    {
      "loss": 2.7915,
      "grad_norm": 2.594430446624756,
      "learning_rate": 1.3248995319083484e-05,
      "epoch": 57.62852404643449
    },
    {
      "loss": 2.8192,
      "grad_norm": 2.712989330291748,
      "learning_rate": 1.3213777965299768e-05,
      "epoch": 57.711442786069654
    },
    {
      "eval_loss": 2.919623374938965,
      "eval_runtime": 46.3358,
      "eval_samples_per_second": 89.11,
      "eval_steps_per_second": 5.59,
      "epoch": 57.711442786069654
    },
    {
      "loss": 2.8045,
      "grad_norm": 2.787637233734131,
      "learning_rate": 1.3178516104916162e-05,
      "epoch": 57.79436152570481
    },
    {
      "loss": 2.7579,
      "grad_norm": 2.454118251800537,
      "learning_rate": 1.3143210226263082e-05,
      "epoch": 57.87728026533997
    },
    {
      "eval_loss": 2.926647424697876,
      "eval_runtime": 46.4459,
      "eval_samples_per_second": 88.899,
      "eval_steps_per_second": 5.576,
      "epoch": 57.87728026533997
    },
    {
      "loss": 2.8001,
      "grad_norm": 2.577587127685547,
      "learning_rate": 1.3107860818280533e-05,
      "epoch": 57.960199004975124
    },
    {
      "loss": 2.7777,
      "grad_norm": 2.5629026889801025,
      "learning_rate": 1.307246837051134e-05,
      "epoch": 58.043117744610285
    },
    {
      "eval_loss": 2.918654203414917,
      "eval_runtime": 46.2209,
      "eval_samples_per_second": 89.332,
      "eval_steps_per_second": 5.604,
      "epoch": 58.043117744610285
    },
    {
      "loss": 2.7602,
      "grad_norm": 2.7232589721679688,
      "learning_rate": 1.3037033373094377e-05,
      "epoch": 58.12603648424544
    },
    {
      "loss": 2.7924,
      "grad_norm": 2.5203351974487305,
      "learning_rate": 1.3001556316757773e-05,
      "epoch": 58.208955223880594
    },
    {
      "eval_loss": 2.911079168319702,
      "eval_runtime": 46.0328,
      "eval_samples_per_second": 89.697,
      "eval_steps_per_second": 5.626,
      "epoch": 58.208955223880594
    },
    {
      "loss": 2.7603,
      "grad_norm": 2.709641218185425,
      "learning_rate": 1.2966037692812124e-05,
      "epoch": 58.291873963515755
    },
    {
      "loss": 2.7478,
      "grad_norm": 2.572925090789795,
      "learning_rate": 1.2930477993143676e-05,
      "epoch": 58.37479270315091
    },
    {
      "eval_loss": 2.9151082038879395,
      "eval_runtime": 45.7232,
      "eval_samples_per_second": 90.304,
      "eval_steps_per_second": 5.665,
      "epoch": 58.37479270315091
    },
    {
      "loss": 2.8009,
      "grad_norm": 2.6554410457611084,
      "learning_rate": 1.2894877710207519e-05,
      "epoch": 58.45771144278607
    },
    {
      "loss": 2.7902,
      "grad_norm": 2.5887107849121094,
      "learning_rate": 1.2859237337020774e-05,
      "epoch": 58.540630182421225
    },
    {
      "eval_loss": 2.92311429977417,
      "eval_runtime": 45.5972,
      "eval_samples_per_second": 90.554,
      "eval_steps_per_second": 5.68,
      "epoch": 58.540630182421225
    },
    {
      "loss": 2.7765,
      "grad_norm": 2.5305638313293457,
      "learning_rate": 1.2823557367155753e-05,
      "epoch": 58.623548922056386
    },
    {
      "loss": 2.7611,
      "grad_norm": 2.7038261890411377,
      "learning_rate": 1.2787838294733132e-05,
      "epoch": 58.70646766169154
    },
    {
      "eval_loss": 2.8994140625,
      "eval_runtime": 45.173,
      "eval_samples_per_second": 91.404,
      "eval_steps_per_second": 5.734,
      "epoch": 58.70646766169154
    },
    {
      "loss": 2.771,
      "grad_norm": 2.5383334159851074,
      "learning_rate": 1.2752080614415113e-05,
      "epoch": 58.7893864013267
    },
    {
      "loss": 2.7796,
      "grad_norm": 2.5609655380249023,
      "learning_rate": 1.2716284821398547e-05,
      "epoch": 58.872305140961856
    },
    {
      "eval_loss": 2.898406744003296,
      "eval_runtime": 45.6736,
      "eval_samples_per_second": 90.402,
      "eval_steps_per_second": 5.671,
      "epoch": 58.872305140961856
    },
    {
      "loss": 2.7761,
      "grad_norm": 2.498276710510254,
      "learning_rate": 1.2680451411408123e-05,
      "epoch": 58.95522388059702
    },
    {
      "loss": 2.7476,
      "grad_norm": 2.5842247009277344,
      "learning_rate": 1.2644580880689457e-05,
      "epoch": 59.03814262023217
    },
    {
      "eval_loss": 2.909198760986328,
      "eval_runtime": 49.6595,
      "eval_samples_per_second": 83.146,
      "eval_steps_per_second": 5.216,
      "epoch": 59.03814262023217
    },
    {
      "loss": 2.7797,
      "grad_norm": 2.6011807918548584,
      "learning_rate": 1.260867372600224e-05,
      "epoch": 59.12106135986733
    },
    {
      "loss": 2.7824,
      "grad_norm": 2.5924949645996094,
      "learning_rate": 1.2572730444613363e-05,
      "epoch": 59.20398009950249
    },
    {
      "eval_loss": 2.9260153770446777,
      "eval_runtime": 47.5861,
      "eval_samples_per_second": 86.769,
      "eval_steps_per_second": 5.443,
      "epoch": 59.20398009950249
    },
    {
      "loss": 2.7609,
      "grad_norm": 2.6564574241638184,
      "learning_rate": 1.2536751534290019e-05,
      "epoch": 59.28689883913764
    },
    {
      "loss": 2.8021,
      "grad_norm": 2.6182076930999756,
      "learning_rate": 1.2500737493292818e-05,
      "epoch": 59.3698175787728
    },
    {
      "eval_loss": 2.914180278778076,
      "eval_runtime": 45.5134,
      "eval_samples_per_second": 90.721,
      "eval_steps_per_second": 5.691,
      "epoch": 59.3698175787728
    },
    {
      "loss": 2.7381,
      "grad_norm": 2.6116135120391846,
      "learning_rate": 1.2464688820368884e-05,
      "epoch": 59.45273631840796
    },
    {
      "loss": 2.77,
      "grad_norm": 2.714517593383789,
      "learning_rate": 1.2428606014744943e-05,
      "epoch": 59.53565505804312
    },
    {
      "eval_loss": 2.9286255836486816,
      "eval_runtime": 45.1649,
      "eval_samples_per_second": 91.421,
      "eval_steps_per_second": 5.735,
      "epoch": 59.53565505804312
    },
    {
      "loss": 2.7742,
      "grad_norm": 2.7161996364593506,
      "learning_rate": 1.2392489576120417e-05,
      "epoch": 59.61857379767827
    },
    {
      "loss": 2.7711,
      "grad_norm": 2.6376402378082275,
      "learning_rate": 1.2356340004660507e-05,
      "epoch": 59.701492537313435
    },
    {
      "eval_loss": 2.9006969928741455,
      "eval_runtime": 45.2674,
      "eval_samples_per_second": 91.214,
      "eval_steps_per_second": 5.722,
      "epoch": 59.701492537313435
    },
    {
      "loss": 2.7545,
      "grad_norm": 2.7101027965545654,
      "learning_rate": 1.232015780098925e-05,
      "epoch": 59.78441127694859
    },
    {
      "loss": 2.7444,
      "grad_norm": 2.578376293182373,
      "learning_rate": 1.22839434661826e-05,
      "epoch": 59.86733001658375
    },
    {
      "eval_loss": 2.9103360176086426,
      "eval_runtime": 45.295,
      "eval_samples_per_second": 91.158,
      "eval_steps_per_second": 5.718,
      "epoch": 59.86733001658375
    },
    {
      "loss": 2.7507,
      "grad_norm": 2.5386998653411865,
      "learning_rate": 1.2247697501761484e-05,
      "epoch": 59.950248756218905
    },
    {
      "loss": 2.7701,
      "grad_norm": 2.643906831741333,
      "learning_rate": 1.2211420409684867e-05,
      "epoch": 60.033167495854066
    },
    {
      "eval_loss": 2.90128231048584,
      "eval_runtime": 44.637,
      "eval_samples_per_second": 92.502,
      "eval_steps_per_second": 5.802,
      "epoch": 60.033167495854066
    },
    {
      "loss": 2.7417,
      "grad_norm": 2.5577454566955566,
      "learning_rate": 1.2175112692342773e-05,
      "epoch": 60.11608623548922
    },
    {
      "loss": 2.7274,
      "grad_norm": 2.5493829250335693,
      "learning_rate": 1.2138774852549366e-05,
      "epoch": 60.19900497512438
    },
    {
      "eval_loss": 2.90374493598938,
      "eval_runtime": 44.572,
      "eval_samples_per_second": 92.637,
      "eval_steps_per_second": 5.811,
      "epoch": 60.19900497512438
    },
    {
      "loss": 2.7439,
      "grad_norm": 2.62650990486145,
      "learning_rate": 1.210240739353595e-05,
      "epoch": 60.281923714759536
    },
    {
      "loss": 2.7317,
      "grad_norm": 2.626532554626465,
      "learning_rate": 1.2066010818944024e-05,
      "epoch": 60.36484245439469
    },
    {
      "eval_loss": 2.87467360496521,
      "eval_runtime": 48.9853,
      "eval_samples_per_second": 84.291,
      "eval_steps_per_second": 5.287,
      "epoch": 60.36484245439469
    },
    {
      "loss": 2.7415,
      "grad_norm": 2.553262233734131,
      "learning_rate": 1.2029585632818294e-05,
      "epoch": 60.44776119402985
    },
    {
      "loss": 2.7502,
      "grad_norm": 2.5994648933410645,
      "learning_rate": 1.1993132339599711e-05,
      "epoch": 60.530679933665006
    },
    {
      "eval_loss": 2.884253740310669,
      "eval_runtime": 48.9243,
      "eval_samples_per_second": 84.396,
      "eval_steps_per_second": 5.294,
      "epoch": 60.530679933665006
    },
    {
      "loss": 2.7176,
      "grad_norm": 2.503819227218628,
      "learning_rate": 1.1956651444118454e-05,
      "epoch": 60.61359867330017
    },
    {
      "loss": 2.7513,
      "grad_norm": 2.5096678733825684,
      "learning_rate": 1.1920143451586973e-05,
      "epoch": 60.69651741293532
    },
    {
      "eval_loss": 2.876120090484619,
      "eval_runtime": 49.7181,
      "eval_samples_per_second": 83.048,
      "eval_steps_per_second": 5.209,
      "epoch": 60.69651741293532
    },
    {
      "loss": 2.773,
      "grad_norm": 2.6284029483795166,
      "learning_rate": 1.188360886759297e-05,
      "epoch": 60.77943615257048
    },
    {
      "loss": 2.7524,
      "grad_norm": 2.593637704849243,
      "learning_rate": 1.1847048198092402e-05,
      "epoch": 60.86235489220564
    },
    {
      "eval_loss": 2.890573024749756,
      "eval_runtime": 49.3819,
      "eval_samples_per_second": 83.614,
      "eval_steps_per_second": 5.245,
      "epoch": 60.86235489220564
    },
    {
      "loss": 2.7353,
      "grad_norm": 2.546921491622925,
      "learning_rate": 1.1810461949402474e-05,
      "epoch": 60.9452736318408
    },
    {
      "loss": 2.7475,
      "grad_norm": 2.8388683795928955,
      "learning_rate": 1.177385062819464e-05,
      "epoch": 61.02819237147595
    },
    {
      "eval_loss": 2.8625218868255615,
      "eval_runtime": 49.7339,
      "eval_samples_per_second": 83.022,
      "eval_steps_per_second": 5.208,
      "epoch": 61.02819237147595
    },
    {
      "loss": 2.7046,
      "grad_norm": 2.6506094932556152,
      "learning_rate": 1.1737214741487563e-05,
      "epoch": 61.111111111111114
    },
    {
      "loss": 2.7286,
      "grad_norm": 2.527695894241333,
      "learning_rate": 1.1700554796640113e-05,
      "epoch": 61.19402985074627
    },
    {
      "eval_loss": 2.8707284927368164,
      "eval_runtime": 51.2141,
      "eval_samples_per_second": 80.622,
      "eval_steps_per_second": 5.057,
      "epoch": 61.19402985074627
    },
    {
      "loss": 2.7313,
      "grad_norm": 2.5835044384002686,
      "learning_rate": 1.1663871301344332e-05,
      "epoch": 61.27694859038142
    },
    {
      "loss": 2.7436,
      "grad_norm": 2.708101272583008,
      "learning_rate": 1.1627164763618406e-05,
      "epoch": 61.359867330016584
    },
    {
      "eval_loss": 2.8870952129364014,
      "eval_runtime": 49.2738,
      "eval_samples_per_second": 83.797,
      "eval_steps_per_second": 5.256,
      "epoch": 61.359867330016584
    },
    {
      "loss": 2.7261,
      "grad_norm": 2.493542194366455,
      "learning_rate": 1.1590435691799624e-05,
      "epoch": 61.44278606965174
    },
    {
      "loss": 2.7474,
      "grad_norm": 2.664074182510376,
      "learning_rate": 1.1553684594537349e-05,
      "epoch": 61.5257048092869
    },
    {
      "eval_loss": 2.8803999423980713,
      "eval_runtime": 45.8203,
      "eval_samples_per_second": 90.113,
      "eval_steps_per_second": 5.653,
      "epoch": 61.5257048092869
    },
    {
      "loss": 2.7292,
      "grad_norm": 2.5926294326782227,
      "learning_rate": 1.1516911980785958e-05,
      "epoch": 61.608623548922054
    },
    {
      "loss": 2.7261,
      "grad_norm": 2.606707811355591,
      "learning_rate": 1.1480118359797817e-05,
      "epoch": 61.691542288557216
    },
    {
      "eval_loss": 2.8675715923309326,
      "eval_runtime": 45.9128,
      "eval_samples_per_second": 89.931,
      "eval_steps_per_second": 5.641,
      "epoch": 61.691542288557216
    },
    {
      "loss": 2.7167,
      "grad_norm": 2.621661901473999,
      "learning_rate": 1.1443304241116203e-05,
      "epoch": 61.77446102819237
    },
    {
      "loss": 2.7465,
      "grad_norm": 2.660273313522339,
      "learning_rate": 1.140647013456826e-05,
      "epoch": 61.85737976782753
    },
    {
      "eval_loss": 2.87373948097229,
      "eval_runtime": 49.496,
      "eval_samples_per_second": 83.421,
      "eval_steps_per_second": 5.233,
      "epoch": 61.85737976782753
    },
    {
      "loss": 2.724,
      "grad_norm": 2.679640054702759,
      "learning_rate": 1.1369616550257943e-05,
      "epoch": 61.940298507462686
    },
    {
      "loss": 2.7267,
      "grad_norm": 2.4985883235931396,
      "learning_rate": 1.1332743998558944e-05,
      "epoch": 62.02321724709785
    },
    {
      "eval_loss": 2.8703103065490723,
      "eval_runtime": 45.7416,
      "eval_samples_per_second": 90.268,
      "eval_steps_per_second": 5.662,
      "epoch": 62.02321724709785
    },
    {
      "loss": 2.7329,
      "grad_norm": 2.765932321548462,
      "learning_rate": 1.1295852990107634e-05,
      "epoch": 62.106135986733
    },
    {
      "loss": 2.7053,
      "grad_norm": 2.6678519248962402,
      "learning_rate": 1.125894403579598e-05,
      "epoch": 62.18905472636816
    },
    {
      "eval_loss": 2.8625564575195312,
      "eval_runtime": 47.3131,
      "eval_samples_per_second": 87.27,
      "eval_steps_per_second": 5.474,
      "epoch": 62.18905472636816
    },
    {
      "loss": 2.7036,
      "grad_norm": 2.5786983966827393,
      "learning_rate": 1.1222017646764475e-05,
      "epoch": 62.27197346600332
    },
    {
      "loss": 2.7241,
      "grad_norm": 2.5020642280578613,
      "learning_rate": 1.1185074334395065e-05,
      "epoch": 62.35489220563847
    },
    {
      "eval_loss": 2.8645052909851074,
      "eval_runtime": 47.1442,
      "eval_samples_per_second": 87.582,
      "eval_steps_per_second": 5.494,
      "epoch": 62.35489220563847
    },
    {
      "loss": 2.7029,
      "grad_norm": 2.698390007019043,
      "learning_rate": 1.1148114610304061e-05,
      "epoch": 62.43781094527363
    },
    {
      "loss": 2.7255,
      "grad_norm": 2.621612548828125,
      "learning_rate": 1.1111138986335052e-05,
      "epoch": 62.52072968490879
    },
    {
      "eval_loss": 2.877948760986328,
      "eval_runtime": 47.0093,
      "eval_samples_per_second": 87.834,
      "eval_steps_per_second": 5.51,
      "epoch": 62.52072968490879
    },
    {
      "loss": 2.7178,
      "grad_norm": 2.5771093368530273,
      "learning_rate": 1.107414797455182e-05,
      "epoch": 62.60364842454395
    },
    {
      "loss": 2.7292,
      "grad_norm": 2.4121978282928467,
      "learning_rate": 1.1037142087231247e-05,
      "epoch": 62.6865671641791
    },
    {
      "eval_loss": 2.8776817321777344,
      "eval_runtime": 46.9644,
      "eval_samples_per_second": 87.918,
      "eval_steps_per_second": 5.515,
      "epoch": 62.6865671641791
    },
    {
      "loss": 2.696,
      "grad_norm": 2.5290143489837646,
      "learning_rate": 1.1000121836856228e-05,
      "epoch": 62.769485903814264
    },
    {
      "loss": 2.6884,
      "grad_norm": 2.541264533996582,
      "learning_rate": 1.096308773610856e-05,
      "epoch": 62.85240464344942
    },
    {
      "eval_loss": 2.854315757751465,
      "eval_runtime": 46.844,
      "eval_samples_per_second": 88.144,
      "eval_steps_per_second": 5.529,
      "epoch": 62.85240464344942
    },
    {
      "loss": 2.6926,
      "grad_norm": 2.606375217437744,
      "learning_rate": 1.0926040297861851e-05,
      "epoch": 62.93532338308458
    },
    {
      "loss": 2.7144,
      "grad_norm": 2.534533977508545,
      "learning_rate": 1.0888980035174414e-05,
      "epoch": 63.018242122719734
    },
    {
      "eval_loss": 2.858328104019165,
      "eval_runtime": 46.9413,
      "eval_samples_per_second": 87.961,
      "eval_steps_per_second": 5.518,
      "epoch": 63.018242122719734
    },
    {
      "loss": 2.684,
      "grad_norm": 2.654209613800049,
      "learning_rate": 1.0851907461282174e-05,
      "epoch": 63.101160862354895
    },
    {
      "loss": 2.6995,
      "grad_norm": 2.6150143146514893,
      "learning_rate": 1.0814823089591542e-05,
      "epoch": 63.18407960199005
    },
    {
      "eval_loss": 2.857604742050171,
      "eval_runtime": 46.9001,
      "eval_samples_per_second": 88.038,
      "eval_steps_per_second": 5.522,
      "epoch": 63.18407960199005
    },
    {
      "loss": 2.7057,
      "grad_norm": 2.709798574447632,
      "learning_rate": 1.0777727433672311e-05,
      "epoch": 63.26699834162521
    },
    {
      "loss": 2.7207,
      "grad_norm": 2.6051745414733887,
      "learning_rate": 1.0740621007250545e-05,
      "epoch": 63.349917081260365
    },
    {
      "eval_loss": 2.863326072692871,
      "eval_runtime": 46.5677,
      "eval_samples_per_second": 88.667,
      "eval_steps_per_second": 5.562,
      "epoch": 63.349917081260365
    },
    {
      "loss": 2.7129,
      "grad_norm": 2.5947763919830322,
      "learning_rate": 1.0703504324201472e-05,
      "epoch": 63.43283582089552
    },
    {
      "loss": 2.6971,
      "grad_norm": 2.603424072265625,
      "learning_rate": 1.066637789854236e-05,
      "epoch": 63.51575456053068
    },
    {
      "eval_loss": 2.8597261905670166,
      "eval_runtime": 47.3473,
      "eval_samples_per_second": 87.207,
      "eval_steps_per_second": 5.47,
      "epoch": 63.51575456053068
    },
    {
      "loss": 2.69,
      "grad_norm": 2.651726484298706,
      "learning_rate": 1.0629242244425394e-05,
      "epoch": 63.598673300165835
    },
    {
      "loss": 2.6991,
      "grad_norm": 2.6992974281311035,
      "learning_rate": 1.059209787613056e-05,
      "epoch": 63.681592039801
    },
    {
      "eval_loss": 2.863450527191162,
      "eval_runtime": 48.7489,
      "eval_samples_per_second": 84.699,
      "eval_steps_per_second": 5.313,
      "epoch": 63.681592039801
    },
    {
      "loss": 2.6995,
      "grad_norm": 2.594829559326172,
      "learning_rate": 1.0554945308058537e-05,
      "epoch": 63.76451077943615
    },
    {
      "loss": 2.6711,
      "grad_norm": 2.554511785507202,
      "learning_rate": 1.0517785054723546e-05,
      "epoch": 63.84742951907131
    },
    {
      "eval_loss": 2.8526246547698975,
      "eval_runtime": 49.5981,
      "eval_samples_per_second": 83.249,
      "eval_steps_per_second": 5.222,
      "epoch": 63.84742951907131
    },
    {
      "loss": 2.6921,
      "grad_norm": 2.580731153488159,
      "learning_rate": 1.0480617630746241e-05,
      "epoch": 63.930348258706466
    },
    {
      "loss": 2.6912,
      "grad_norm": 2.734182834625244,
      "learning_rate": 1.0443443550846586e-05,
      "epoch": 64.01326699834162
    },
    {
      "eval_loss": 2.847461462020874,
      "eval_runtime": 46.1877,
      "eval_samples_per_second": 89.396,
      "eval_steps_per_second": 5.608,
      "epoch": 64.01326699834162
    },
    {
      "loss": 2.6823,
      "grad_norm": 2.7193052768707275,
      "learning_rate": 1.0406263329836714e-05,
      "epoch": 64.09618573797678
    },
    {
      "loss": 2.6695,
      "grad_norm": 2.7004690170288086,
      "learning_rate": 1.0369077482613812e-05,
      "epoch": 64.17910447761194
    },
    {
      "eval_loss": 2.84598708152771,
      "eval_runtime": 47.0468,
      "eval_samples_per_second": 87.764,
      "eval_steps_per_second": 5.505,
      "epoch": 64.17910447761194
    },
    {
      "loss": 2.678,
      "grad_norm": 2.5916712284088135,
      "learning_rate": 1.0331886524152978e-05,
      "epoch": 64.2620232172471
    },
    {
      "loss": 2.6977,
      "grad_norm": 2.6555120944976807,
      "learning_rate": 1.0294690969500092e-05,
      "epoch": 64.34494195688225
    },
    {
      "eval_loss": 2.851780652999878,
      "eval_runtime": 49.1041,
      "eval_samples_per_second": 84.087,
      "eval_steps_per_second": 5.275,
      "epoch": 64.34494195688225
    },
    {
      "loss": 2.6809,
      "grad_norm": 2.6786949634552,
      "learning_rate": 1.0257491333764688e-05,
      "epoch": 64.42786069651741
    },
    {
      "loss": 2.7065,
      "grad_norm": 2.591284990310669,
      "learning_rate": 1.022028813211282e-05,
      "epoch": 64.51077943615257
    },
    {
      "eval_loss": 2.857077121734619,
      "eval_runtime": 47.1842,
      "eval_samples_per_second": 87.508,
      "eval_steps_per_second": 5.489,
      "epoch": 64.51077943615257
    },
    {
      "loss": 2.6738,
      "grad_norm": 2.558490753173828,
      "learning_rate": 1.0183081879759916e-05,
      "epoch": 64.59369817578772
    },
    {
      "loss": 2.6674,
      "grad_norm": 2.6576380729675293,
      "learning_rate": 1.0145873091963665e-05,
      "epoch": 64.67661691542288
    },
    {
      "eval_loss": 2.8360917568206787,
      "eval_runtime": 46.9381,
      "eval_samples_per_second": 87.967,
      "eval_steps_per_second": 5.518,
      "epoch": 64.67661691542288
    },
    {
      "loss": 2.6734,
      "grad_norm": 2.8266961574554443,
      "learning_rate": 1.0108662284016857e-05,
      "epoch": 64.75953565505804
    },
    {
      "loss": 2.7016,
      "grad_norm": 2.656938314437866,
      "learning_rate": 1.0071449971240268e-05,
      "epoch": 64.8424543946932
    },
    {
      "eval_loss": 2.8438503742218018,
      "eval_runtime": 46.0655,
      "eval_samples_per_second": 89.633,
      "eval_steps_per_second": 5.622,
      "epoch": 64.8424543946932
    },
    {
      "loss": 2.6665,
      "grad_norm": 2.581664562225342,
      "learning_rate": 1.0034236668975508e-05,
      "epoch": 64.92537313432835
    },
    {
      "loss": 2.6844,
      "grad_norm": 2.715837240219116,
      "learning_rate": 9.99702289257789e-06,
      "epoch": 65.00829187396351
    },
    {
      "eval_loss": 2.8502748012542725,
      "eval_runtime": 46.5296,
      "eval_samples_per_second": 88.739,
      "eval_steps_per_second": 5.566,
      "epoch": 65.00829187396351
    },
    {
      "loss": 2.6938,
      "grad_norm": 2.6830215454101562,
      "learning_rate": 9.9598091574093e-06,
      "epoch": 65.09121061359868
    },
    {
      "loss": 2.6807,
      "grad_norm": 2.684473991394043,
      "learning_rate": 9.922595978831039e-06,
      "epoch": 65.17412935323384
    },
    {
      "eval_loss": 2.844392776489258,
      "eval_runtime": 49.1545,
      "eval_samples_per_second": 84.0,
      "eval_steps_per_second": 5.269,
      "epoch": 65.17412935323384
    },
    {
      "loss": 2.6849,
      "grad_norm": 2.7103796005249023,
      "learning_rate": 9.885383872196713e-06,
      "epoch": 65.25704809286898
    },
    {
      "loss": 2.6688,
      "grad_norm": 2.5902340412139893,
      "learning_rate": 9.848173352845082e-06,
      "epoch": 65.33996683250415
    },
    {
      "eval_loss": 2.852677583694458,
      "eval_runtime": 49.2205,
      "eval_samples_per_second": 83.888,
      "eval_steps_per_second": 5.262,
      "epoch": 65.33996683250415
    },
    {
      "loss": 2.645,
      "grad_norm": 2.666278123855591,
      "learning_rate": 9.810964936092917e-06,
      "epoch": 65.42288557213931
    },
    {
      "loss": 2.6993,
      "grad_norm": 2.5789380073547363,
      "learning_rate": 9.773759137227878e-06,
      "epoch": 65.50580431177445
    },
    {
      "eval_loss": 2.8355860710144043,
      "eval_runtime": 49.3992,
      "eval_samples_per_second": 83.584,
      "eval_steps_per_second": 5.243,
      "epoch": 65.50580431177445
    },
    {
      "loss": 2.6621,
      "grad_norm": 2.713064193725586,
      "learning_rate": 9.736556471501363e-06,
      "epoch": 65.58872305140962
    },
    {
      "loss": 2.6845,
      "grad_norm": 2.6036365032196045,
      "learning_rate": 9.699357454121385e-06,
      "epoch": 65.67164179104478
    },
    {
      "eval_loss": 2.8268582820892334,
      "eval_runtime": 48.9505,
      "eval_samples_per_second": 84.35,
      "eval_steps_per_second": 5.291,
      "epoch": 65.67164179104478
    },
    {
      "loss": 2.6449,
      "grad_norm": 2.545564651489258,
      "learning_rate": 9.662162600245437e-06,
      "epoch": 65.75456053067994
    },
    {
      "loss": 2.6712,
      "grad_norm": 2.6406123638153076,
      "learning_rate": 9.62497242497334e-06,
      "epoch": 65.83747927031509
    },
    {
      "eval_loss": 2.837480068206787,
      "eval_runtime": 49.2201,
      "eval_samples_per_second": 83.888,
      "eval_steps_per_second": 5.262,
      "epoch": 65.83747927031509
    },
    {
      "loss": 2.6589,
      "grad_norm": 2.526522636413574,
      "learning_rate": 9.587787443340134e-06,
      "epoch": 65.92039800995025
    },
    {
      "loss": 2.6499,
      "grad_norm": 2.535550594329834,
      "learning_rate": 9.550608170308935e-06,
      "epoch": 66.00331674958541
    },
    {
      "eval_loss": 2.835357427597046,
      "eval_runtime": 49.0587,
      "eval_samples_per_second": 84.164,
      "eval_steps_per_second": 5.279,
      "epoch": 66.00331674958541
    },
    {
      "loss": 2.6486,
      "grad_norm": 2.499955892562866,
      "learning_rate": 9.513435120763791e-06,
      "epoch": 66.08623548922057
    },
    {
      "loss": 2.6436,
      "grad_norm": 2.6330344676971436,
      "learning_rate": 9.47626880950257e-06,
      "epoch": 66.16915422885572
    },
    {
      "eval_loss": 2.836359977722168,
      "eval_runtime": 48.9618,
      "eval_samples_per_second": 84.331,
      "eval_steps_per_second": 5.29,
      "epoch": 66.16915422885572
    },
    {
      "loss": 2.6483,
      "grad_norm": 2.5194027423858643,
      "learning_rate": 9.439109751229825e-06,
      "epoch": 66.25207296849088
    },
    {
      "loss": 2.664,
      "grad_norm": 2.678152322769165,
      "learning_rate": 9.401958460549658e-06,
      "epoch": 66.33499170812604
    },
    {
      "eval_loss": 2.832552909851074,
      "eval_runtime": 49.1562,
      "eval_samples_per_second": 83.998,
      "eval_steps_per_second": 5.269,
      "epoch": 66.33499170812604
    },
    {
      "loss": 2.6328,
      "grad_norm": 2.7169365882873535,
      "learning_rate": 9.364815451958615e-06,
      "epoch": 66.41791044776119
    },
    {
      "loss": 2.6441,
      "grad_norm": 2.627577304840088,
      "learning_rate": 9.327681239838523e-06,
      "epoch": 66.50082918739635
    },
    {
      "eval_loss": 2.834019422531128,
      "eval_runtime": 49.6947,
      "eval_samples_per_second": 83.087,
      "eval_steps_per_second": 5.212,
      "epoch": 66.50082918739635
    },
    {
      "loss": 2.6513,
      "grad_norm": 2.6837351322174072,
      "learning_rate": 9.29055633844941e-06,
      "epoch": 66.58374792703151
    },
    {
      "loss": 2.6324,
      "grad_norm": 2.6815102100372314,
      "learning_rate": 9.253441261922353e-06,
      "epoch": 66.66666666666667
    },
    {
      "eval_loss": 2.8283779621124268,
      "eval_runtime": 49.3062,
      "eval_samples_per_second": 83.742,
      "eval_steps_per_second": 5.253,
      "epoch": 66.66666666666667
    },
    {
      "loss": 2.6648,
      "grad_norm": 2.650498628616333,
      "learning_rate": 9.216336524252368e-06,
      "epoch": 66.74958540630182
    },
    {
      "loss": 2.6361,
      "grad_norm": 2.6691997051239014,
      "learning_rate": 9.179242639291297e-06,
      "epoch": 66.83250414593698
    },
    {
      "eval_loss": 2.814612627029419,
      "eval_runtime": 48.9841,
      "eval_samples_per_second": 84.293,
      "eval_steps_per_second": 5.287,
      "epoch": 66.83250414593698
    },
    {
      "loss": 2.6715,
      "grad_norm": 2.6693203449249268,
      "learning_rate": 9.142160120740673e-06,
      "epoch": 66.91542288557214
    },
    {
      "loss": 2.6708,
      "grad_norm": 2.6856088638305664,
      "learning_rate": 9.105089482144635e-06,
      "epoch": 66.9983416252073
    },
    {
      "eval_loss": 2.8218207359313965,
      "eval_runtime": 49.3158,
      "eval_samples_per_second": 83.726,
      "eval_steps_per_second": 5.252,
      "epoch": 66.9983416252073
    },
    {
      "loss": 2.6401,
      "grad_norm": 2.629140615463257,
      "learning_rate": 9.068031236882798e-06,
      "epoch": 67.08126036484245
    },
    {
      "loss": 2.6452,
      "grad_norm": 2.6151883602142334,
      "learning_rate": 9.030985898163128e-06,
      "epoch": 67.16417910447761
    },
    {
      "eval_loss": 2.8187050819396973,
      "eval_runtime": 49.3121,
      "eval_samples_per_second": 83.732,
      "eval_steps_per_second": 5.252,
      "epoch": 67.16417910447761
    },
    {
      "loss": 2.6622,
      "grad_norm": 2.608490467071533,
      "learning_rate": 8.993953979014877e-06,
      "epoch": 67.24709784411277
    },
    {
      "loss": 2.6268,
      "grad_norm": 2.609046220779419,
      "learning_rate": 8.956935992281435e-06,
      "epoch": 67.33001658374793
    },
    {
      "eval_loss": 2.822303533554077,
      "eval_runtime": 49.2373,
      "eval_samples_per_second": 83.859,
      "eval_steps_per_second": 5.26,
      "epoch": 67.33001658374793
    },
    {
      "loss": 2.637,
      "grad_norm": 2.727060079574585,
      "learning_rate": 8.919932450613254e-06,
      "epoch": 67.41293532338308
    },
    {
      "loss": 2.6417,
      "grad_norm": 2.6682188510894775,
      "learning_rate": 8.882943866460746e-06,
      "epoch": 67.49585406301824
    },
    {
      "eval_loss": 2.8169522285461426,
      "eval_runtime": 49.1168,
      "eval_samples_per_second": 84.065,
      "eval_steps_per_second": 5.273,
      "epoch": 67.49585406301824
    },
    {
      "loss": 2.6363,
      "grad_norm": 2.5543792247772217,
      "learning_rate": 8.845970752067165e-06,
      "epoch": 67.5787728026534
    },
    {
      "loss": 2.6538,
      "grad_norm": 2.6879994869232178,
      "learning_rate": 8.809013619461546e-06,
      "epoch": 67.66169154228855
    },
    {
      "eval_loss": 2.8173396587371826,
      "eval_runtime": 49.3768,
      "eval_samples_per_second": 83.622,
      "eval_steps_per_second": 5.245,
      "epoch": 67.66169154228855
    },
    {
      "loss": 2.6573,
      "grad_norm": 2.6476290225982666,
      "learning_rate": 8.772072980451594e-06,
      "epoch": 67.74461028192371
    },
    {
      "loss": 2.6669,
      "grad_norm": 2.814992666244507,
      "learning_rate": 8.73514934661659e-06,
      "epoch": 67.82752902155887
    },
    {
      "eval_loss": 2.798412322998047,
      "eval_runtime": 48.5875,
      "eval_samples_per_second": 84.981,
      "eval_steps_per_second": 5.331,
      "epoch": 67.82752902155887
    },
    {
      "loss": 2.621,
      "grad_norm": 2.618320941925049,
      "learning_rate": 8.69824322930032e-06,
      "epoch": 67.91044776119404
    },
    {
      "loss": 2.6234,
      "grad_norm": 2.668865203857422,
      "learning_rate": 8.661355139603998e-06,
      "epoch": 67.99336650082918
    },
    {
      "eval_loss": 2.805098295211792,
      "eval_runtime": 49.196,
      "eval_samples_per_second": 83.93,
      "eval_steps_per_second": 5.265,
      "epoch": 67.99336650082918
    },
    {
      "loss": 2.6609,
      "grad_norm": 2.6378605365753174,
      "learning_rate": 8.624485588379164e-06,
      "epoch": 68.07628524046434
    },
    {
      "loss": 2.6154,
      "grad_norm": 2.702723979949951,
      "learning_rate": 8.587635086220642e-06,
      "epoch": 68.1592039800995
    },
    {
      "eval_loss": 2.8122010231018066,
      "eval_runtime": 49.472,
      "eval_samples_per_second": 83.461,
      "eval_steps_per_second": 5.235,
      "epoch": 68.1592039800995
    },
    {
      "loss": 2.6162,
      "grad_norm": 2.6275179386138916,
      "learning_rate": 8.550804143459431e-06,
      "epoch": 68.24212271973467
    },
    {
      "loss": 2.6405,
      "grad_norm": 2.7347664833068848,
      "learning_rate": 8.513993270155681e-06,
      "epoch": 68.32504145936981
    },
    {
      "eval_loss": 2.8020834922790527,
      "eval_runtime": 48.6443,
      "eval_samples_per_second": 84.881,
      "eval_steps_per_second": 5.324,
      "epoch": 68.32504145936981
    },
    {
      "loss": 2.6547,
      "grad_norm": 2.668870449066162,
      "learning_rate": 8.477202976091583e-06,
      "epoch": 68.40796019900498
    },
    {
      "loss": 2.6571,
      "grad_norm": 2.6861395835876465,
      "learning_rate": 8.440433770764352e-06,
      "epoch": 68.49087893864014
    },
    {
      "eval_loss": 2.80495023727417,
      "eval_runtime": 48.5033,
      "eval_samples_per_second": 85.128,
      "eval_steps_per_second": 5.34,
      "epoch": 68.49087893864014
    },
    {
      "loss": 2.6388,
      "grad_norm": 2.6949496269226074,
      "learning_rate": 8.403686163379144e-06,
      "epoch": 68.57379767827528
    },
    {
      "loss": 2.6236,
      "grad_norm": 2.6134684085845947,
      "learning_rate": 8.366960662842004e-06,
      "epoch": 68.65671641791045
    },
    {
      "eval_loss": 2.8130273818969727,
      "eval_runtime": 49.13,
      "eval_samples_per_second": 84.042,
      "eval_steps_per_second": 5.272,
      "epoch": 68.65671641791045
    },
    {
      "loss": 2.6115,
      "grad_norm": 2.7658510208129883,
      "learning_rate": 8.330257777752841e-06,
      "epoch": 68.7396351575456
    },
    {
      "loss": 2.6387,
      "grad_norm": 2.6879663467407227,
      "learning_rate": 8.293578016398364e-06,
      "epoch": 68.82255389718077
    },
    {
      "eval_loss": 2.8064723014831543,
      "eval_runtime": 49.4819,
      "eval_samples_per_second": 83.445,
      "eval_steps_per_second": 5.234,
      "epoch": 68.82255389718077
    },
    {
      "loss": 2.6409,
      "grad_norm": 2.7814276218414307,
      "learning_rate": 8.256921886745041e-06,
      "epoch": 68.90547263681592
    },
    {
      "loss": 2.6406,
      "grad_norm": 2.724334239959717,
      "learning_rate": 8.220289896432085e-06,
      "epoch": 68.98839137645108
    },
    {
      "eval_loss": 2.7963640689849854,
      "eval_runtime": 49.1048,
      "eval_samples_per_second": 84.085,
      "eval_steps_per_second": 5.274,
      "epoch": 68.98839137645108
    },
    {
      "loss": 2.6251,
      "grad_norm": 2.7448232173919678,
      "learning_rate": 8.183682552764401e-06,
      "epoch": 69.07131011608624
    },
    {
      "loss": 2.6327,
      "grad_norm": 2.637857437133789,
      "learning_rate": 8.14710036270557e-06,
      "epoch": 69.1542288557214
    },
    {
      "eval_loss": 2.807535171508789,
      "eval_runtime": 46.3254,
      "eval_samples_per_second": 89.13,
      "eval_steps_per_second": 5.591,
      "epoch": 69.1542288557214
    },
    {
      "loss": 2.6242,
      "grad_norm": 2.7387027740478516,
      "learning_rate": 8.110543832870838e-06,
      "epoch": 69.23714759535655
    },
    {
      "loss": 2.6243,
      "grad_norm": 2.611849784851074,
      "learning_rate": 8.07401346952008e-06,
      "epoch": 69.32006633499171
    },
    {
      "eval_loss": 2.79221510887146,
      "eval_runtime": 46.2699,
      "eval_samples_per_second": 89.237,
      "eval_steps_per_second": 5.598,
      "epoch": 69.32006633499171
    },
    {
      "loss": 2.6257,
      "grad_norm": 2.70796275138855,
      "learning_rate": 8.037509778550808e-06,
      "epoch": 69.40298507462687
    },
    {
      "loss": 2.6287,
      "grad_norm": 2.597116470336914,
      "learning_rate": 8.001033265491156e-06,
      "epoch": 69.48590381426202
    },
    {
      "eval_loss": 2.795353651046753,
      "eval_runtime": 46.1589,
      "eval_samples_per_second": 89.452,
      "eval_steps_per_second": 5.611,
      "epoch": 69.48590381426202
    },
    {
      "loss": 2.6333,
      "grad_norm": 2.7035934925079346,
      "learning_rate": 7.96458443549287e-06,
      "epoch": 69.56882255389718
    },
    {
      "loss": 2.6472,
      "grad_norm": 2.6586973667144775,
      "learning_rate": 7.928163793324331e-06,
      "epoch": 69.65174129353234
    },
    {
      "eval_loss": 2.792280435562134,
      "eval_runtime": 46.3347,
      "eval_samples_per_second": 89.113,
      "eval_steps_per_second": 5.59,
      "epoch": 69.65174129353234
    },
    {
      "loss": 2.6138,
      "grad_norm": 2.738905429840088,
      "learning_rate": 7.891771843363557e-06,
      "epoch": 69.7346600331675
    },
    {
      "loss": 2.6346,
      "grad_norm": 2.648172616958618,
      "learning_rate": 7.855409089591206e-06,
      "epoch": 69.81757877280265
    },
    {
      "eval_loss": 2.7935268878936768,
      "eval_runtime": 46.2703,
      "eval_samples_per_second": 89.237,
      "eval_steps_per_second": 5.598,
      "epoch": 69.81757877280265
    },
    {
      "loss": 2.587,
      "grad_norm": 2.773963689804077,
      "learning_rate": 7.81907603558362e-06,
      "epoch": 69.90049751243781
    },
    {
      "loss": 2.6129,
      "grad_norm": 2.7465689182281494,
      "learning_rate": 7.78277318450583e-06,
      "epoch": 69.98341625207297
    },
    {
      "eval_loss": 2.7966244220733643,
      "eval_runtime": 46.1779,
      "eval_samples_per_second": 89.415,
      "eval_steps_per_second": 5.609,
      "epoch": 69.98341625207297
    },
    {
      "loss": 2.6231,
      "grad_norm": 2.6270785331726074,
      "learning_rate": 7.746501039104596e-06,
      "epoch": 70.06633499170813
    },
    {
      "loss": 2.5902,
      "grad_norm": 2.6625618934631348,
      "learning_rate": 7.710260101701457e-06,
      "epoch": 70.14925373134328
    },
    {
      "eval_loss": 2.805509567260742,
      "eval_runtime": 46.2794,
      "eval_samples_per_second": 89.219,
      "eval_steps_per_second": 5.596,
      "epoch": 70.14925373134328
    },
    {
      "loss": 2.6142,
      "grad_norm": 2.6762704849243164,
      "learning_rate": 7.674050874185742e-06,
      "epoch": 70.23217247097844
    },
    {
      "loss": 2.6554,
      "grad_norm": 2.689724922180176,
      "learning_rate": 7.637873858007658e-06,
      "epoch": 70.3150912106136
    },
    {
      "eval_loss": 2.7834789752960205,
      "eval_runtime": 46.2618,
      "eval_samples_per_second": 89.253,
      "eval_steps_per_second": 5.599,
      "epoch": 70.3150912106136
    },
    {
      "loss": 2.6111,
      "grad_norm": 2.6568408012390137,
      "learning_rate": 7.6017295541713106e-06,
      "epoch": 70.39800995024876
    },
    {
      "loss": 2.6021,
      "grad_norm": 2.5900585651397705,
      "learning_rate": 7.565618463227797e-06,
      "epoch": 70.48092868988391
    },
    {
      "eval_loss": 2.7995612621307373,
      "eval_runtime": 46.233,
      "eval_samples_per_second": 89.309,
      "eval_steps_per_second": 5.602,
      "epoch": 70.48092868988391
    },
    {
      "loss": 2.6067,
      "grad_norm": 2.703669786453247,
      "learning_rate": 7.529541085268257e-06,
      "epoch": 70.56384742951907
    },
    {
      "loss": 2.6034,
      "grad_norm": 2.8062336444854736,
      "learning_rate": 7.493497919916941e-06,
      "epoch": 70.64676616915423
    },
    {
      "eval_loss": 2.80234432220459,
      "eval_runtime": 46.2178,
      "eval_samples_per_second": 89.338,
      "eval_steps_per_second": 5.604,
      "epoch": 70.64676616915423
    },
    {
      "loss": 2.6451,
      "grad_norm": 2.716012954711914,
      "learning_rate": 7.457489466324313e-06,
      "epoch": 70.72968490878938
    },
    {
      "loss": 2.6164,
      "grad_norm": 2.6901638507843018,
      "learning_rate": 7.421516223160108e-06,
      "epoch": 70.81260364842454
    },
    {
      "eval_loss": 2.7835021018981934,
      "eval_runtime": 46.2397,
      "eval_samples_per_second": 89.296,
      "eval_steps_per_second": 5.601,
      "epoch": 70.81260364842454
    },
    {
      "loss": 2.6074,
      "grad_norm": 2.697892427444458,
      "learning_rate": 7.385578688606456e-06,
      "epoch": 70.8955223880597
    },
    {
      "loss": 2.609,
      "grad_norm": 2.6864304542541504,
      "learning_rate": 7.349677360350972e-06,
      "epoch": 70.97844112769486
    },
    {
      "eval_loss": 2.791728973388672,
      "eval_runtime": 46.215,
      "eval_samples_per_second": 89.343,
      "eval_steps_per_second": 5.604,
      "epoch": 70.97844112769486
    },
    {
      "loss": 2.6114,
      "grad_norm": 2.721158504486084,
      "learning_rate": 7.313812735579846e-06,
      "epoch": 71.06135986733001
    },
    {
      "loss": 2.5955,
      "grad_norm": 2.7050278186798096,
      "learning_rate": 7.277985310970985e-06,
      "epoch": 71.14427860696517
    },
    {
      "eval_loss": 2.7878260612487793,
      "eval_runtime": 47.0997,
      "eval_samples_per_second": 87.665,
      "eval_steps_per_second": 5.499,
      "epoch": 71.14427860696517
    },
    {
      "loss": 2.6085,
      "grad_norm": 2.643568992614746,
      "learning_rate": 7.2421955826871285e-06,
      "epoch": 71.22719734660033
    },
    {
      "loss": 2.5863,
      "grad_norm": 2.6618735790252686,
      "learning_rate": 7.20644404636895e-06,
      "epoch": 71.3101160862355
    },
    {
      "eval_loss": 2.792365550994873,
      "eval_runtime": 46.2717,
      "eval_samples_per_second": 89.234,
      "eval_steps_per_second": 5.597,
      "epoch": 71.3101160862355
    },
    {
      "loss": 2.5996,
      "grad_norm": 2.6532039642333984,
      "learning_rate": 7.170731197128238e-06,
      "epoch": 71.39303482587064
    },
    {
      "loss": 2.5989,
      "grad_norm": 2.547654867172241,
      "learning_rate": 7.135057529540998e-06,
      "epoch": 71.4759535655058
    },
    {
      "eval_loss": 2.7806942462921143,
      "eval_runtime": 46.2736,
      "eval_samples_per_second": 89.23,
      "eval_steps_per_second": 5.597,
      "epoch": 71.4759535655058
    },
    {
      "loss": 2.5898,
      "grad_norm": 2.71529221534729,
      "learning_rate": 7.099423537640631e-06,
      "epoch": 71.55887230514097
    },
    {
      "loss": 2.6288,
      "grad_norm": 2.7938344478607178,
      "learning_rate": 7.063829714911081e-06,
      "epoch": 71.64179104477611
    },
    {
      "eval_loss": 2.777536630630493,
      "eval_runtime": 49.205,
      "eval_samples_per_second": 83.914,
      "eval_steps_per_second": 5.264,
      "epoch": 71.64179104477611
    },
    {
      "loss": 2.5986,
      "grad_norm": 2.7372617721557617,
      "learning_rate": 7.028276554279996e-06,
      "epoch": 71.72470978441127
    },
    {
      "loss": 2.6196,
      "grad_norm": 2.622724771499634,
      "learning_rate": 6.9927645481119125e-06,
      "epoch": 71.80762852404644
    },
    {
      "eval_loss": 2.7781546115875244,
      "eval_runtime": 47.4873,
      "eval_samples_per_second": 86.95,
      "eval_steps_per_second": 5.454,
      "epoch": 71.80762852404644
    },
    {
      "loss": 2.6141,
      "grad_norm": 2.765181064605713,
      "learning_rate": 6.957294188201438e-06,
      "epoch": 71.8905472636816
    },
    {
      "loss": 2.6112,
      "grad_norm": 2.564234972000122,
      "learning_rate": 6.9218659657664164e-06,
      "epoch": 71.97346600331674
    },
    {
      "eval_loss": 2.7832276821136475,
      "eval_runtime": 49.2185,
      "eval_samples_per_second": 83.891,
      "eval_steps_per_second": 5.262,
      "epoch": 71.97346600331674
    },
    {
      "loss": 2.5973,
      "grad_norm": 2.7912979125976562,
      "learning_rate": 6.886480371441162e-06,
      "epoch": 72.0563847429519
    },
    {
      "loss": 2.5904,
      "grad_norm": 2.7319869995117188,
      "learning_rate": 6.851137895269629e-06,
      "epoch": 72.13930348258707
    },
    {
      "eval_loss": 2.775618314743042,
      "eval_runtime": 48.7108,
      "eval_samples_per_second": 84.766,
      "eval_steps_per_second": 5.317,
      "epoch": 72.13930348258707
    },
    {
      "loss": 2.5859,
      "grad_norm": 2.670576333999634,
      "learning_rate": 6.815839026698653e-06,
      "epoch": 72.22222222222223
    },
    {
      "loss": 2.5901,
      "grad_norm": 2.7021846771240234,
      "learning_rate": 6.780584254571164e-06,
      "epoch": 72.30514096185738
    },
    {
      "eval_loss": 2.791391611099243,
      "eval_runtime": 47.8968,
      "eval_samples_per_second": 86.206,
      "eval_steps_per_second": 5.407,
      "epoch": 72.30514096185738
    },
    {
      "loss": 2.6015,
      "grad_norm": 2.6387991905212402,
      "learning_rate": 6.745374067119401e-06,
      "epoch": 72.38805970149254
    },
    {
      "loss": 2.6026,
      "grad_norm": 2.8301031589508057,
      "learning_rate": 6.71020895195818e-06,
      "epoch": 72.4709784411277
    },
    {
      "eval_loss": 2.770627975463867,
      "eval_runtime": 48.6661,
      "eval_samples_per_second": 84.843,
      "eval_steps_per_second": 5.322,
      "epoch": 72.4709784411277
    },
    {
      "loss": 2.6027,
      "grad_norm": 2.664266586303711,
      "learning_rate": 6.675089396078108e-06,
      "epoch": 72.55389718076285
    },
    {
      "loss": 2.6174,
      "grad_norm": 2.761650562286377,
      "learning_rate": 6.640015885838872e-06,
      "epoch": 72.636815920398
    },
    {
      "eval_loss": 2.7644472122192383,
      "eval_runtime": 45.9746,
      "eval_samples_per_second": 89.81,
      "eval_steps_per_second": 5.634,
      "epoch": 72.636815920398
    },
    {
      "loss": 2.5799,
      "grad_norm": 2.677297830581665,
      "learning_rate": 6.604988906962478e-06,
      "epoch": 72.71973466003317
    },
    {
      "loss": 2.5796,
      "grad_norm": 2.717806100845337,
      "learning_rate": 6.570008944526531e-06,
      "epoch": 72.80265339966833
    },
    {
      "eval_loss": 2.798290252685547,
      "eval_runtime": 48.3617,
      "eval_samples_per_second": 85.377,
      "eval_steps_per_second": 5.355,
      "epoch": 72.80265339966833
    },
    {
      "loss": 2.6234,
      "grad_norm": 2.7263338565826416,
      "learning_rate": 6.53507648295753e-06,
      "epoch": 72.88557213930348
    },
    {
      "loss": 2.6032,
      "grad_norm": 2.623961925506592,
      "learning_rate": 6.500192006024146e-06,
      "epoch": 72.96849087893864
    },
    {
      "eval_loss": 2.769012451171875,
      "eval_runtime": 48.0235,
      "eval_samples_per_second": 85.979,
      "eval_steps_per_second": 5.393,
      "epoch": 72.96849087893864
    },
    {
      "loss": 2.5752,
      "grad_norm": 2.665296792984009,
      "learning_rate": 6.46535599683052e-06,
      "epoch": 73.0514096185738
    },
    {
      "loss": 2.597,
      "grad_norm": 2.771176815032959,
      "learning_rate": 6.430568937809587e-06,
      "epoch": 73.13432835820896
    },
    {
      "eval_loss": 2.7822704315185547,
      "eval_runtime": 47.5356,
      "eval_samples_per_second": 86.861,
      "eval_steps_per_second": 5.449,
      "epoch": 73.13432835820896
    },
    {
      "loss": 2.5878,
      "grad_norm": 2.68989896774292,
      "learning_rate": 6.395831310716378e-06,
      "epoch": 73.21724709784411
    },
    {
      "loss": 2.604,
      "grad_norm": 2.7165040969848633,
      "learning_rate": 6.361143596621362e-06,
      "epoch": 73.30016583747927
    },
    {
      "eval_loss": 2.7691400051116943,
      "eval_runtime": 49.7905,
      "eval_samples_per_second": 82.927,
      "eval_steps_per_second": 5.202,
      "epoch": 73.30016583747927
    },
    {
      "loss": 2.5939,
      "grad_norm": 2.6148743629455566,
      "learning_rate": 6.326506275903781e-06,
      "epoch": 73.38308457711443
    },
    {
      "loss": 2.5962,
      "grad_norm": 2.7487242221832275,
      "learning_rate": 6.291919828244986e-06,
      "epoch": 73.46600331674958
    },
    {
      "eval_loss": 2.7629706859588623,
      "eval_runtime": 48.1943,
      "eval_samples_per_second": 85.674,
      "eval_steps_per_second": 5.374,
      "epoch": 73.46600331674958
    },
    {
      "loss": 2.5922,
      "grad_norm": 2.6500847339630127,
      "learning_rate": 6.257384732621812e-06,
      "epoch": 73.54892205638474
    },
    {
      "loss": 2.5999,
      "grad_norm": 2.704500436782837,
      "learning_rate": 6.2229014672999366e-06,
      "epoch": 73.6318407960199
    },
    {
      "eval_loss": 2.7559871673583984,
      "eval_runtime": 50.7942,
      "eval_samples_per_second": 81.289,
      "eval_steps_per_second": 5.099,
      "epoch": 73.6318407960199
    },
    {
      "loss": 2.602,
      "grad_norm": 2.7686655521392822,
      "learning_rate": 6.188470509827244e-06,
      "epoch": 73.71475953565506
    },
    {
      "loss": 2.5758,
      "grad_norm": 2.807804822921753,
      "learning_rate": 6.1540923370272394e-06,
      "epoch": 73.79767827529021
    },
    {
      "eval_loss": 2.7634212970733643,
      "eval_runtime": 49.9629,
      "eval_samples_per_second": 82.641,
      "eval_steps_per_second": 5.184,
      "epoch": 73.79767827529021
    },
    {
      "loss": 2.5635,
      "grad_norm": 2.6159536838531494,
      "learning_rate": 6.119767424992411e-06,
      "epoch": 73.88059701492537
    },
    {
      "loss": 2.5918,
      "grad_norm": 2.737560749053955,
      "learning_rate": 6.085496249077671e-06,
      "epoch": 73.96351575456053
    },
    {
      "eval_loss": 2.767535448074341,
      "eval_runtime": 51.0423,
      "eval_samples_per_second": 80.894,
      "eval_steps_per_second": 5.074,
      "epoch": 73.96351575456053
    },
    {
      "loss": 2.5581,
      "grad_norm": 2.7129452228546143,
      "learning_rate": 6.051279283893758e-06,
      "epoch": 74.0464344941957
    },
    {
      "loss": 2.5736,
      "grad_norm": 2.7567999362945557,
      "learning_rate": 6.017117003300648e-06,
      "epoch": 74.12935323383084
    },
    {
      "eval_loss": 2.7696590423583984,
      "eval_runtime": 51.2373,
      "eval_samples_per_second": 80.586,
      "eval_steps_per_second": 5.055,
      "epoch": 74.12935323383084
    },
    {
      "loss": 2.5812,
      "grad_norm": 2.7670106887817383,
      "learning_rate": 5.9830098804010205e-06,
      "epoch": 74.212271973466
    },
    {
      "loss": 2.5737,
      "grad_norm": 2.732140064239502,
      "learning_rate": 5.948958387533688e-06,
      "epoch": 74.29519071310116
    },
    {
      "eval_loss": 2.7650465965270996,
      "eval_runtime": 49.881,
      "eval_samples_per_second": 82.777,
      "eval_steps_per_second": 5.192,
      "epoch": 74.29519071310116
    },
    {
      "loss": 2.6104,
      "grad_norm": 2.6274526119232178,
      "learning_rate": 5.914962996267064e-06,
      "epoch": 74.37810945273633
    },
    {
      "loss": 2.6017,
      "grad_norm": 2.773092746734619,
      "learning_rate": 5.881024177392627e-06,
      "epoch": 74.46102819237147
    },
    {
      "eval_loss": 2.7782602310180664,
      "eval_runtime": 47.6768,
      "eval_samples_per_second": 86.604,
      "eval_steps_per_second": 5.432,
      "epoch": 74.46102819237147
    },
    {
      "loss": 2.6089,
      "grad_norm": 2.666109085083008,
      "learning_rate": 5.8471424009183954e-06,
      "epoch": 74.54394693200663
    },
    {
      "loss": 2.5666,
      "grad_norm": 2.6835076808929443,
      "learning_rate": 5.8133181360624336e-06,
      "epoch": 74.6268656716418
    },
    {
      "eval_loss": 2.7736215591430664,
      "eval_runtime": 47.5637,
      "eval_samples_per_second": 86.81,
      "eval_steps_per_second": 5.445,
      "epoch": 74.6268656716418
    },
    {
      "loss": 2.5485,
      "grad_norm": 2.742487668991089,
      "learning_rate": 5.779551851246348e-06,
      "epoch": 74.70978441127694
    },
    {
      "loss": 2.5852,
      "grad_norm": 2.7575390338897705,
      "learning_rate": 5.745844014088788e-06,
      "epoch": 74.7927031509121
    },
    {
      "eval_loss": 2.766679525375366,
      "eval_runtime": 47.6256,
      "eval_samples_per_second": 86.697,
      "eval_steps_per_second": 5.438,
      "epoch": 74.7927031509121
    },
    {
      "loss": 2.6081,
      "grad_norm": 2.70516037940979,
      "learning_rate": 5.712195091398989e-06,
      "epoch": 74.87562189054727
    },
    {
      "loss": 2.5738,
      "grad_norm": 2.6954588890075684,
      "learning_rate": 5.678605549170288e-06,
      "epoch": 74.95854063018243
    },
    {
      "eval_loss": 2.768174171447754,
      "eval_runtime": 47.7348,
      "eval_samples_per_second": 86.499,
      "eval_steps_per_second": 5.426,
      "epoch": 74.95854063018243
    },
    {
      "loss": 2.5678,
      "grad_norm": 2.712873697280884,
      "learning_rate": 5.645075852573692e-06,
      "epoch": 75.04145936981757
    },
    {
      "loss": 2.5715,
      "grad_norm": 2.6906161308288574,
      "learning_rate": 5.611606465951421e-06,
      "epoch": 75.12437810945273
    },
    {
      "eval_loss": 2.765151023864746,
      "eval_runtime": 47.6348,
      "eval_samples_per_second": 86.68,
      "eval_steps_per_second": 5.437,
      "epoch": 75.12437810945273
    },
    {
      "loss": 2.5692,
      "grad_norm": 2.8091204166412354,
      "learning_rate": 5.578197852810479e-06,
      "epoch": 75.2072968490879
    },
    {
      "loss": 2.5867,
      "grad_norm": 2.7085142135620117,
      "learning_rate": 5.544850475816232e-06,
      "epoch": 75.29021558872306
    },
    {
      "eval_loss": 2.7482223510742188,
      "eval_runtime": 47.4893,
      "eval_samples_per_second": 86.946,
      "eval_steps_per_second": 5.454,
      "epoch": 75.29021558872306
    },
    {
      "loss": 2.5754,
      "grad_norm": 2.8477487564086914,
      "learning_rate": 5.511564796786021e-06,
      "epoch": 75.3731343283582
    },
    {
      "loss": 2.5662,
      "grad_norm": 2.5835509300231934,
      "learning_rate": 5.478341276682733e-06,
      "epoch": 75.45605306799337
    },
    {
      "eval_loss": 2.750168800354004,
      "eval_runtime": 47.598,
      "eval_samples_per_second": 86.747,
      "eval_steps_per_second": 5.441,
      "epoch": 75.45605306799337
    },
    {
      "loss": 2.5829,
      "grad_norm": 2.662257194519043,
      "learning_rate": 5.445180375608453e-06,
      "epoch": 75.53897180762853
    },
    {
      "loss": 2.568,
      "grad_norm": 2.7049691677093506,
      "learning_rate": 5.412082552798059e-06,
      "epoch": 75.62189054726367
    },
    {
      "eval_loss": 2.7595765590667725,
      "eval_runtime": 47.5137,
      "eval_samples_per_second": 86.901,
      "eval_steps_per_second": 5.451,
      "epoch": 75.62189054726367
    },
    {
      "loss": 2.5509,
      "grad_norm": 2.729262351989746,
      "learning_rate": 5.379048266612889e-06,
      "epoch": 75.70480928689884
    },
    {
      "loss": 2.5782,
      "grad_norm": 2.8369548320770264,
      "learning_rate": 5.346077974534384e-06,
      "epoch": 75.787728026534
    },
    {
      "eval_loss": 2.7550623416900635,
      "eval_runtime": 47.3672,
      "eval_samples_per_second": 87.17,
      "eval_steps_per_second": 5.468,
      "epoch": 75.787728026534
    },
    {
      "loss": 2.5496,
      "grad_norm": 2.6774046421051025,
      "learning_rate": 5.313172133157736e-06,
      "epoch": 75.87064676616916
    },
    {
      "loss": 2.5467,
      "grad_norm": 2.7489101886749268,
      "learning_rate": 5.280331198185593e-06,
      "epoch": 75.9535655058043
    },
    {
      "eval_loss": 2.756676197052002,
      "eval_runtime": 47.4442,
      "eval_samples_per_second": 87.029,
      "eval_steps_per_second": 5.459,
      "epoch": 75.9535655058043
    },
    {
      "loss": 2.5809,
      "grad_norm": 2.7505369186401367,
      "learning_rate": 5.247555624421736e-06,
      "epoch": 76.03648424543947
    },
    {
      "loss": 2.5588,
      "grad_norm": 2.7050042152404785,
      "learning_rate": 5.214845865764766e-06,
      "epoch": 76.11940298507463
    },
    {
      "eval_loss": 2.762590169906616,
      "eval_runtime": 47.4976,
      "eval_samples_per_second": 86.931,
      "eval_steps_per_second": 5.453,
      "epoch": 76.11940298507463
    },
    {
      "loss": 2.5844,
      "grad_norm": 2.754795551300049,
      "learning_rate": 5.1822023752018445e-06,
      "epoch": 76.20232172470979
    },
    {
      "loss": 2.5616,
      "grad_norm": 2.7267231941223145,
      "learning_rate": 5.149625604802396e-06,
      "epoch": 76.28524046434494
    },
    {
      "eval_loss": 2.75960111618042,
      "eval_runtime": 47.5171,
      "eval_samples_per_second": 86.895,
      "eval_steps_per_second": 5.451,
      "epoch": 76.28524046434494
    },
    {
      "loss": 2.5573,
      "grad_norm": 2.6510658264160156,
      "learning_rate": 5.117116005711872e-06,
      "epoch": 76.3681592039801
    },
    {
      "loss": 2.5599,
      "grad_norm": 2.7283740043640137,
      "learning_rate": 5.084674028145476e-06,
      "epoch": 76.45107794361526
    },
    {
      "eval_loss": 2.7718472480773926,
      "eval_runtime": 47.6575,
      "eval_samples_per_second": 86.639,
      "eval_steps_per_second": 5.435,
      "epoch": 76.45107794361526
    },
    {
      "loss": 2.5614,
      "grad_norm": 2.621262550354004,
      "learning_rate": 5.0523001213819464e-06,
      "epoch": 76.53399668325042
    },
    {
      "loss": 2.5712,
      "grad_norm": 2.6701242923736572,
      "learning_rate": 5.0199947337573386e-06,
      "epoch": 76.61691542288557
    },
    {
      "eval_loss": 2.7385473251342773,
      "eval_runtime": 47.7757,
      "eval_samples_per_second": 86.425,
      "eval_steps_per_second": 5.421,
      "epoch": 76.61691542288557
    },
    {
      "loss": 2.5555,
      "grad_norm": 2.6688222885131836,
      "learning_rate": 4.987758312658795e-06,
      "epoch": 76.69983416252073
    },
    {
      "loss": 2.516,
      "grad_norm": 2.7219057083129883,
      "learning_rate": 4.95559130451837e-06,
      "epoch": 76.78275290215589
    },
    {
      "eval_loss": 2.7447195053100586,
      "eval_runtime": 47.589,
      "eval_samples_per_second": 86.764,
      "eval_steps_per_second": 5.442,
      "epoch": 76.78275290215589
    },
    {
      "loss": 2.5685,
      "grad_norm": 2.750861644744873,
      "learning_rate": 4.923494154806845e-06,
      "epoch": 76.86567164179104
    },
    {
      "loss": 2.5838,
      "grad_norm": 2.7492034435272217,
      "learning_rate": 4.891467308027539e-06,
      "epoch": 76.9485903814262
    },
    {
      "eval_loss": 2.7447822093963623,
      "eval_runtime": 47.6,
      "eval_samples_per_second": 86.744,
      "eval_steps_per_second": 5.441,
      "epoch": 76.9485903814262
    },
    {
      "loss": 2.5273,
      "grad_norm": 2.6925172805786133,
      "learning_rate": 4.85951120771018e-06,
      "epoch": 77.03150912106136
    },
    {
      "loss": 2.5391,
      "grad_norm": 2.727611541748047,
      "learning_rate": 4.827626296404751e-06,
      "epoch": 77.11442786069652
    },
    {
      "eval_loss": 2.759553909301758,
      "eval_runtime": 47.6862,
      "eval_samples_per_second": 86.587,
      "eval_steps_per_second": 5.431,
      "epoch": 77.11442786069652
    },
    {
      "loss": 2.5576,
      "grad_norm": 2.76997709274292,
      "learning_rate": 4.795813015675351e-06,
      "epoch": 77.19734660033167
    },
    {
      "loss": 2.5501,
      "grad_norm": 2.8407013416290283,
      "learning_rate": 4.7640718060941e-06,
      "epoch": 77.28026533996683
    },
    {
      "eval_loss": 2.7610840797424316,
      "eval_runtime": 47.8649,
      "eval_samples_per_second": 86.264,
      "eval_steps_per_second": 5.411,
      "epoch": 77.28026533996683
    },
    {
      "loss": 2.5576,
      "grad_norm": 2.6784074306488037,
      "learning_rate": 4.732403107235015e-06,
      "epoch": 77.363184079602
    },
    {
      "loss": 2.5489,
      "grad_norm": 2.663088798522949,
      "learning_rate": 4.700807357667953e-06,
      "epoch": 77.44610281923715
    },
    {
      "eval_loss": 2.747469902038574,
      "eval_runtime": 47.9231,
      "eval_samples_per_second": 86.159,
      "eval_steps_per_second": 5.404,
      "epoch": 77.44610281923715
    },
    {
      "loss": 2.5821,
      "grad_norm": 2.777245044708252,
      "learning_rate": 4.669284994952499e-06,
      "epoch": 77.5290215588723
    },
    {
      "loss": 2.5591,
      "grad_norm": 2.7685952186584473,
      "learning_rate": 4.637836455631943e-06,
      "epoch": 77.61194029850746
    },
    {
      "eval_loss": 2.762399435043335,
      "eval_runtime": 47.952,
      "eval_samples_per_second": 86.107,
      "eval_steps_per_second": 5.401,
      "epoch": 77.61194029850746
    },
    {
      "loss": 2.5536,
      "grad_norm": 2.698882818222046,
      "learning_rate": 4.606462175227207e-06,
      "epoch": 77.69485903814262
    },
    {
      "loss": 2.5489,
      "grad_norm": 2.7369518280029297,
      "learning_rate": 4.5751625882308335e-06,
      "epoch": 77.77777777777777
    },
    {
      "eval_loss": 2.7355268001556396,
      "eval_runtime": 47.7652,
      "eval_samples_per_second": 86.444,
      "eval_steps_per_second": 5.422,
      "epoch": 77.77777777777777
    },
    {
      "loss": 2.5621,
      "grad_norm": 2.6725916862487793,
      "learning_rate": 4.5439381281009494e-06,
      "epoch": 77.86069651741293
    },
    {
      "loss": 2.5577,
      "grad_norm": 2.7050423622131348,
      "learning_rate": 4.512789227255285e-06,
      "epoch": 77.9436152570481
    },
    {
      "eval_loss": 2.749480724334717,
      "eval_runtime": 48.5471,
      "eval_samples_per_second": 85.051,
      "eval_steps_per_second": 5.335,
      "epoch": 77.9436152570481
    },
    {
      "loss": 2.566,
      "grad_norm": 2.7364232540130615,
      "learning_rate": 4.481716317065163e-06,
      "epoch": 78.02653399668326
    },
    {
      "loss": 2.5294,
      "grad_norm": 2.7243666648864746,
      "learning_rate": 4.450719827849539e-06,
      "epoch": 78.1094527363184
    },
    {
      "eval_loss": 2.742067337036133,
      "eval_runtime": 48.8459,
      "eval_samples_per_second": 84.531,
      "eval_steps_per_second": 5.302,
      "epoch": 78.1094527363184
    },
    {
      "loss": 2.5621,
      "grad_norm": 2.665127754211426,
      "learning_rate": 4.419800188869048e-06,
      "epoch": 78.19237147595356
    },
    {
      "loss": 2.549,
      "grad_norm": 2.572354555130005,
      "learning_rate": 4.388957828320032e-06,
      "epoch": 78.27529021558873
    },
    {
      "eval_loss": 2.7442996501922607,
      "eval_runtime": 49.2529,
      "eval_samples_per_second": 83.833,
      "eval_steps_per_second": 5.259,
      "epoch": 78.27529021558873
    },
    {
      "loss": 2.5448,
      "grad_norm": 2.714120388031006,
      "learning_rate": 4.358193173328642e-06,
      "epoch": 78.35820895522389
    },
    {
      "loss": 2.5431,
      "grad_norm": 2.721668243408203,
      "learning_rate": 4.3275066499449105e-06,
      "epoch": 78.44112769485903
    },
    {
      "eval_loss": 2.7463793754577637,
      "eval_runtime": 46.8694,
      "eval_samples_per_second": 88.096,
      "eval_steps_per_second": 5.526,
      "epoch": 78.44112769485903
    },
    {
      "loss": 2.5318,
      "grad_norm": 2.7191779613494873,
      "learning_rate": 4.296898683136836e-06,
      "epoch": 78.5240464344942
    },
    {
      "loss": 2.5685,
      "grad_norm": 2.78904390335083,
      "learning_rate": 4.26636969678453e-06,
      "epoch": 78.60696517412936
    },
    {
      "eval_loss": 2.757061719894409,
      "eval_runtime": 48.7761,
      "eval_samples_per_second": 84.652,
      "eval_steps_per_second": 5.31,
      "epoch": 78.60696517412936
    },
    {
      "loss": 2.5404,
      "grad_norm": 2.690495729446411,
      "learning_rate": 4.235920113674314e-06,
      "epoch": 78.6898839137645
    },
    {
      "loss": 2.5563,
      "grad_norm": 2.7803003787994385,
      "learning_rate": 4.2055503554928805e-06,
      "epoch": 78.77280265339967
    },
    {
      "eval_loss": 2.7360684871673584,
      "eval_runtime": 47.6522,
      "eval_samples_per_second": 86.649,
      "eval_steps_per_second": 5.435,
      "epoch": 78.77280265339967
    },
    {
      "loss": 2.5465,
      "grad_norm": 2.699474334716797,
      "learning_rate": 4.175260842821462e-06,
      "epoch": 78.85572139303483
    },
    {
      "loss": 2.5696,
      "grad_norm": 2.7459053993225098,
      "learning_rate": 4.145051995129984e-06,
      "epoch": 78.93864013266999
    },
    {
      "eval_loss": 2.7462432384490967,
      "eval_runtime": 46.2494,
      "eval_samples_per_second": 89.277,
      "eval_steps_per_second": 5.6,
      "epoch": 78.93864013266999
    },
    {
      "loss": 2.5346,
      "grad_norm": 2.601768732070923,
      "learning_rate": 4.114924230771279e-06,
      "epoch": 79.02155887230514
    },
    {
      "loss": 2.5511,
      "grad_norm": 2.708120346069336,
      "learning_rate": 4.08487796697527e-06,
      "epoch": 79.1044776119403
    },
    {
      "eval_loss": 2.7301025390625,
      "eval_runtime": 46.1033,
      "eval_samples_per_second": 89.56,
      "eval_steps_per_second": 5.618,
      "epoch": 79.1044776119403
    },
    {
      "loss": 2.5794,
      "grad_norm": 2.8581981658935547,
      "learning_rate": 4.054913619843215e-06,
      "epoch": 79.18739635157546
    },
    {
      "loss": 2.5409,
      "grad_norm": 2.7884347438812256,
      "learning_rate": 4.025031604341932e-06,
      "epoch": 79.27031509121062
    },
    {
      "eval_loss": 2.7538321018218994,
      "eval_runtime": 46.1944,
      "eval_samples_per_second": 89.383,
      "eval_steps_per_second": 5.607,
      "epoch": 79.27031509121062
    },
    {
      "loss": 2.5469,
      "grad_norm": 2.8024725914001465,
      "learning_rate": 3.9952323342980456e-06,
      "epoch": 79.35323383084577
    },
    {
      "loss": 2.5527,
      "grad_norm": 2.6730148792266846,
      "learning_rate": 3.965516222392274e-06,
      "epoch": 79.43615257048093
    },
    {
      "eval_loss": 2.7451651096343994,
      "eval_runtime": 46.178,
      "eval_samples_per_second": 89.415,
      "eval_steps_per_second": 5.609,
      "epoch": 79.43615257048093
    },
    {
      "loss": 2.5361,
      "grad_norm": 2.762082099914551,
      "learning_rate": 3.935883680153706e-06,
      "epoch": 79.51907131011609
    },
    {
      "loss": 2.5317,
      "grad_norm": 2.7266080379486084,
      "learning_rate": 3.906335117954087e-06,
      "epoch": 79.60199004975124
    },
    {
      "eval_loss": 2.739567995071411,
      "eval_runtime": 46.019,
      "eval_samples_per_second": 89.724,
      "eval_steps_per_second": 5.628,
      "epoch": 79.60199004975124
    },
    {
      "loss": 2.5613,
      "grad_norm": 2.79699969291687,
      "learning_rate": 3.876870945002166e-06,
      "epoch": 79.6849087893864
    },
    {
      "loss": 2.5662,
      "grad_norm": 2.7768912315368652,
      "learning_rate": 3.847491569337997e-06,
      "epoch": 79.76782752902156
    },
    {
      "eval_loss": 2.732771396636963,
      "eval_runtime": 46.0981,
      "eval_samples_per_second": 89.57,
      "eval_steps_per_second": 5.618,
      "epoch": 79.76782752902156
    },
    {
      "loss": 2.5291,
      "grad_norm": 2.8113229274749756,
      "learning_rate": 3.8181973978273165e-06,
      "epoch": 79.85074626865672
    },
    {
      "loss": 2.5424,
      "grad_norm": 2.6907992362976074,
      "learning_rate": 3.7889888361558856e-06,
      "epoch": 79.93366500829187
    },
    {
      "eval_loss": 2.731144666671753,
      "eval_runtime": 46.0731,
      "eval_samples_per_second": 89.618,
      "eval_steps_per_second": 5.621,
      "epoch": 79.93366500829187
    },
    {
      "loss": 2.5224,
      "grad_norm": 2.682138204574585,
      "learning_rate": 3.7598662888238813e-06,
      "epoch": 80.01658374792703
    },
    {
      "loss": 2.5307,
      "grad_norm": 2.657093048095703,
      "learning_rate": 3.730830159140302e-06,
      "epoch": 80.09950248756219
    },
    {
      "eval_loss": 2.737589120864868,
      "eval_runtime": 46.1443,
      "eval_samples_per_second": 89.48,
      "eval_steps_per_second": 5.613,
      "epoch": 80.09950248756219
    },
    {
      "loss": 2.5392,
      "grad_norm": 2.6088106632232666,
      "learning_rate": 3.7018808492173753e-06,
      "epoch": 80.18242122719735
    },
    {
      "loss": 2.5442,
      "grad_norm": 2.7841274738311768,
      "learning_rate": 3.6730187599649804e-06,
      "epoch": 80.2653399668325
    },
    {
      "eval_loss": 2.73954701423645,
      "eval_runtime": 46.0841,
      "eval_samples_per_second": 89.597,
      "eval_steps_per_second": 5.62,
      "epoch": 80.2653399668325
    },
    {
      "loss": 2.5504,
      "grad_norm": 2.687060832977295,
      "learning_rate": 3.6442442910851163e-06,
      "epoch": 80.34825870646766
    },
    {
      "loss": 2.5605,
      "grad_norm": 2.7705507278442383,
      "learning_rate": 3.6155578410663418e-06,
      "epoch": 80.43117744610282
    },
    {
      "eval_loss": 2.7315926551818848,
      "eval_runtime": 46.0998,
      "eval_samples_per_second": 89.567,
      "eval_steps_per_second": 5.618,
      "epoch": 80.43117744610282
    },
    {
      "loss": 2.5844,
      "grad_norm": 2.646543264389038,
      "learning_rate": 3.5869598071782828e-06,
      "epoch": 80.51409618573798
    },
    {
      "loss": 2.5446,
      "grad_norm": 2.802816152572632,
      "learning_rate": 3.5584505854661143e-06,
      "epoch": 80.59701492537313
    },
    {
      "eval_loss": 2.743705987930298,
      "eval_runtime": 46.0813,
      "eval_samples_per_second": 89.603,
      "eval_steps_per_second": 5.621,
      "epoch": 80.59701492537313
    },
    {
      "loss": 2.5889,
      "grad_norm": 2.9084763526916504,
      "learning_rate": 3.530030570745072e-06,
      "epoch": 80.67993366500829
    },
    {
      "loss": 2.5467,
      "grad_norm": 2.7459208965301514,
      "learning_rate": 3.5017001565950047e-06,
      "epoch": 80.76285240464345
    },
    {
      "eval_loss": 2.742995023727417,
      "eval_runtime": 46.1096,
      "eval_samples_per_second": 89.547,
      "eval_steps_per_second": 5.617,
      "epoch": 80.76285240464345
    },
    {
      "loss": 2.5729,
      "grad_norm": 2.755321979522705,
      "learning_rate": 3.473459735354897e-06,
      "epoch": 80.8457711442786
    },
    {
      "loss": 2.5173,
      "grad_norm": 2.8042330741882324,
      "learning_rate": 3.445309698117465e-06,
      "epoch": 80.92868988391376
    },
    {
      "eval_loss": 2.731686592102051,
      "eval_runtime": 46.1684,
      "eval_samples_per_second": 89.434,
      "eval_steps_per_second": 5.61,
      "epoch": 80.92868988391376
    },
    {
      "loss": 2.5228,
      "grad_norm": 2.6049578189849854,
      "learning_rate": 3.41725043472371e-06,
      "epoch": 81.01160862354892
    },
    {
      "loss": 2.5195,
      "grad_norm": 2.6719131469726562,
      "learning_rate": 3.38928233375755e-06,
      "epoch": 81.09452736318408
    },
    {
      "eval_loss": 2.7307310104370117,
      "eval_runtime": 46.0782,
      "eval_samples_per_second": 89.609,
      "eval_steps_per_second": 5.621,
      "epoch": 81.09452736318408
    },
    {
      "loss": 2.5123,
      "grad_norm": 2.719801902770996,
      "learning_rate": 3.361405782540408e-06,
      "epoch": 81.17744610281923
    },
    {
      "loss": 2.5303,
      "grad_norm": 2.804353713989258,
      "learning_rate": 3.333621167125878e-06,
      "epoch": 81.2603648424544
    },
    {
      "eval_loss": 2.7427139282226562,
      "eval_runtime": 46.0183,
      "eval_samples_per_second": 89.725,
      "eval_steps_per_second": 5.628,
      "epoch": 81.2603648424544
    },
    {
      "loss": 2.532,
      "grad_norm": 2.832200527191162,
      "learning_rate": 3.3059288722943496e-06,
      "epoch": 81.34328358208955
    },
    {
      "loss": 2.5189,
      "grad_norm": 2.6917812824249268,
      "learning_rate": 3.2783292815477095e-06,
      "epoch": 81.42620232172472
    },
    {
      "eval_loss": 2.7369229793548584,
      "eval_runtime": 46.1402,
      "eval_samples_per_second": 89.488,
      "eval_steps_per_second": 5.613,
      "epoch": 81.42620232172472
    },
    {
      "loss": 2.5408,
      "grad_norm": 2.7008116245269775,
      "learning_rate": 3.250822777103998e-06,
      "epoch": 81.50912106135986
    },
    {
      "loss": 2.5327,
      "grad_norm": 2.7016794681549072,
      "learning_rate": 3.223409739892145e-06,
      "epoch": 81.59203980099502
    },
    {
      "eval_loss": 2.7358248233795166,
      "eval_runtime": 46.0421,
      "eval_samples_per_second": 89.679,
      "eval_steps_per_second": 5.625,
      "epoch": 81.59203980099502
    },
    {
      "train_runtime": 76639.7516,
      "train_samples_per_second": 25.174,
      "train_steps_per_second": 0.787,
      "total_flos": 2.0721365911026893e+17,
      "train_loss": 3.9558307713997074,
      "epoch": 81.59203980099502
    },
    {
      "eval_loss": 2.7365987300872803,
      "eval_runtime": 45.673,
      "eval_samples_per_second": 90.404,
      "eval_steps_per_second": 5.671,
      "epoch": 81.59203980099502
    }
  ]
}